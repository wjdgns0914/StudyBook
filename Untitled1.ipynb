{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3376001  -0.43742612 -0.4318817   0.43456566 -0.32312432]\n",
      "[(array([ -0.26495996,  -2.2998817 ,  -5.1547747 ,  -3.61878   ,\n",
      "       -13.231243  ], dtype=float32), array([ 0.3376001 , -0.43742612, -0.4318817 ,  0.43456566, -0.32312432],\n",
      "      dtype=float32))]\n",
      "[ 0.3376001  -0.43742612 -0.4318817   0.43456566 -0.32312432] \n",
      " [ 0.33786508 -0.43512625 -0.4267269   0.43818444 -0.30989307] \n",
      " [0.00026497 0.00229988 0.00515479 0.00361878 0.01323125]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "y_true=tf.constant([1,2.,3,4,5])\n",
    "x=tf.constant([1,2,3,4.,5])\n",
    "w=tf.get_variable(name='w',shape=[5,],initializer=tf.contrib.layers.xavier_initializer(),)\n",
    "y=w*x\n",
    "loss=tf.reduce_mean(tf.pow(y-y_true,2))\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "grads = opt.compute_gradients(loss)\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    w1=sess.run(w)\n",
    "    print(w1)\n",
    "    print(sess.run(grads))\n",
    "    sess.run(apply_gradient_op)\n",
    "    w2=sess.run(w)\n",
    "    print(w1,'\\n',w2,'\\n',w2-w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999896315728952\n",
      "2.1153662\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.log(2.718))\n",
    "def Shift(x):\n",
    "  return 2 ** tf.round(tf.log(x) / tf.log(2.0))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.log(4.333)/tf.log(2.)))   #log2(4.333)=log(4.333)/log(2)\n",
    "    print(sess.run(Shift(7.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shift(x):\n",
    "    return 2**np.round(np.log(x)/np.log(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "1.4142135\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "x=[i*0.000001 for i in range(4000001,8000000)]\n",
    "print((Shift(x)).min())\n",
    "print((np.array(x)/Shift(x)).max())\n",
    "print(np.sqrt(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n"
     ]
    }
   ],
   "source": [
    "x=[i*0.01 for i in range(401,800)]\n",
    "print((Shift(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n"
     ]
    }
   ],
   "source": [
    "print(np.round(np.sqrt(2)*11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def scalebybits(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "\n",
    "def shiftbyval(x):\n",
    "    return 2.0 ** tf.round(tf.log(x) / tf.log(2.0))\n",
    "\n",
    "def quantize(x, target_level=2**8):\n",
    "    assert np.floor(target_level)==target_level,\"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    \n",
    "    # shift=shiftbyval(tf.abs(x))  #We don't shift x in this function, this function is only for quantizatizing any input.\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        # 15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        y = x\n",
    "    elif bits == 1:  # BNN\n",
    "        print(\"The target_level=1 and we use BNN\")\n",
    "        y = tf.sign(x)\n",
    "    elif target_level%2==1:\n",
    "        print('target_level is a odd number')\n",
    "        y = tf.round(x * tf.floor(target_level/2))/ SCALE\n",
    "        #20180410 \n",
    "        #레벨 수는 맞추되, 그 레벨의 값이 2의 승수로 만든다.\n",
    "        #다만 이 부분에서 확인이 안된건 weight 값에 따라 학습결과가 달라지나 안달라지나를 확인해야한다.\n",
    "        #-0.5, 0, 0.5와 -1, 0, 1의 학습결과가 달라지나 안달라지나?\n",
    "        #BatchNorm쓰면 별로 안달리질거고, WAGE에서는 scale layer를 쓰는데 그걸 여기 구현하지는 않았다.\n",
    "    else:\n",
    "        print('target_level is a even number')\n",
    "        SCALE=SCALE*2\n",
    "        \n",
    "        y = (tf.sign(x)*(2*tf.round(tf.abs(x)*(target_level/2)+0.5)-1)+tf.to_float(tf.equal(x,0)))/SCALE\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "\n",
    "def clip(x,target_level=2**8):\n",
    "    #이게 왜 필요하냐?\n",
    "    #레벨 수가 몇이든 간에 결국 해당하는 bits의 2의 승수배로 weight 값을 주게된다.\n",
    "    #그니까 그 최댓값을 넘어가는 값은 다 최댓값으로 맵핑시킬 필요가 있다.\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    if bits > 15 or bits == 1:\n",
    "        delta = 0.\n",
    "    else:\n",
    "        delta = 1. / SCALE\n",
    "    if target_level%2==1:\n",
    "        limit=SCALE-tf.floor(target_level)\n",
    "    else:\n",
    "        limit=SCALE-target_level+1\n",
    "    MAX = +1 - limit*delta\n",
    "    MIN = -1 + limit*delta\n",
    "    y = tf.clip_by_value(x, MIN, MAX, name='saturate')\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "\n",
    "def quantize_G(x,target_level=2**8):\n",
    "    bitsG = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsG)\n",
    "    with tf.name_scope('Quantize_G'):\n",
    "        if bitsG > 15:\n",
    "            return x\n",
    "        else:\n",
    "            if x.name.lower().find('batchnorm') > -1:\n",
    "                return x  # batch norm parameters, not quantize now\n",
    "            xmax = tf.reduce_max(tf.abs(x))\n",
    "            y = x / shiftbyval(xmax)\n",
    "            norm = quantize(LR * y, 4095)\n",
    "            norm_sign = tf.sign(norm)\n",
    "            norm_abs = tf.abs(norm)\n",
    "            norm_int = tf.floor(norm_abs)\n",
    "            norm_float = norm_abs - norm_int\n",
    "            rand_float = tf.random_uniform(x.get_shape(), 0, 1)\n",
    "            norm = norm_sign * (norm_int + 0.5 * (tf.sign(norm_float - rand_float) + 1))\n",
    "            return norm / SCALE\n",
    "\n",
    "def quantize_W(x,target_level=2**8):\n",
    "    bitsW = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsW)\n",
    "    with tf.name_scope('Quantize_W'):\n",
    "        if bitsW > 15:\n",
    "            return x\n",
    "        else:\n",
    "            xmax = tf.to_float(tf.reduce_max(tf.abs(x)))\n",
    "            print(xmax)\n",
    "            y = x / shiftbyval(xmax)\n",
    "#             y = clip(quantize(y, target_level), target_level)\n",
    "            y = quantize(clip(y, target_level), target_level)\n",
    "            #20180410: clipr과 quantize의 적용 순서에 대한 명확한 이해가 없다.\n",
    "            # we scale W in QW rather than QA for simplicity\n",
    "            return x + tf.stop_gradient(y - x)  # skip derivation of Quantize and Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_level is a odd number\n",
      "[-0.25  0.    0.  ]\n",
      "Original x:\n",
      " [-0.0012 -0.001  -0.0008 -0.0006 -0.0004 -0.0002  0.      0.0002  0.0004\n",
      "  0.0006  0.0008  0.001   0.0012]\n",
      "0.0012\n",
      "0.0009765625\n",
      "inf\n",
      "\n",
      "Quantize x to 6 levels:\n",
      "target_level is a even number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.     -1.     -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.      1.    ]\n",
      "  temp_quantize: [-0.875 -0.875 -0.625 -0.375 -0.375 -0.125  0.125  0.125  0.375  0.375\n",
      "  0.625  0.875  0.875]\n",
      "[3.         3.         2.4575999  1.8432001  1.2287999  0.61439997\n",
      " 0.         0.61439997 1.2287999  1.8432001  2.4575999  3.\n",
      " 3.        ]\n",
      "Tensor(\"Quantize_W_63/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a even number\n",
      "     quantize_W: [-0.625 -0.625 -0.625 -0.375 -0.375 -0.125 -0.125  0.125  0.375  0.375\n",
      "  0.625  0.625  0.625]\n",
      "[-1.]\n",
      "\n",
      "Quantize x to 11 levels:\n",
      "target_level is a odd number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.     -1.     -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.      1.    ]\n",
      "  temp_quantize: [-0.75  -0.625 -0.5   -0.375 -0.25  -0.125  0.     0.125  0.25   0.375\n",
      "  0.5    0.625  0.75 ]\n",
      "[5.5       5.5       4.5056    3.3792002 2.2528    1.1264    0.\n",
      " 1.1264    2.2528    3.3792002 4.5056    5.5       5.5      ]\n",
      "Tensor(\"Quantize_W_64/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a odd number\n",
      "     quantize_W: [-0.625 -0.625 -0.5   -0.375 -0.25  -0.125  0.     0.125  0.25   0.375\n",
      "  0.5    0.625  0.625]\n",
      "[1.]\n",
      "\n",
      "Quantize x to 12 levels:\n",
      "target_level is a even number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.     -1.     -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.      1.    ]\n",
      "  temp_quantize: [-0.9375 -0.8125 -0.5625 -0.4375 -0.3125 -0.1875  0.0625  0.1875  0.3125\n",
      "  0.4375  0.5625  0.8125  0.9375]\n",
      "[6.        6.        4.9151998 3.6864002 2.4575999 1.2287999 0.\n",
      " 1.2287999 2.4575999 3.6864002 4.9151998 6.        6.       ]\n",
      "Tensor(\"Quantize_W_65/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a even number\n",
      "     quantize_W: [-0.6875 -0.6875 -0.5625 -0.4375 -0.3125 -0.1875  0.0625  0.1875  0.3125\n",
      "  0.4375  0.5625  0.6875  0.6875]\n",
      "[-1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scalebybits(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "\n",
    "\n",
    "def shiftbyval(x):\n",
    "    return 2.0 ** tf.round(tf.log(x) / tf.log(2.0))\n",
    "\n",
    "\n",
    "def quantize(x, target_level=2 ** 8):\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bits)\n",
    "\n",
    "    # shift=shiftbyval(tf.abs(x))  #We don't shift x in this function, this function is only for quantizatizing any input.\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        # 15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        y = x\n",
    "    elif bits == 1:  # BNN\n",
    "        print(\"The target_level=1 and we use BNN\")\n",
    "        y = tf.sign(x)\n",
    "    elif target_level % 2 == 1:\n",
    "        print('target_level is a odd number')\n",
    "        y = tf.round(x * tf.floor(target_level / 2)) / SCALE\n",
    "        # 20180410\n",
    "        # 레벨 수는 맞추되, 그 레벨의 값이 2의 승수로 만든다.\n",
    "        # 다만 이 부분에서 확인이 안된건 weight 값에 따라 학습결과가 달라지나 안달라지나를 확인해야한다.\n",
    "        # -0.5, 0, 0.5와 -1, 0, 1의 학습결과가 달라지나 안달라지나?\n",
    "        # BatchNorm쓰면 별로 안달리질거고, WAGE에서는 scale layer를 쓰는데 그걸 여기 구현하지는 않았다.\n",
    "    else:\n",
    "        print('target_level is a even number')\n",
    "        SCALE = SCALE * 2\n",
    "        y = (tf.sign(x)*(2*tf.ceil(tf.abs(x)*(target_level/2))-1)+\n",
    "             tf.to_float(tf.equal(x,0))*tf.sign(tf.random_uniform(x.get_shape(),-1.,1.)))/SCALE\n",
    "    return tf.stop_gradient(y - x) + x\n",
    "\n",
    "\n",
    "def clip(x, target_level=2 ** 8):\n",
    "    # 이게 왜 필요하냐?\n",
    "    # 레벨 수가 몇이든 간에 결국 해당하는 bits의 2의 승수배로 weight 값을 주게된다.\n",
    "    # 그니까 그 최댓값을 넘어가는 값은 다 최댓값으로 맵핑시킬 필요가 있다.\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bits)\n",
    "    if bits > 15 or bits == 1:\n",
    "        delta = 0.\n",
    "    elif target_level % 2 == 1:\n",
    "        limit = SCALE - tf.floor(target_level)\n",
    "        delta = 1. / SCALE\n",
    "    else:\n",
    "        limit = SCALE*2 - target_level + 1\n",
    "        delta = 1. / (SCALE*2)\n",
    "    MAX = +1 #- limit * delta\n",
    "    MIN = -1 #+ limit * delta\n",
    "    y = tf.clip_by_value(x, MIN, MAX, name='saturate')\n",
    "    return tf.stop_gradient(y - x) + x\n",
    "\n",
    "\n",
    "def quantize_G(x, target_level=2 ** 8):\n",
    "    bitsG = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsG)\n",
    "    with tf.name_scope('Quantize_G'):\n",
    "        if bitsG > 15:\n",
    "            return x\n",
    "        else:\n",
    "            if x.name.lower().find('batchnorm') > -1:\n",
    "                return x  # batch norm parameters, not quantize now\n",
    "            xmax = tf.reduce_max(tf.abs(x))\n",
    "            y = x / shiftbyval(xmax)\n",
    "            norm = quantize(LR * y, 4095)\n",
    "            norm_sign = tf.sign(norm)\n",
    "            norm_abs = tf.abs(norm)\n",
    "            norm_int = tf.floor(norm_abs)\n",
    "            norm_float = norm_abs - norm_int\n",
    "            rand_float = tf.random_uniform(x.get_shape(), 0, 1)\n",
    "            norm = norm_sign * (norm_int + 0.5 * (tf.sign(norm_float - rand_float) + 1))\n",
    "            return norm / SCALE\n",
    "\n",
    "\n",
    "def quantize_W(x, target_level=2 ** 8):\n",
    "    bitsW = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsW)\n",
    "    with tf.name_scope('Quantize_W'):\n",
    "        if bitsW > 15:\n",
    "            return x\n",
    "        else:\n",
    "            xmax = tf.to_float(tf.reduce_max(tf.abs(x)))\n",
    "            print(xmax)\n",
    "            y = x / shiftbyval(xmax)\n",
    "            #             y = clip(quantize(y, target_level), target_level)\n",
    "            y = quantize(clip(y, target_level), target_level)\n",
    "            # 20180410: clipr과 quantize의 적용 순서에 대한 명확한 이해가 없다.\n",
    "            # we scale W in QW rather than QA for simplicity\n",
    "            return x + tf.stop_gradient(y - x)  # skip derivation of Quantize and Clip\n",
    "\n",
    "        \n",
    "LR=1\n",
    "target_level_list=[6.,11.,12.]\n",
    "x=tf.constant([0.0002*i for i in range(-6,7)],name='Const')\n",
    "y1=tf.Variable(0)\n",
    "y2=tf.Variable(1)\n",
    "y3=y2/y1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(quantize_G(tf.constant([-11111111111111111111111111111.100000000000000000000000000000000000000000000000000000000000000000000000000346001302,.0000000000000000000000000000000000000000000000000000000000000000000000062343,0.000000000000000000000000000000000000000003512412400000011263]),7)))\n",
    "    print(\"Original x:\\n\",sess.run(x))\n",
    "    print(sess.run(tf.to_float(tf.reduce_max(tf.abs(x)))))\n",
    "    print(sess.run(shiftbyval(tf.to_float(tf.reduce_max(tf.abs(x))))))\n",
    "    temp_scaled=x/shiftbyval(tf.to_float(tf.reduce_max(tf.abs(x))))\n",
    "    print(sess.run(y3))\n",
    "    for target_level in target_level_list:\n",
    "        print(\"\\nQuantize x to %d levels:\"%target_level)\n",
    "        temp_clip=clip(temp_scaled,target_level)\n",
    "        temp_quantize=quantize(temp_scaled,target_level)\n",
    "        print(\"%15s:\"%\"temp_scaled\",sess.run(temp_scaled))\n",
    "        print(\"%15s:\"%\"temp_clip\",sess.run(temp_clip))\n",
    "        print(\"%15s:\"%\"temp_quantize\",sess.run(temp_quantize))\n",
    "        print(sess.run(tf.abs(temp_clip)*(target_level/2)))\n",
    "        x1=quantize_W(x,target_level)\n",
    "        # x2=quantize_G(x,target_level)\n",
    "        print(\"%15s:\"%\"quantize_W\",sess.run(x1))\n",
    "        print(sess.run(tf.sign(tf.random_uniform([1],-1.,1.))))\n",
    "        # print(\"%15s:\"%\"quantize_G\",sess.run(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to keyword (<ipython-input-60-31d8e0c7cda5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-31d8e0c7cda5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    a= None = 3\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to keyword\n"
     ]
    }
   ],
   "source": [
    "# print(open('Option.py').read())\n",
    "a= None = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.46893153, -0.8372386 , -1.1603353 , -1.5564557 , -2.1467435 ],\n",
      "      dtype=float32)]\n",
      "[[-0.46893153 -0.8372386  -1.1603353  -1.5564557  -2.1467435 ]]\n",
      "[array([1., 1., 1., 1., 1.], dtype=float32)]\n",
      "[(<tf.Tensor 'gradients/mul_1_grad/tuple/control_dependency:0' shape=(5,) dtype=float32>, <tf.Variable 'w:0' shape=(5,) dtype=float32_ref>)]\n",
      "[(array([-0.11723288, -0.20930965, -0.58016765, -0.77822787, -1.6100576 ],\n",
      "      dtype=float32), array([-0.68931526, -0.37238613,  0.19832414,  0.21772164, -0.48914415],\n",
      "      dtype=float32))]\n",
      "[0.75 0.75 0.75]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "def scale_bit(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "def quantize(x, target_level=7):\n",
    "    assert target_level%2==1,'target_level should be a odd number'\n",
    "    y=x / tf.reduce_max(tf.abs(x))\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        #15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        return x\n",
    "    elif bits == 1:  # BNN\n",
    "        return tf.sign(x)\n",
    "    else:\n",
    "        y = tf.to_float(tf.clip_by_value(y, -1.0, 1.0, name='saturate'))\n",
    "        y = tf.round(y*tf.floor(target_level/2))/scale_bit(bits)\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "\n",
    "y_true=tf.constant([1,2.,3,4,5])\n",
    "x=tf.constant([1,2,3,4.,5])\n",
    "w=tf.get_variable(name='w',shape=[5,],initializer=tf.contrib.layers.xavier_initializer(),)\n",
    "y0=quantize(x)\n",
    "y=w*y0\n",
    "loss=tf.reduce_mean(tf.pow(y-y_true,2))\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "grads = opt.compute_gradients(loss)\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(tf.gradients(loss,y)))\n",
    "    print(sess.run(tf.gradients(loss,x)/w))\n",
    "    print(sess.run(tf.gradients(y0,x)))\n",
    "    print(grads)\n",
    "    print(sess.run(grads))\n",
    "    print(sess.run(quantize(np.array([0.0000021,.0000021,.0000021]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5 -1.4 -1.3 -1.2 -1.1 -1.  -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2\n",
      " -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.   1.1  1.2\n",
      "  1.3  1.4  1.5]\n",
      "target_level should be a odd number\n",
      "4.0\n",
      "[-1.5  -1.5  -1.25 -1.25 -1.   -1.   -1.   -0.75 -0.75 -0.5  -0.5  -0.5\n",
      " -0.25 -0.25  0.    0.    0.    0.25  0.25  0.5   0.5   0.5   0.75  0.75\n",
      "  1.    1.    1.    1.25  1.25  1.5   1.5 ]\n",
      "[-0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.7  -0.6  -0.5  -0.4\n",
      " -0.3  -0.2  -0.1   0.    0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.75\n",
      "  0.75  0.75  0.75  0.75  0.75  0.75  0.75]\n",
      "target_level should be a odd number\n",
      "4.0\n",
      "[-0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.5  -0.5  -0.5\n",
      " -0.25 -0.25  0.    0.    0.    0.25  0.25  0.5   0.5   0.5   0.75  0.75\n",
      "  0.75  0.75  0.75  0.75  0.75  0.75  0.75]\n",
      "*********************************\n",
      "target_level should be a even number\n",
      "[-1.66666667 -1.66666667 -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -0.33333333 -0.33333333 -0.33333333\n",
      " -0.33333333 -0.33333333 -0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333  1.          1.\n",
      "  1.          1.          1.          1.          1.          1.66666667\n",
      "  1.66666667]\n",
      "[-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.4 -0.3 -0.2\n",
      " -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5  0.5  0.5]\n",
      "target_level should be a even number\n",
      "[-0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333\n",
      " -0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333\n",
      " -0.33333333 -0.33333333 -0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def scalebybits(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "\n",
    "def quantize(x, target_level=7):\n",
    "    assert np.floor(target_level)==target_level,\"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    # shift=shiftbyval(tf.abs(x))  #We don't shift x in this function, this function is only for quantizatizing any input.\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        # 15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        y = x\n",
    "    elif bits == 1:  # BNN\n",
    "        print(\"The target_level=1 and we use BNN\")\n",
    "        y = tf.sign(x)\n",
    "    elif target_level%2==1:\n",
    "        print('target_level should be a odd number')\n",
    "        print(SCALE)\n",
    "        y = tf.round(x * SCALE) / SCALE\n",
    "        \n",
    "    else:\n",
    "        print('target_level should be a even number')\n",
    "        y = tf.round((x * 0.5 + 0.5)*(target_level-1))/(target_level-1)\n",
    "        y = 2*y-1\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "def clip(x,target_level=7):\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    if bits > 15 or bits == 1:\n",
    "        delta = 0.\n",
    "    else:\n",
    "        delta = 1. / SCALE\n",
    "    MAX = +1 - delta\n",
    "    MIN = -1 + delta\n",
    "    y = tf.clip_by_value(x, MIN, MAX, name='saturate')\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "x=np.array([0.1*i for i in range(-15,16)])\n",
    "print(x)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(quantize(x)))\n",
    "    print(sess.run(clip(x)))\n",
    "    print(sess.run(quantize(clip(x))))\n",
    "    print('*********************************')\n",
    "    print(sess.run(quantize(x,4)))\n",
    "    print(sess.run(clip(x,4)))\n",
    "    print(sess.run(quantize(clip(x,4),4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def make_index(x,ref):\n",
    "    leng = len(ref)\n",
    "    weight_shape = x.get_shape().as_list()\n",
    "    index = tf.ones(shape=weight_shape, dtype=tf.int32)\n",
    "    for i in range(leng):\n",
    "        level = tf.ones(shape=weight_shape, dtype=tf.int32) * (leng - i + 1)\n",
    "        mask = tf.logical_and(x>=ref[leng-i-1],tf.equal(index,1))\n",
    "        index = tf.where(mask,level,index)\n",
    "    return index\n",
    "x=tf.constant([10,20,30,40,50,60,70,80,90,100])\n",
    "ref=np.array([15,25,35,45,55,65,75,85,95])\n",
    "result=make_index(x,ref)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))\n",
    "#텐서에게 있어서 == 과 tf.equal은 다르다.\n",
    "print(True and True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-921a9c0c085b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaircase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-921a9c0c085b>\u001b[0m in \u001b[0;36mstaircase\u001b[0;34m(x, ref)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstaircase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mleng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def staircase(x,ref):\n",
    "    leng=len(ref)\n",
    "    if x<ref[0]:\n",
    "        y=1\n",
    "    elif x>=ref[-1]:\n",
    "        y=leng+1\n",
    "    else:\n",
    "        for i in range(leng-1):\n",
    "            if x<ref[i+1] and x>=ref[i]:\n",
    "                y=i+2\n",
    "                break\n",
    "    return y\n",
    "print(staircase(np.array([10,20]),ref))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'strided_slice_1/_assign:0' shape=(2, 3) dtype=float32_ref>, <tf.Tensor 'strided_slice_3/_assign:0' shape=(2, 3) dtype=float32_ref>, <tf.Tensor 'strided_slice_5/_assign:0' shape=(2, 3) dtype=float32_ref>, <tf.Tensor 'strided_slice_7/_assign:0' shape=(2, 3) dtype=float32_ref>, <tf.Tensor 'strided_slice_9/_assign:0' shape=(2, 3) dtype=float32_ref>, <tf.Tensor 'strided_slice_11/_assign:0' shape=(2, 3) dtype=float32_ref>]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "def staircase(x,ref):\n",
    "    leng=len(ref)\n",
    "    y=tf.cond(x<ref[0],lambda: 1,lambda: 0)\n",
    "    for i in range(leng-1):\n",
    "        y+=tf.cond(tf.logical_and(tf.greater_equal(x,ref[i]),tf.less(x,ref[i+1])),lambda: i+2, lambda: 0)\n",
    "    y+=tf.cond(x>ref[-1],lambda: leng+1,lambda: 0)\n",
    "    return tf.to_float(y)\n",
    "\n",
    "x=tf.constant([[10,20,30],[40,50,60]])\n",
    "ref=np.array([15,25,35,45,55])\n",
    "\n",
    "index=tf.Variable(initial_value=tf.zeros([2,3]),dtype=tf.float32,trainable=False)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        y_one=staircase(x[i,j],ref)\n",
    "        op=tf.assign(index[i,j],y_one)\n",
    "        tf.add_to_collection('op',op)\n",
    "op_list=tf.get_collection('op')\n",
    "with tf.control_dependencies(op_list):\n",
    "    index=tf.identity(index)\n",
    "print(op_list)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(index))\n",
    "    sess.run(op_list)\n",
    "    print(sess.run(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "yt_one=np.array([[1,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0]])\n",
    "y=np.array([[0.9,0.1,0,0,0,0,0,0,0,0],[0.7,0.2,0.1,0,0,0,0,0,0,0]])\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=yt_one, logits=y), name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax=tf.nn.softmax(logits=y)\n",
    "cross_loss = tf.reduce_mean(-tf.reduce_sum(yt_one * tf.log(softmax+0.00000001),axis=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_loss = tf.reduce_mean(-tf.reduce_sum(yt_one * tf.log(softmax+0.00000001),axis=[1]))\n",
    "max_index=tf.arg_max(y,dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9142707568233535\n",
      "1.6381643662417984\n",
      "[7 0]\n",
      "['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n",
      "(2, 1)\n",
      "[[ 0.9  0.1  0.   0.   0.   0.   0.  11.   0.   0. ]\n",
      " [ 0.7  0.2  0.1  0.   0.   0.   0.   0.   0.   0. ]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(loss))\n",
    "    print(sess.run(cross_loss))\n",
    "    print(sess.run(max_index))\n",
    "    \n",
    "count_confidence=np.zeros(shape=[2,10])\n",
    "count_correct=np.zeros(shape=[2])\n",
    "count_error = np.zeros(shape=[2])\n",
    "a=np.array([[1],[0]])\n",
    "print(dir(a))\n",
    "print(a.shape)\n",
    "print(y)\n",
    "# count_confidence[[1,0,1],:]+=y\n",
    "print(1!=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "[[4.000e+00]\n",
      " [6.666e+03]\n",
      " [4.000e+00]\n",
      " [6.666e+03]]\n",
      "[[0.9 0.1 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "yt_one=np.array([[1,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0]])\n",
    "y=np.array([[0.9,0.1,0,0,0,0,0,0,0,0],[0.7,0.2,0.1,0,0,0,0,0,0,0]])\n",
    "count_confidence=np.zeros(shape=[2,10])\n",
    "count_correct=np.zeros(shape=[2,1])\n",
    "count_error = np.zeros(shape=[2,1])\n",
    "print(y.shape)\n",
    "count_correct[np.array([[1],[0],[1],[0]]),0]+=[[2],[3],[4],[6666]]\n",
    "print(count_correct[np.array([[1],[0],[1],[0]]),0])\n",
    "a=np.array([[True],[False]])\n",
    "b=a\n",
    "print(a*b*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        1.         1.5       ]\n",
      " [0.33333333 0.66666667 1.        ]\n",
      " [0.5        0.75       1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([[1,2,3],[1,2,3],[2,3,4]])/np.array([[2],[3],[4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "55"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "211px",
    "left": "990.5px",
    "right": "8px",
    "top": "26px",
    "width": "416px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
