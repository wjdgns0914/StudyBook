{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3376001  -0.43742612 -0.4318817   0.43456566 -0.32312432]\n",
      "[(array([ -0.26495996,  -2.2998817 ,  -5.1547747 ,  -3.61878   ,\n",
      "       -13.231243  ], dtype=float32), array([ 0.3376001 , -0.43742612, -0.4318817 ,  0.43456566, -0.32312432],\n",
      "      dtype=float32))]\n",
      "[ 0.3376001  -0.43742612 -0.4318817   0.43456566 -0.32312432] \n",
      " [ 0.33786508 -0.43512625 -0.4267269   0.43818444 -0.30989307] \n",
      " [0.00026497 0.00229988 0.00515479 0.00361878 0.01323125]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "y_true=tf.constant([1,2.,3,4,5])\n",
    "x=tf.constant([1,2,3,4.,5])\n",
    "w=tf.get_variable(name='w',shape=[5,],initializer=tf.contrib.layers.xavier_initializer(),)\n",
    "y=w*x\n",
    "loss=tf.reduce_mean(tf.pow(y-y_true,2))\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "grads = opt.compute_gradients(loss)\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    w1=sess.run(w)\n",
    "    print(w1)\n",
    "    print(sess.run(grads))\n",
    "    sess.run(apply_gradient_op)\n",
    "    w2=sess.run(w)\n",
    "    print(w1,'\\n',w2,'\\n',w2-w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999896315728952\n",
      "2.1153662\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.log(2.718))\n",
    "def Shift(x):\n",
    "  return 2 ** tf.round(tf.log(x) / tf.log(2.0))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.log(4.333)/tf.log(2.)))   #log2(4.333)=log(4.333)/log(2)\n",
    "    print(sess.run(Shift(7.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shift(x):\n",
    "    return 2**np.round(np.log(x)/np.log(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "1.4142135\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "x=[i*0.000001 for i in range(4000001,8000000)]\n",
    "print((Shift(x)).min())\n",
    "print((np.array(x)/Shift(x)).max())\n",
    "print(np.sqrt(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n"
     ]
    }
   ],
   "source": [
    "x=[i*0.01 for i in range(401,800)]\n",
    "print((Shift(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n"
     ]
    }
   ],
   "source": [
    "print(np.round(np.sqrt(2)*11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def scalebybits(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "\n",
    "def shiftbyval(x):\n",
    "    return 2.0 ** tf.round(tf.log(x) / tf.log(2.0))\n",
    "\n",
    "def quantize(x, target_level=2**8):\n",
    "    assert np.floor(target_level)==target_level,\"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    \n",
    "    # shift=shiftbyval(tf.abs(x))  #We don't shift x in this function, this function is only for quantizatizing any input.\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        # 15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        y = x\n",
    "    elif bits == 1:  # BNN\n",
    "        print(\"The target_level=1 and we use BNN\")\n",
    "        y = tf.sign(x)\n",
    "    elif target_level%2==1:\n",
    "        print('target_level is a odd number')\n",
    "        y = tf.round(x * tf.floor(target_level/2))/ SCALE\n",
    "        #20180410 \n",
    "        #레벨 수는 맞추되, 그 레벨의 값이 2의 승수로 만든다.\n",
    "        #다만 이 부분에서 확인이 안된건 weight 값에 따라 학습결과가 달라지나 안달라지나를 확인해야한다.\n",
    "        #-0.5, 0, 0.5와 -1, 0, 1의 학습결과가 달라지나 안달라지나?\n",
    "        #BatchNorm쓰면 별로 안달리질거고, WAGE에서는 scale layer를 쓰는데 그걸 여기 구현하지는 않았다.\n",
    "    else:\n",
    "        print('target_level is a even number')\n",
    "        SCALE=SCALE*2\n",
    "        \n",
    "        y = (tf.sign(x)*(2*tf.round(tf.abs(x)*(target_level/2)+0.5)-1)+tf.to_float(tf.equal(x,0)))/SCALE\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "\n",
    "def clip(x,target_level=2**8):\n",
    "    #이게 왜 필요하냐?\n",
    "    #레벨 수가 몇이든 간에 결국 해당하는 bits의 2의 승수배로 weight 값을 주게된다.\n",
    "    #그니까 그 최댓값을 넘어가는 값은 다 최댓값으로 맵핑시킬 필요가 있다.\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    if bits > 15 or bits == 1:\n",
    "        delta = 0.\n",
    "    else:\n",
    "        delta = 1. / SCALE\n",
    "    if target_level%2==1:\n",
    "        limit=SCALE-tf.floor(target_level)\n",
    "    else:\n",
    "        limit=SCALE-target_level+1\n",
    "    MAX = +1 - limit*delta\n",
    "    MIN = -1 + limit*delta\n",
    "    y = tf.clip_by_value(x, MIN, MAX, name='saturate')\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "\n",
    "def quantize_G(x,target_level=2**8):\n",
    "    bitsG = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsG)\n",
    "    with tf.name_scope('Quantize_G'):\n",
    "        if bitsG > 15:\n",
    "            return x\n",
    "        else:\n",
    "            if x.name.lower().find('batchnorm') > -1:\n",
    "                return x  # batch norm parameters, not quantize now\n",
    "            xmax = tf.reduce_max(tf.abs(x))\n",
    "            y = x / shiftbyval(xmax)\n",
    "            norm = quantize(LR * y, 4095)\n",
    "            norm_sign = tf.sign(norm)\n",
    "            norm_abs = tf.abs(norm)\n",
    "            norm_int = tf.floor(norm_abs)\n",
    "            norm_float = norm_abs - norm_int\n",
    "            rand_float = tf.random_uniform(x.get_shape(), 0, 1)\n",
    "            norm = norm_sign * (norm_int + 0.5 * (tf.sign(norm_float - rand_float) + 1))\n",
    "            return norm / SCALE\n",
    "\n",
    "def quantize_W(x,target_level=2**8):\n",
    "    bitsW = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsW)\n",
    "    with tf.name_scope('Quantize_W'):\n",
    "        if bitsW > 15:\n",
    "            return x\n",
    "        else:\n",
    "            xmax = tf.to_float(tf.reduce_max(tf.abs(x)))\n",
    "            print(xmax)\n",
    "            y = x / shiftbyval(xmax)\n",
    "#             y = clip(quantize(y, target_level), target_level)\n",
    "            y = quantize(clip(y, target_level), target_level)\n",
    "            #20180410: clipr과 quantize의 적용 순서에 대한 명확한 이해가 없다.\n",
    "            # we scale W in QW rather than QA for simplicity\n",
    "            return x + tf.stop_gradient(y - x)  # skip derivation of Quantize and Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x:\n",
      " [-0.0012 -0.001  -0.0008 -0.0006 -0.0004 -0.0002  0.      0.0002  0.0004\n",
      "  0.0006  0.0008  0.001   0.0012]\n",
      "0.0012\n",
      "0.0009765625\n",
      "\n",
      "Quantize x to 6 levels:\n",
      "target_level is a even number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "  temp_quantize: [-0.875 -0.875 -0.625 -0.375 -0.375 -0.125  0.125  0.125  0.375  0.375\n",
      "  0.625  0.875  0.875]\n",
      "Level index_even: [-4. -4. -3. -2. -2. -1.  0.  1.  2.  2.  3.  4.  4.]\n",
      "Level index_odd: [-3.6864002  -3.072      -2.4575999  -1.8432001  -1.2287999  -0.61439997\n",
      "  0.          0.61439997  1.2287999   1.8432001   2.4575999   3.072\n",
      "  3.6864002 ]\n",
      "Tensor(\"Quantize_W_261/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a even number\n",
      "target_level is a odd number\n",
      "     quantize_W: [-0.875 -0.875 -0.625 -0.375 -0.375 -0.125  0.125  0.125  0.375  0.375\n",
      "  0.625  0.875  0.875]\n",
      "     quantize_G: [-0.25 -0.25 -0.   -0.25 -0.   -0.    0.    0.    0.    0.    0.25  0.25\n",
      "  0.25]\n",
      "\n",
      "Quantize x to 11 levels:\n",
      "target_level is a odd number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "  temp_quantize: [-0.75  -0.625 -0.5   -0.375 -0.25  -0.125  0.     0.125  0.25   0.375\n",
      "  0.5    0.625  0.75 ]\n",
      "Level index_even: [-7. -6. -5. -4. -3. -2.  0.  2.  3.  4.  5.  6.  7.]\n",
      "Level index_odd: [-6.144     -5.1200004 -4.0959997 -3.072     -2.0479999 -1.0239999\n",
      "  0.         1.0239999  2.0479999  3.072      4.0959997  5.1200004\n",
      "  6.144    ]\n",
      "Tensor(\"Quantize_W_262/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a odd number\n",
      "target_level is a odd number\n",
      "     quantize_W: [-0.75  -0.625 -0.5   -0.375 -0.25  -0.125  0.     0.125  0.25   0.375\n",
      "  0.5    0.625  0.75 ]\n",
      "     quantize_G: [-0.125 -0.125 -0.125 -0.125 -0.125 -0.     0.     0.     0.     0.125\n",
      "  0.125  0.125  0.25 ]\n",
      "\n",
      "Quantize x to 12 levels:\n",
      "target_level is a even number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "  temp_quantize: [-0.9375 -0.8125 -0.5625 -0.4375 -0.3125 -0.1875  0.0625  0.1875  0.3125\n",
      "  0.4375  0.5625  0.8125  0.9375]\n",
      "Level index_even: [-8. -7. -5. -4. -3. -2.  0.  2.  3.  4.  5.  7.  8.]\n",
      "Level index_odd: [-7.3728004 -6.144     -4.9151998 -3.6864002 -2.4575999 -1.2287999\n",
      "  0.         1.2287999  2.4575999  3.6864002  4.9151998  6.144\n",
      "  7.3728004]\n",
      "Tensor(\"Quantize_W_263/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a even number\n",
      "target_level is a odd number\n",
      "     quantize_W: [-0.9375 -0.8125 -0.5625 -0.4375 -0.3125 -0.1875  0.0625  0.1875  0.3125\n",
      "  0.4375  0.5625  0.8125  0.9375]\n",
      "     quantize_G: [-0.125 -0.125 -0.125 -0.125 -0.    -0.125  0.     0.     0.     0.\n",
      "  0.     0.125  0.25 ]\n",
      "\n",
      "Quantize x to 15 levels:\n",
      "target_level is a odd number\n",
      "    temp_scaled: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "      temp_clip: [-1.2288 -1.024  -0.8192 -0.6144 -0.4096 -0.2048  0.      0.2048  0.4096\n",
      "  0.6144  0.8192  1.024   1.2288]\n",
      "  temp_quantize: [-1.125 -0.875 -0.75  -0.5   -0.375 -0.125  0.     0.125  0.375  0.5\n",
      "  0.75   0.875  1.125]\n",
      "Level index_even: [-10.  -8.  -7.  -5.  -4.  -2.   0.   2.   4.   5.   7.   8.  10.]\n",
      "Level index_odd: [-8.601601  -7.168     -5.7344    -4.3008003 -2.8672    -1.4336\n",
      "  0.         1.4336     2.8672     4.3008003  5.7344     7.168\n",
      "  8.601601 ]\n",
      "Tensor(\"Quantize_W_264/Max:0\", shape=(), dtype=float32)\n",
      "target_level is a odd number\n",
      "target_level is a odd number\n",
      "     quantize_W: [-1.125 -0.875 -0.75  -0.5   -0.375 -0.125  0.     0.125  0.375  0.5\n",
      "  0.75   0.875  1.125]\n",
      "     quantize_G: [-0.125 -0.125 -0.125 -0.    -0.125 -0.125  0.     0.     0.     0.\n",
      "  0.125  0.125  0.125]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def scalebybits(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "\n",
    "\n",
    "def shiftbyval(x):\n",
    "    return 2.0 ** tf.round(tf.log(x) / tf.log(2.0))\n",
    "\n",
    "\n",
    "def quantize(x, target_level=2 ** 8):\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bits)\n",
    "\n",
    "    # shift=shiftbyval(tf.abs(x))  #We don't shift x in this function, this function is only for quantizatizing any input.\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        # 15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        y = x\n",
    "    elif bits == 1:  # BNN\n",
    "        print(\"The target_level=1 and we use BNN\")\n",
    "        y = tf.sign(x)\n",
    "    elif target_level % 2 == 1:\n",
    "        print('target_level is a odd number')\n",
    "        y = tf.round(x * tf.floor(target_level / 2)) / SCALE\n",
    "        # 20180410\n",
    "        # 레벨 수는 맞추되, 그 레벨의 값이 2의 승수로 만든다.\n",
    "        # 다만 이 부분에서 확인이 안된건 weight 값에 따라 학습결과가 달라지나 안달라지나를 확인해야한다.\n",
    "        # -0.5, 0, 0.5와 -1, 0, 1의 학습결과가 달라지나 안달라지나?\n",
    "        # BatchNorm쓰면 별로 안달리질거고, WAGE에서는 scale layer를 쓰는데 그걸 여기 구현하지는 않았다.\n",
    "    else:\n",
    "        print('target_level is a even number')\n",
    "        SCALE = SCALE * 2\n",
    "        y = (tf.sign(x)*(2*tf.ceil(tf.abs(x)*(target_level/2))-1)+\n",
    "             tf.to_float(tf.equal(x,0))*tf.sign(tf.random_uniform(x.get_shape(),-1.,1.)))/SCALE\n",
    "    return tf.stop_gradient(y - x) + x\n",
    "\n",
    "\n",
    "def clip(x, target_level=2 ** 8):\n",
    "    # 이게 왜 필요하냐?\n",
    "    # 레벨 수가 몇이든 간에 결국 해당하는 bits의 2의 승수배로 weight 값을 주게된다.\n",
    "    # 그니까 그 최댓값을 넘어가는 값은 다 최댓값으로 맵핑시킬 필요가 있다.\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bits)\n",
    "    if bits > 15 or bits == 1:\n",
    "        delta = 0.\n",
    "    elif target_level % 2 == 1:\n",
    "        limit = SCALE - tf.floor(target_level)\n",
    "        delta = 1. / SCALE\n",
    "    else:\n",
    "        limit = SCALE*2 - target_level + 1\n",
    "        delta = 1. / (SCALE*2)\n",
    "    MAX = +1 #- limit * delta\n",
    "    MIN = -1 #+ limit * delta\n",
    "    y = tf.clip_by_value(x, MIN, MAX, name='saturate')\n",
    "    return tf.stop_gradient(y - x) + x\n",
    "\n",
    "\n",
    "def quantize_G(x, target_level=2 ** 8):\n",
    "    bitsG = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsG)\n",
    "    with tf.name_scope('Quantize_G'):\n",
    "        if bitsG > 15:\n",
    "            return x\n",
    "        else:\n",
    "            if x.name.lower().find('batchnorm') > -1:\n",
    "                return x  # batch norm parameters, not quantize now\n",
    "            xmax = tf.reduce_max(tf.abs(x))\n",
    "            y = x / shiftbyval(xmax)\n",
    "            norm = quantize(LR * y, 4095)\n",
    "            norm_sign = tf.sign(norm)\n",
    "            norm_abs = tf.abs(norm)\n",
    "            norm_int = tf.floor(norm_abs)\n",
    "            norm_float = norm_abs - norm_int\n",
    "            rand_float = tf.random_uniform(x.get_shape(), 0, 1)\n",
    "            norm = norm_sign * (norm_int + 0.5 * (tf.sign(norm_float - rand_float) + 1))\n",
    "            return norm / SCALE\n",
    "\n",
    "\n",
    "def quantize_W(x, target_level=2 ** 8):\n",
    "    bitsW = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE = scalebybits(bitsW)\n",
    "    with tf.name_scope('Quantize_W'):\n",
    "        if bitsW > 15:\n",
    "            return x\n",
    "        else:\n",
    "            xmax = tf.to_float(tf.reduce_max(tf.abs(x)))\n",
    "            print(xmax)\n",
    "            y = x / shiftbyval(xmax)\n",
    "            #             y = clip(quantize(y, target_level), target_level)\n",
    "            y = quantize(clip(y, target_level), target_level)\n",
    "            # 20180410: clipr과 quantize의 적용 순서에 대한 명확한 이해가 없다.\n",
    "            # we scale W in QW rather than QA for simplicity\n",
    "            return x + tf.stop_gradient(y - x)  # skip derivation of Quantize and Clip\n",
    "\n",
    "LR=1\n",
    "target_level_list=[6.,11.,12.]\n",
    "x=tf.constant([0.0002*i for i in range(-6,7)],name='Const')\n",
    "with tf.Session() as sess:\n",
    "    print(\"Original x:\\n\",sess.run(x))\n",
    "    print(sess.run(tf.to_float(tf.reduce_max(tf.abs(x)))))\n",
    "    print(sess.run(shiftbyval(tf.to_float(tf.reduce_max(tf.abs(x))))))\n",
    "    temp_scaled=x/shiftbyval(tf.to_float(tf.reduce_max(tf.abs(x))))\n",
    "\n",
    "    for target_level in target_level_list:\n",
    "        print(\"\\nQuantize x to %d levels:\"%target_level)\n",
    "        temp_clip=clip(temp_scaled,target_level)\n",
    "        temp_quantize=quantize(temp_scaled,target_level)\n",
    "        print(\"%15s:\"%\"temp_scaled\",sess.run(temp_scaled))\n",
    "        print(\"%15s:\"%\"temp_clip\",sess.run(temp_clip))\n",
    "        print(\"%15s:\"%\"temp_quantize\",sess.run(temp_quantize))\n",
    "        print(sess.run(tf.abs(temp_clip)*(target_level/2)))\n",
    "        x1=quantize_W(x,target_level)\n",
    "        # x2=quantize_G(x,target_level)\n",
    "        print(\"%15s:\"%\"quantize_W\",sess.run(x1))\n",
    "        print(sess.run(tf.sign(tf.random_uniform([1],-1.,1.))))\n",
    "        # print(\"%15s:\"%\"quantize_G\",sess.run(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to keyword (<ipython-input-60-31d8e0c7cda5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-31d8e0c7cda5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    a= None = 3\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to keyword\n"
     ]
    }
   ],
   "source": [
    "# print(open('Option.py').read())\n",
    "a= None = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.46893153, -0.8372386 , -1.1603353 , -1.5564557 , -2.1467435 ],\n",
      "      dtype=float32)]\n",
      "[[-0.46893153 -0.8372386  -1.1603353  -1.5564557  -2.1467435 ]]\n",
      "[array([1., 1., 1., 1., 1.], dtype=float32)]\n",
      "[(<tf.Tensor 'gradients/mul_1_grad/tuple/control_dependency:0' shape=(5,) dtype=float32>, <tf.Variable 'w:0' shape=(5,) dtype=float32_ref>)]\n",
      "[(array([-0.11723288, -0.20930965, -0.58016765, -0.77822787, -1.6100576 ],\n",
      "      dtype=float32), array([-0.68931526, -0.37238613,  0.19832414,  0.21772164, -0.48914415],\n",
      "      dtype=float32))]\n",
      "[0.75 0.75 0.75]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "def scale_bit(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "def quantize(x, target_level=7):\n",
    "    assert target_level%2==1,'target_level should be a odd number'\n",
    "    y=x / tf.reduce_max(tf.abs(x))\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        #15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        return x\n",
    "    elif bits == 1:  # BNN\n",
    "        return tf.sign(x)\n",
    "    else:\n",
    "        y = tf.to_float(tf.clip_by_value(y, -1.0, 1.0, name='saturate'))\n",
    "        y = tf.round(y*tf.floor(target_level/2))/scale_bit(bits)\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "\n",
    "y_true=tf.constant([1,2.,3,4,5])\n",
    "x=tf.constant([1,2,3,4.,5])\n",
    "w=tf.get_variable(name='w',shape=[5,],initializer=tf.contrib.layers.xavier_initializer(),)\n",
    "y0=quantize(x)\n",
    "y=w*y0\n",
    "loss=tf.reduce_mean(tf.pow(y-y_true,2))\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "grads = opt.compute_gradients(loss)\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(tf.gradients(loss,y)))\n",
    "    print(sess.run(tf.gradients(loss,x)/w))\n",
    "    print(sess.run(tf.gradients(y0,x)))\n",
    "    print(grads)\n",
    "    print(sess.run(grads))\n",
    "    print(sess.run(quantize(np.array([0.0000021,.0000021,.0000021]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5 -1.4 -1.3 -1.2 -1.1 -1.  -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2\n",
      " -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.   1.1  1.2\n",
      "  1.3  1.4  1.5]\n",
      "target_level should be a odd number\n",
      "4.0\n",
      "[-1.5  -1.5  -1.25 -1.25 -1.   -1.   -1.   -0.75 -0.75 -0.5  -0.5  -0.5\n",
      " -0.25 -0.25  0.    0.    0.    0.25  0.25  0.5   0.5   0.5   0.75  0.75\n",
      "  1.    1.    1.    1.25  1.25  1.5   1.5 ]\n",
      "[-0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.7  -0.6  -0.5  -0.4\n",
      " -0.3  -0.2  -0.1   0.    0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.75\n",
      "  0.75  0.75  0.75  0.75  0.75  0.75  0.75]\n",
      "target_level should be a odd number\n",
      "4.0\n",
      "[-0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.75 -0.5  -0.5  -0.5\n",
      " -0.25 -0.25  0.    0.    0.    0.25  0.25  0.5   0.5   0.5   0.75  0.75\n",
      "  0.75  0.75  0.75  0.75  0.75  0.75  0.75]\n",
      "*********************************\n",
      "target_level should be a even number\n",
      "[-1.66666667 -1.66666667 -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -0.33333333 -0.33333333 -0.33333333\n",
      " -0.33333333 -0.33333333 -0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333  1.          1.\n",
      "  1.          1.          1.          1.          1.          1.66666667\n",
      "  1.66666667]\n",
      "[-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.4 -0.3 -0.2\n",
      " -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  0.5  0.5  0.5]\n",
      "target_level should be a even number\n",
      "[-0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333\n",
      " -0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333 -0.33333333\n",
      " -0.33333333 -0.33333333 -0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def scalebybits(bits):\n",
    "    return 2.0 ** (bits - 1)\n",
    "\n",
    "def quantize(x, target_level=7):\n",
    "    assert np.floor(target_level)==target_level,\"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    # shift=shiftbyval(tf.abs(x))  #We don't shift x in this function, this function is only for quantizatizing any input.\n",
    "    if bits > 15:\n",
    "        print(\"The target_level is to high, We don't quantize this.\")\n",
    "        # 15비트 넘어가면 양자화 안하겠다는건데, 나름 괜찮은 선택 같다, 그 위는 양자화해도 크게 득도 없고 아래에서 결판나니까\n",
    "        y = x\n",
    "    elif bits == 1:  # BNN\n",
    "        print(\"The target_level=1 and we use BNN\")\n",
    "        y = tf.sign(x)\n",
    "    elif target_level%2==1:\n",
    "        print('target_level should be a odd number')\n",
    "        print(SCALE)\n",
    "        y = tf.round(x * SCALE) / SCALE\n",
    "        \n",
    "    else:\n",
    "        print('target_level should be a even number')\n",
    "        y = tf.round((x * 0.5 + 0.5)*(target_level-1))/(target_level-1)\n",
    "        y = 2*y-1\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "def clip(x,target_level=7):\n",
    "    assert np.floor(target_level) == target_level, \"Target_level should be a integer\"\n",
    "    bits = np.ceil(np.log(target_level) / np.log(2.0))\n",
    "    SCALE=scalebybits(bits)\n",
    "    if bits > 15 or bits == 1:\n",
    "        delta = 0.\n",
    "    else:\n",
    "        delta = 1. / SCALE\n",
    "    MAX = +1 - delta\n",
    "    MIN = -1 + delta\n",
    "    y = tf.clip_by_value(x, MIN, MAX, name='saturate')\n",
    "    return tf.stop_gradient(y-x)+x\n",
    "x=np.array([0.1*i for i in range(-15,16)])\n",
    "print(x)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(quantize(x)))\n",
    "    print(sess.run(clip(x)))\n",
    "    print(sess.run(quantize(clip(x))))\n",
    "    print('*********************************')\n",
    "    print(sess.run(quantize(x,4)))\n",
    "    print(sess.run(clip(x,4)))\n",
    "    print(sess.run(quantize(clip(x,4),4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "55"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
