{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 1.TensorBoard 사용법과 히스토그램의 의미                \n",
    "### 2.내장 함수                                           #함수를 중심으로 다룬다\n",
    "###   2.1 enumerate, reshape,plt.imshow,Filter of CNN\n",
    "###   2.2 range, append\n",
    "###   2.3 dir:메서드를 보여준다\n",
    "###   2.4 np.eye, hstack\n",
    "###   2.5 binomial 베르누이 분포, 확률분포\n",
    "### 3. 딥러닝 다루기                                      #딥러닝 혹은 텐서플로우만의 사상을 다룬다\n",
    "###   3.1 train tensor에서 기울기 수정하기\n",
    "###   3.2 Tensor에 대하여\n",
    "### 4 자잘한 지식들                                       #기능을 중심으로 다룬을 중심으로 다룬다\n",
    "###   4.1 placeholder, feed_dict, matmul, boradcasting \n",
    "###   4.2 데이터 읽고 슬라이싱,shape,len \n",
    "###   4.3 tf.map_fn\n",
    "###   4.4 np.random.choice\n",
    "사실 별의미는 없는 인덱싱이지만..그래도!\n",
    "1장에는 텐서플로우(그래프적인 것과 이것만의 특성에 관한 것)\n",
    "2장에는 파이썬 내장함수\n",
    "3장에는 텐서플로우에 관한 것 \n",
    "4장에는 뭔가 애매한 자잘한 것들?\n",
    "5장 딥러닝적인 내용(사실 여기에는 CNN 같은게 들어가야지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import를 해준다 특별한 설명은 하지않겠다.\n",
    "아래에서 보충이 필요한 부분은 ???로 검색 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.TensorBoard 및 Tensor Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 TensorBoard에 관하여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([200, 10], stddev=1.0))\n",
    "W2 = tf.Variable(tf.random_normal([200, 10], stddev=0.13))\n",
    "W3 = tf.Variable(tf.random_normal([1], stddev=10))\n",
    "\n",
    "w1_hist = tf.summary.histogram(\"weights-stdev_1.0\", W1)\n",
    "w2_hist = tf.summary.histogram(\"weights-stdev_0.13\", W2)\n",
    "w3_hist = tf.summary.histogram(\"My_weights_W3\", W3)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())#이때 초기화가 된다.\n",
    "\n",
    "for i in range(100):\n",
    "    writer.add_summary(sess.run(summary_op),i)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 텐서보드에서 히스토그램을 보는 법을 쉽게 알아보기 위해서 이미 아는 분포, 정규분포를 만들어서 텐서보드에 써본 것이다,다만 여기서는 for을 굳이 넣어서 같은 데이터를 100번 썼는데 그렇게 해야,global_step에 따른 히스토그램의 변화가 보이기 때문이다, 사실 이 글로벌스텝은 스텝에 따라 변하는 변수를 위한 것이기 때문에 위와 같이 한번 초기화하고 바꾸지도않는 분포에서는 별 필요가 없다, 그래서 아래처럼 쓸 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())#이때 초기화가 된다.\n",
    "\n",
    "writer.add_summary(sess.run(summary_op),1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른건 달라진게 없고 딱 for만 없애줬다, 이렇게 하면 딱 한번 기록되기 때문에 distribution에서는 보이지도 않는다. 참고로 텐서보드는 위와 같이 한 다음에 터미널에서 tensorboard --logdir=./logs를 입력해주면 된다. --logdir= 여기서 등호 오른쪽에는 텐서보드 파일이 있는 경로를 적어주면 된다.\n",
    "참고로 ./logs에서 ./은 상대경로라고 해야되나, 지금 파일이 저장된 폴더까지의 경로를 말해준다. 즉 저렇게 적으면 코드파일이 들어있는 폴더에 logs라는 폴더를 만들어서 그 안에 텐서보드 파일을 넣는다.\n",
    "-스칼라 사용법,텐서플로우(그래프)사용법을 작성하지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 what tf.identity is used for?, 그리고 Tensorflow에서 Tensor Graph에 관하여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_10:0' shape=(3,) dtype=int32_ref>\n",
      "<tf.Variable 'Variable_10:0' shape=(3,) dtype=int32_ref>\n",
      "[15]\n",
      "[1 2 3]\n",
      "===================We will try case1====================\n",
      "Tensor(\"Const_16:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Identity_14:0\", shape=(3,), dtype=int32)\n",
      "[15]\n",
      "[1 2 3]\n",
      "===================We will try case2====================\n",
      "<tf.Variable 'Variable_12:0' shape=(3,) dtype=int32_ref>\n",
      "Tensor(\"Identity_15:0\", shape=(3,), dtype=int32)\n",
      "[15]\n",
      "[1 2 3]\n",
      "===================We will try another case3====================\n",
      "<tf.Variable 'Variable_14:0' shape=(3,) dtype=int32_ref>\n",
      "Tensor(\"Identity_16:0\", shape=(3,), dtype=int32)\n",
      "[9 9 9]\n",
      "[9 9 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n아래의 코드실행결과에서 알 수 있듯이\\nconstant든 variable이든 identity는 그걸 복사하여 b에 넣어주었다. 그리고 a를 바꾸면 잘바뀐다\\n그런데\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.Variable([1,2,3])\n",
    "print(a)\n",
    "b=a\n",
    "print(b)\n",
    "a=tf.Variable([15])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "print(\"===================We will try case1====================\")\n",
    "a=tf.constant([1,2,3])\n",
    "print(a)\n",
    "b=tf.identity(a)\n",
    "print(b)\n",
    "a=tf.constant([15])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    \n",
    "print(\"===================We will try case2====================\")\n",
    "a=tf.Variable([1,2,3])\n",
    "print(a)\n",
    "b=tf.identity(a)\n",
    "print(b)\n",
    "a=tf.Variable([15])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "print(\"===================We will try another case3====================\")\n",
    "a=tf.Variable([1,2,3])\n",
    "print(a)\n",
    "b=tf.identity(a)\n",
    "print(b)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.assign(a,[9,9,9]))\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "\"\"\"\n",
    "아래의 코드실행결과에서 알 수 있듯이\n",
    "constant든 variable이든 identity는 그걸 복사하여 b에 넣어주었다. 그리고 a를 바꾸면 잘바뀐다\n",
    "그런데 사실 어차피 a에 아예 새로운 텐서를 넣을거면 그냥 b=a해도 잘된다. assign으로 바꿀거면 b=a하든 b=tf.identity(a)하든 똑같이 안된다\n",
    "흠..그러면 도대체 tf.identity의 역할은 무엇일까? 아래를 보자\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Return a tensor with the same shape and contents as the input tensor or value.\n",
    "2)복사한 중간 계산 결과 값을 이용하므로 네트워크의 부담을 줄일 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "3.0\n",
      "6.0\n",
      "9.0\n",
      "12.0\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(0.0)\n",
    "x_plus_1 = tf.assign_add(x, 1) # x_plus_1 = (x = x + 1)\n",
    "\n",
    "with tf.control_dependencies([x_plus_1]):\n",
    "    y = tf.identity(3*x)\n",
    "init = tf.global_variables_initializer()\n",
    "#tensorflow는 expression parser, 위에서 y는 아직 variable이 아니다.\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    for i in range(5):\n",
    "        print(y.eval())\n",
    "        # print(x_plus_1.eval()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러개의 Gpu,cpu가 서로 데이터를 주고 받을 때 쓰기도 한다. 하지만 내가 써놓은 예제와 아래 상황에서는 그저 flow를 조금 컨트롤하는 용도로 쓰인다.\n",
    "with tf.control_dependencies([loss_averages_op]): 안에서 아무것도 안하고 y=x 이런 식으로 해놓으면 flow control이 작동을 안한다고 한다, 무언가 텐서를 넣어줘야하고 그 적당한 텐서로 tf.identity를 쓰는 듯 하다.\n",
    "\n",
    "In addition, tf.identity can be used used as a dummy node to update a reference to the tensor. This is useful with various control flow ops. In the CIFAR case we want to enforce that the ExponentialMovingAverageOp will update relevant variables before retrieving the value of the loss. This can be implemented as:\n",
    "\n",
    "with tf.control_dependencies([loss_averages_op]):\n",
    "  total_loss = tf.identity(total_loss)\n",
    "\n",
    "Here, the tf.identity doesn't do anything useful aside of marking the total_loss tensor to be ran after evaluating loss_averages_op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2)\n",
    "new_x = x.assign_add(10)\n",
    "y = x\n",
    "#원래대로라면 y를 실행 할 때 new_x를 들릴 필요가 없다, 그런데 아래 두 줄을 이용해 y=x라는 연산이 일어나기 전에 꼭 new_x를 들리도록 강제한다.\n",
    "with tf.control_dependencies([new_x]):\n",
    "    y = tf.identity(x)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result = sess.run(y)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 tf.cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(2)\n",
    "y = tf.constant(5)\n",
    "def f1(): return tf.multiply(x, 17)\n",
    "def f2(): return tf.add(y, 23)\n",
    "r = tf.cond(tf.less(x, y), f1, f2)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    sess.run(r)\n",
    "    print(r.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 About tf.gradients() and RegisterGradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[3]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "인터넷에 가면 이에 대해 아래와 같은 설명이 있다\n",
    "gradients(\n",
    "    ys,\n",
    "    xs,\n",
    "    grad_ys=None,\n",
    "    name='gradients',\n",
    "    colocate_gradients_with_ops=False,\n",
    "    gate_gradients=False,\n",
    "    aggregation_method=None\n",
    ")\n",
    "ys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. \n",
    "The list must be the same length as ys.\n",
    "gradients() adds ops to the graph to output the partial derivatives of ys with respect to xs. \n",
    "It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys.\n",
    "grad_ys is a list of tensors of the same length as ys that holds the initial gradients for each y in ys. \n",
    "When grad_ys is None, we fill in a tensor of '1's of the shape of y for each y in ys. \n",
    "A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y \n",
    "(e.g., if one wanted to weight the gradient differently for each value in each y).\n",
    "\n",
    "즉 ys를 xs에 대하여 미분한 값을 반환하는 텐서이다. grad_ys는 미분값의 초기값, 따로 설정을 안하면 1로,\n",
    "미분값에 초기값이라는건 마지막에 이 초기값을 곱해서 최종결과를 얻는다는 것다.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "아래는 이거 정리하기 전에 내가 스크랩 해놨던 코드\n",
    "@tf.RegisterGradient(\"CustomGrad\")\n",
    "\n",
    "def _const_mul_grad(unused_op, grad):\n",
    "    return 5.0 * grad\n",
    "\n",
    "g = tf.get_default_graph()\n",
    "with g.gradient_override_map({\"Identity\": \"CustomGrad\"}):\n",
    "    output = tf.identity(input, name=\"Identity\")\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "a=tf.constant(3)\n",
    "b=3*a+2\n",
    "c=tf.gradients(b,a)\n",
    "d=tf.gradients(b,a,1)\n",
    "e=tf.gradients(b,a,2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))\n",
    "    print(sess.run(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 텐서 e를 실행한 결과에서 grad_ys가 dys/dxs 에 곱해졌다는걸 알 수 있다.\n",
    "즉 backpropagation을 할 때 전레이어까지의 propagation 결과를 이용 할 때 쓰는 녀석이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyfunc grad <function addone_grad at 0x7efb80356e18>\n",
      "step 0\n",
      "5.0, 0.10000000149011612\n",
      "step 0\n",
      "4.978000164031982, 0.07800000160932541\n",
      "step 1\n",
      "4.978000164031982, 0.07800000160932541\n",
      "step 1\n",
      "4.95688009262085, 0.05687999352812767\n",
      "step 2\n",
      "4.95688009262085, 0.05687999352812767\n",
      "step 2\n",
      "4.936604976654053, 0.03660479187965393\n",
      "step 3\n",
      "4.936604976654053, 0.03660479187965393\n",
      "step 3\n",
      "4.917140960693359, 0.017140593379735947\n",
      "step 4\n",
      "4.917140960693359, 0.017140593379735947\n",
      "step 4\n",
      "4.8984551429748535, -0.0015450343489646912\n",
      "step 5\n",
      "4.8984551429748535, -0.0015450343489646912\n",
      "step 5\n",
      "4.88051700592041, -0.019483238458633423\n",
      "step 6\n",
      "4.88051700592041, -0.019483238458633423\n",
      "step 6\n",
      "4.8632965087890625, -0.036703918129205704\n",
      "step 7\n",
      "4.8632965087890625, -0.036703918129205704\n",
      "step 7\n",
      "4.84676456451416, -0.053235769271850586\n",
      "step 8\n",
      "4.84676456451416, -0.053235769271850586\n",
      "step 8\n",
      "4.8308939933776855, -0.0691063404083252\n",
      "step 9\n",
      "4.8308939933776855, -0.0691063404083252\n",
      "step 9\n",
      "4.815658092498779, -0.0843420922756195\n",
      "step 10\n",
      "4.815658092498779, -0.0843420922756195\n",
      "step 10\n",
      "4.801031589508057, -0.09896841645240784\n",
      "step 11\n",
      "4.801031589508057, -0.09896841645240784\n",
      "step 11\n",
      "4.786990165710449, -0.11300967633724213\n",
      "step 12\n",
      "4.786990165710449, -0.11300967633724213\n",
      "step 12\n",
      "4.773510456085205, -0.1264892965555191\n",
      "step 13\n",
      "4.773510456085205, -0.1264892965555191\n",
      "step 13\n",
      "4.760570049285889, -0.1394297182559967\n",
      "step 14\n",
      "4.760570049285889, -0.1394297182559967\n",
      "step 14\n",
      "4.748147010803223, -0.1518525332212448\n",
      "step 15\n",
      "4.748147010803223, -0.1518525332212448\n",
      "step 15\n",
      "4.7362213134765625, -0.1637784242630005\n",
      "step 16\n",
      "4.7362213134765625, -0.1637784242630005\n",
      "step 16\n",
      "4.7247724533081055, -0.17522728443145752\n",
      "step 17\n",
      "4.7247724533081055, -0.17522728443145752\n",
      "step 17\n",
      "4.713781356811523, -0.18621818721294403\n",
      "step 18\n",
      "4.713781356811523, -0.18621818721294403\n",
      "step 18\n",
      "4.703229904174805, -0.19676944613456726\n",
      "step 19\n",
      "4.703229904174805, -0.19676944613456726\n",
      "step 19\n",
      "4.693100929260254, -0.20689865946769714\n",
      "step 20\n",
      "4.693100929260254, -0.20689865946769714\n",
      "step 20\n",
      "4.683376789093018, -0.2166227102279663\n",
      "step 21\n",
      "4.683376789093018, -0.2166227102279663\n",
      "step 21\n",
      "4.674041748046875, -0.22595779597759247\n",
      "step 22\n",
      "4.674041748046875, -0.22595779597759247\n",
      "step 22\n",
      "4.6650800704956055, -0.234919473528862\n",
      "step 23\n",
      "4.6650800704956055, -0.234919473528862\n",
      "step 23\n",
      "4.656476974487305, -0.24352268874645233\n",
      "step 24\n",
      "4.656476974487305, -0.24352268874645233\n",
      "step 24\n",
      "4.648217678070068, -0.25178176164627075\n",
      "step 25\n",
      "4.648217678070068, -0.25178176164627075\n",
      "step 25\n",
      "4.640288829803467, -0.25971049070358276\n",
      "step 26\n",
      "4.640288829803467, -0.25971049070358276\n",
      "step 26\n",
      "4.63267707824707, -0.2673220634460449\n",
      "step 27\n",
      "4.63267707824707, -0.2673220634460449\n",
      "step 27\n",
      "4.625370025634766, -0.2746291756629944\n",
      "step 28\n",
      "4.625370025634766, -0.2746291756629944\n",
      "step 28\n",
      "4.6183552742004395, -0.28164398670196533\n",
      "step 29\n",
      "4.6183552742004395, -0.28164398670196533\n",
      "step 29\n",
      "4.611620903015137, -0.28837820887565613\n",
      "step 30\n",
      "4.611620903015137, -0.28837820887565613\n",
      "step 30\n",
      "4.605155944824219, -0.29484304785728455\n",
      "step 31\n",
      "4.605155944824219, -0.29484304785728455\n",
      "step 31\n",
      "4.598949909210205, -0.30104929208755493\n",
      "step 32\n",
      "4.598949909210205, -0.30104929208755493\n",
      "step 32\n",
      "4.592991828918457, -0.3070073127746582\n",
      "step 33\n",
      "4.592991828918457, -0.3070073127746582\n",
      "step 33\n",
      "4.5872721672058105, -0.3127270042896271\n",
      "step 34\n",
      "4.5872721672058105, -0.3127270042896271\n",
      "step 34\n",
      "4.581781387329102, -0.3182179033756256\n",
      "step 35\n",
      "4.581781387329102, -0.3182179033756256\n",
      "step 35\n",
      "4.576509952545166, -0.3234891891479492\n",
      "step 36\n",
      "4.576509952545166, -0.3234891891479492\n",
      "step 36\n",
      "4.5714497566223145, -0.3285495936870575\n",
      "step 37\n",
      "4.5714497566223145, -0.3285495936870575\n",
      "step 37\n",
      "4.566591739654541, -0.33340761065483093\n",
      "step 38\n",
      "4.566591739654541, -0.33340761065483093\n",
      "step 38\n",
      "4.5619282722473145, -0.3380712866783142\n",
      "step 39\n",
      "4.5619282722473145, -0.3380712866783142\n",
      "step 39\n",
      "4.557451248168945, -0.3425484299659729\n",
      "step 40\n",
      "4.557451248168945, -0.3425484299659729\n",
      "step 40\n",
      "4.553153038024902, -0.34684649109840393\n",
      "step 41\n",
      "4.553153038024902, -0.34684649109840393\n",
      "step 41\n",
      "4.549026966094971, -0.35097262263298035\n",
      "step 42\n",
      "4.549026966094971, -0.35097262263298035\n",
      "step 42\n",
      "4.545065879821777, -0.3549337089061737\n",
      "step 43\n",
      "4.545065879821777, -0.3549337089061737\n",
      "step 43\n",
      "4.541263103485107, -0.3587363660335541\n",
      "step 44\n",
      "4.541263103485107, -0.3587363660335541\n",
      "step 44\n",
      "4.537612438201904, -0.36238691210746765\n",
      "step 45\n",
      "4.537612438201904, -0.36238691210746765\n",
      "step 45\n",
      "4.5341081619262695, -0.3658914268016815\n",
      "step 46\n",
      "4.5341081619262695, -0.3658914268016815\n",
      "step 46\n",
      "4.530743598937988, -0.36925575137138367\n",
      "step 47\n",
      "4.530743598937988, -0.36925575137138367\n",
      "step 47\n",
      "4.5275139808654785, -0.37248551845550537\n",
      "step 48\n",
      "4.5275139808654785, -0.37248551845550537\n",
      "step 48\n",
      "4.524413585662842, -0.3755860924720764\n",
      "step 49\n",
      "4.524413585662842, -0.3755860924720764\n",
      "step 49\n",
      "4.521437168121338, -0.3785626292228699\n",
      "step 50\n",
      "4.521437168121338, -0.3785626292228699\n",
      "step 50\n",
      "4.518579483032227, -0.3814201354980469\n",
      "step 51\n",
      "4.518579483032227, -0.3814201354980469\n",
      "step 51\n",
      "4.515836238861084, -0.3841633200645447\n",
      "step 52\n",
      "4.515836238861084, -0.3841633200645447\n",
      "step 52\n",
      "4.513202667236328, -0.386796772480011\n",
      "step 53\n",
      "4.513202667236328, -0.386796772480011\n",
      "step 53\n",
      "4.510674476623535, -0.3893248736858368\n",
      "step 54\n",
      "4.510674476623535, -0.3893248736858368\n",
      "step 54\n",
      "4.508247375488281, -0.39175185561180115\n",
      "step 55\n",
      "4.508247375488281, -0.39175185561180115\n",
      "step 55\n",
      "4.505917549133301, -0.3940817713737488\n",
      "step 56\n",
      "4.505917549133301, -0.3940817713737488\n",
      "step 56\n",
      "4.50368070602417, -0.3963184952735901\n",
      "step 57\n",
      "4.50368070602417, -0.3963184952735901\n",
      "step 57\n",
      "4.501533508300781, -0.39846575260162354\n",
      "step 58\n",
      "4.501533508300781, -0.39846575260162354\n",
      "step 58\n",
      "4.499472141265869, -0.40052711963653564\n",
      "step 59\n",
      "4.499472141265869, -0.40052711963653564\n",
      "step 59\n",
      "4.497493267059326, -0.402506023645401\n",
      "step 60\n",
      "4.497493267059326, -0.402506023645401\n",
      "step 60\n",
      "4.495593547821045, -0.40440577268600464\n",
      "step 61\n",
      "4.495593547821045, -0.40440577268600464\n",
      "step 61\n",
      "4.493769645690918, -0.40622952580451965\n",
      "step 62\n",
      "4.493769645690918, -0.40622952580451965\n",
      "step 62\n",
      "4.492018699645996, -0.4079803228378296\n",
      "step 63\n",
      "4.492018699645996, -0.4079803228378296\n",
      "step 63\n",
      "4.49033784866333, -0.40966108441352844\n",
      "step 64\n",
      "4.49033784866333, -0.40966108441352844\n",
      "step 64\n",
      "4.488724231719971, -0.41127461194992065\n",
      "step 65\n",
      "4.488724231719971, -0.41127461194992065\n",
      "step 65\n",
      "4.487175464630127, -0.4128236174583435\n",
      "step 66\n",
      "4.487175464630127, -0.4128236174583435\n",
      "step 66\n",
      "4.485688209533691, -0.41431066393852234\n",
      "step 67\n",
      "4.485688209533691, -0.41431066393852234\n",
      "step 67\n",
      "4.484260559082031, -0.41573822498321533\n",
      "step 68\n",
      "4.484260559082031, -0.41573822498321533\n",
      "step 68\n",
      "4.4828901290893555, -0.4171086847782135\n",
      "step 69\n",
      "4.4828901290893555, -0.4171086847782135\n",
      "step 69\n",
      "4.481574535369873, -0.4184243083000183\n",
      "step 70\n",
      "4.481574535369873, -0.4184243083000183\n",
      "step 70\n",
      "4.480311393737793, -0.41968733072280884\n",
      "step 71\n",
      "4.480311393737793, -0.41968733072280884\n",
      "step 71\n",
      "4.479098796844482, -0.42089980840682983\n",
      "step 72\n",
      "4.479098796844482, -0.42089980840682983\n",
      "step 72\n",
      "4.477934837341309, -0.42206379771232605\n",
      "step 73\n",
      "4.477934837341309, -0.42206379771232605\n",
      "step 73\n",
      "4.476817607879639, -0.4231812059879303\n",
      "step 74\n",
      "4.476817607879639, -0.4231812059879303\n",
      "step 74\n",
      "4.475744724273682, -0.4242539405822754\n",
      "step 75\n",
      "4.475744724273682, -0.4242539405822754\n",
      "step 75\n",
      "4.474714756011963, -0.4252837598323822\n",
      "step 76\n",
      "4.474714756011963, -0.4252837598323822\n",
      "step 76\n",
      "4.473726272583008, -0.42627236247062683\n",
      "step 77\n",
      "4.473726272583008, -0.42627236247062683\n",
      "step 77\n",
      "4.472777366638184, -0.4272214472293854\n",
      "step 78\n",
      "4.472777366638184, -0.4272214472293854\n",
      "step 78\n",
      "4.471866130828857, -0.428132563829422\n",
      "step 79\n",
      "4.471866130828857, -0.428132563829422\n",
      "step 79\n",
      "4.470991611480713, -0.42900723218917847\n",
      "step 80\n",
      "4.470991611480713, -0.42900723218917847\n",
      "step 80\n",
      "4.470151901245117, -0.4298469126224518\n",
      "step 81\n",
      "4.470151901245117, -0.4298469126224518\n",
      "step 81\n",
      "4.469345569610596, -0.43065300583839417\n",
      "step 82\n",
      "4.469345569610596, -0.43065300583839417\n",
      "step 82\n",
      "4.468571662902832, -0.43142685294151306\n",
      "step 83\n",
      "4.468571662902832, -0.43142685294151306\n",
      "step 83\n",
      "4.467828750610352, -0.43216973543167114\n",
      "step 84\n",
      "4.467828750610352, -0.43216973543167114\n",
      "step 84\n",
      "4.46711540222168, -0.4328829050064087\n",
      "step 85\n",
      "4.46711540222168, -0.4328829050064087\n",
      "step 85\n",
      "4.4664306640625, -0.4335675537586212\n",
      "step 86\n",
      "4.4664306640625, -0.4335675537586212\n",
      "step 86\n",
      "4.465773582458496, -0.43422481417655945\n",
      "step 87\n",
      "4.465773582458496, -0.43422481417655945\n",
      "step 87\n",
      "4.465142726898193, -0.43485578894615173\n",
      "step 88\n",
      "4.465142726898193, -0.43485578894615173\n",
      "step 88\n",
      "4.464537143707275, -0.43546152114868164\n",
      "step 89\n",
      "4.464537143707275, -0.43546152114868164\n",
      "step 89\n",
      "4.463955402374268, -0.43604302406311035\n",
      "step 90\n",
      "4.463955402374268, -0.43604302406311035\n",
      "step 90\n",
      "4.463397026062012, -0.43660128116607666\n",
      "step 91\n",
      "4.463397026062012, -0.43660128116607666\n",
      "step 91\n",
      "4.462861061096191, -0.4371371865272522\n",
      "step 92\n",
      "4.462861061096191, -0.4371371865272522\n",
      "step 92\n",
      "4.46234655380249, -0.437651664018631\n",
      "step 93\n",
      "4.46234655380249, -0.437651664018631\n",
      "step 93\n",
      "4.461852550506592, -0.43814554810523987\n",
      "step 94\n",
      "4.461852550506592, -0.43814554810523987\n",
      "step 94\n",
      "4.461378574371338, -0.4386196732521057\n",
      "step 95\n",
      "4.461378574371338, -0.4386196732521057\n",
      "step 95\n",
      "4.460923194885254, -0.439074844121933\n",
      "step 96\n",
      "4.460923194885254, -0.439074844121933\n",
      "step 96\n",
      "4.46048641204834, -0.43951180577278137\n",
      "step 97\n",
      "4.46048641204834, -0.43951180577278137\n",
      "step 97\n",
      "4.460066795349121, -0.43993130326271057\n",
      "step 98\n",
      "4.460066795349121, -0.43993130326271057\n",
      "step 98\n",
      "4.4596638679504395, -0.4403340220451355\n",
      "step 99\n",
      "4.4596638679504395, -0.4403340220451355\n",
      "step 99\n",
      "4.459277153015137, -0.4407206177711487\n",
      "test = [ 5.01855659]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf.RegisterGradient()라는 함수가 있다, 특정 이름을 가진 grad함수를 등록하는 것이다, 어딘가에 등록을 하면 만약 rnd_name이라는 이름으로 등록했다면\n",
    "with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "    ~~~~\n",
    "이런 식으로 쓰게된다.\n",
    "\n",
    "@tf.RegisterGradient(이름)\n",
    "def ~~~\n",
    "    ~~~~\n",
    "이렇게 하면 @아래 줄 함수를 저 이름으로 등록하는거고\n",
    "tf.RegisterGradient(이름)(함수) 로 쓸수도 있다. 즉 @는 약간~ with 같은 느낌?\n",
    "\n",
    "남은 의문은 저렇게 gradient에 등록 된 이름은 무엇무엇이 있을까?\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "tf.reset_default_graph()\n",
    "def addone(x):\n",
    "    # print(type(x)\n",
    "    return x + 1\n",
    "\n",
    "def addone_grad(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    return grad\n",
    "\"\"\"\n",
    "op는 이 grad function이 배정 될 함수를 가르킨다.\n",
    "op.inputs[0],op.inputs[1] 은 각각 op의 첫번째 인자, 두번째 인자를 가르킨다.\n",
    "grad는 이 함수 전까지 모인 grad를 말한다. return grad 한다는건 그대로 보내준다는 것 즉 op의 gradient는 1이라는 말이다.\n",
    "\n",
    "아래와 같은 함수의 정의도 가능하다.\n",
    "\"\"\"\n",
    "\n",
    "def np_mod(x,y):\n",
    "    return (x % y).astype(np.float32)    \n",
    "#위 함수는 사실 미분이 정의되지않았지, 나머지 구하는 연산이니까.\n",
    "def modgrad(op, grad):\n",
    "    x = op.inputs[0] # the first argument (normally you need those to calculate the gradient, like the gradient of x^2 is 2x. )\n",
    "    y = op.inputs[1] # the second argument\n",
    "    return grad * 1, grad * tf.neg(tf.floordiv(x, y)) #the propagated gradient with respect to the first and second argument respectively\n",
    "#첫번째 인자 x, 두번째 인자 y에 대하여 각각 gradient를 정의해준다. 주의: 위의 np_mod는 아래 세션에서 쓰지않는다.\n",
    "\n",
    "# Define custom py_func which takes also a grad op as argument:\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "#위 함수는 gradient의 적용을 받은 py_func함수를 반환한다.\n",
    "#py_func는 파이썬 함수를 텐서로 바꿔주는건데 grad가 없으니까 위처럼 정의해주는 것이다.\n",
    "#https://stackoverflow.com/questions/39048984/tensorflow-how-to-write-op-with-gradient-in-python\n",
    "\n",
    "# create data\n",
    "x_data = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "y_data = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "w = tf.Variable(5.)\n",
    "b = tf.Variable(0.1)\n",
    "\n",
    "y1 = tf.multiply(w, x_data, name='y1')\n",
    "y2 = py_func(addone, [y1], [tf.float32], grad=addone_grad)[0]\n",
    "y = tf.add(y2, b)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "print(\"Pyfunc grad\", ops.get_gradient_function(y2.op))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100):\n",
    "        #            ran = np.random.rand(115).astype(np.float32)\n",
    "        ran = np.ones((15)).astype(np.float32)\n",
    "        ans = ran * 2 + 3\n",
    "        dic = {x_data: ran, y_data: ans}\n",
    "        print('step {}'.format(step))\n",
    "        print('{}, {}'.format(w.eval(), b.eval()))\n",
    "        tt, yy, yy1= sess.run([train, y1, y2], feed_dict=dic)\n",
    "        if step % 1 == 0:\n",
    "            print('step {}'.format(step))\n",
    "            print('{}, {}'.format(w.eval(), b.eval()))\n",
    "\n",
    "    test = sess.run(y, feed_dict={x_data:[1]})\n",
    "    print('test = {}'.format(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 tf.GraphKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Stage1/Stage2/Variable:0' shape=() dtype=int32_ref>\n",
      "No filter: [<tf.Variable 'Stage1/Stage2/Variable:0' shape=() dtype=int32_ref>]\n",
      "Bad filter: []\n",
      "Valid filter: []\n",
      "Scope name: Stage1_1/Stage2\n",
      "Scope name: <tensorflow.python.ops.variable_scope.VariableScope object at 0x7f29020e29e8>\n",
      "<tf.Variable 'Stage1/Stage2/Hi:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Stage1/Stage2/Hi2:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Stage1/Stage2/Hi:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "아주 유용한 녀석이다.\n",
    "일단 tf.GraphKeys. 안에 기본적으로 저장 된 키들이 있는데 변수,update_ops,Weights,Activation 등이 있다. 이걸 어디에 쓰냐고?\n",
    "놀랍게도 같은 그래프에 속한다면 언제든지 이걸 이용해서 원하는걸 받을 수 있다. 함수인자 전달 없이도..\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tensorflow.contrib.framework import get_name_scope\n",
    "\n",
    "with tf.name_scope(\"Stage1\") as name:\n",
    "    with tf.name_scope(\"Stage2\"):\n",
    "        w=tf.Variable(1,'Myweight')\n",
    "        tf.add_to_collection('Myset',w)\n",
    "print(w)\n",
    "print(\"No filter:\",tf.get_collection('Myset'))\n",
    "print(\"Bad filter:\",tf.get_collection('Myset',scope='Stage2'))\n",
    "print(\"Valid filter:\",tf.get_collection('Myset',scope='Stage1_'))\n",
    "\n",
    "\"\"\"\n",
    "위 코드에서 두가지 사실을 알 수 있다.\n",
    "1)add_to_collection에서 Key부분에는 임의의 문자열이 들어갈 수 있다\n",
    "2)get_collection에서 Key로 collection을 호출 할 때, scope(일종의 필터다)를 지정 할 수 있는데 이 필터가 몹시 답답한 것이 이름을\n",
    "앞에서부터 매칭해서 틀리면 바로 제외시킨다. Stage2 같이 중간에 들어간 문자열은 제대로 된 필터로 기능하지 않는다.\n",
    "->한 콜렉션에서 다양한 다른 집합에 접근하고싶다면 key를 다르게 하는 것이 정신건강에 좋아보인다.\n",
    "\n",
    "아래의 함수를 이용하여 스코프의 이름을 알 수 있고, 그러면 같은 스코프에서 정의 된 어떤 변수를 접근 할 수 있게되겠지?\n",
    "\"\"\"\n",
    "with tf.name_scope(\"Stage1\"):\n",
    "    with tf.name_scope(\"Stage2\"):\n",
    "        print(\"Scope name:\",get_name_scope())\n",
    "        print(\"Scope name:\",tf.get_variable_scope())\n",
    "        \n",
    "with tf.variable_scope(\"Stage1\"):\n",
    "    with tf.variable_scope(\"Stage2\"):\n",
    "        w=tf.get_variable(\"Hi\",1)\n",
    "        print(w)\n",
    "with tf.variable_scope(\"Stage1\"):\n",
    "    with tf.variable_scope(\"Stage2\"):\n",
    "        w=tf.get_variable(\"Hi2\",1)\n",
    "        #w=tf.get_variable(\"Hi\",1)\n",
    "        print(w)\n",
    "with tf.variable_scope(\"Stage1\",reuse=True):   #Reuse는 예전에 만들었던거 불러와서 쓴다는 이야기다. 여기서 새로운거 즉 앞에서 선언안한거 쓰려고 하면 오류\n",
    "    with tf.variable_scope(\"Stage2\"):          #위에서는 Reuse 안썼으니까 있던거 쓰면 오류 난다. 각각 create mode scope, reuse mode scope    \n",
    "        w=tf.get_variable(\"Hi\",1)\n",
    "        #w2=tf.get_variable(\"Hi3\",1)\n",
    "        print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬 내장 함수\n",
    "### 2.1 enumerate, reshape,plt.imshow,Filter of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7f45fe518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7f4556240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]],\n",
    "                   [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3), cmap='Greys')\n",
    "plt.show()\n",
    "plt.imshow(image.reshape(3,3), cmap='Greys')\n",
    "plt.show()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)InteractiveSession()은 무엇인가???\n",
    "2)image를 저렇게 만들었다, reshape를 쓰면 똑똑하게 저절로 쉐잎을 바꿔준다 \n",
    "이 image는 아래에서 cnn에 쓰인다.\n",
    "3)plt.show()는 방금 imshow에 들어간걸 출력한다. 그리고 한번 출력하고나면 없어진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of conv2d:<class 'tensorflow.python.framework.ops.Tensor'>        Shape of conv2d:(1, 2, 2, 1)                                                \n",
      "Type of conv2d_img:<class 'numpy.ndarray'>                             Shape of conv2d_img:(1, 2, 2, 1)                                                \n",
      "Shape of weight: (2, 2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv2d_img = conv2d.eval()\n",
    "\n",
    "print(\"Type of conv2d:{0:<55}\".format(str(type(conv2d))),\"Shape of conv2d:{0:<60}\".format(str(conv2d.shape)))\n",
    "print(\"Type of conv2d_img:{0:<51}\".format(str(type(conv2d_img))),\"Shape of conv2d_img:{0:<60}\".format(str(conv2d_img.shape)))\n",
    "print(\"Shape of weight:\", weight.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)컨볼루션레이어를 저렇게 안쓰고 layer API를 사용하여 구성할 수도 있다.\n",
    "image (a,b,c,d) 에서 a는 입력 image의 수, b,c는 입력이미지의 가로세로픽셀, d는 입력이미지의 depth를 나타낸다 여기서 depth는 예를 들어 grayscale이면 1, RGB면 3이 된다. 그리고 컨볼루션레이어를 지나가면 예를 들어 filter가 16개인 레이어를 지나가면 depth는 16이 된다. \n",
    "filter (a,b,c,d)에서 a,b,c는 차례대로 filter의 가로세로픽셀, 그리고 depth이며,마지막 d는 filter의 갯수이다.\n",
    "그래서 만약\n",
    "image.shape (10,28,28,3) 이 filter 16개인 컨볼루션 레이어(padding='same')를 지나갈 때, filter.shape (2,2,3,16) 이며\n",
    "출력은 image.shape (10,28,28,16)이 된다.\n",
    "2)conv2d.eval() 을 쓰면 데이터를 array형으로 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 12.]\n",
      "   [ 16.]]\n",
      "\n",
      "  [[ 24.]\n",
      "   [ 28.]]]]\n",
      "[[[[ 12.]\n",
      "   [ 16.]]\n",
      "\n",
      "  [[ 24.]\n",
      "   [ 28.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(conv2d_img)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "print(conv2d_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "one_img: shape: (2, 2, 1) type: <class 'numpy.ndarray'>\n",
      "[[ 12.  16.]\n",
      " [ 24.  28.]]\n",
      "0 김\n",
      "1 민\n",
      "2 경\n"
     ]
    }
   ],
   "source": [
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(i)\n",
    "    print(\"one_img:\", \"shape:\", one_img.shape, \"type:\", type(one_img))\n",
    "    print(one_img.reshape(2,2))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray')\n",
    "a=['김','민','경']\n",
    "for i, two_test in enumerate(a):\n",
    "    print(i,two_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)conv2d_img는 (1,2,2,1)이였으나 enumerate에 들어가면서 한개씩 반환되므로 (2,2,1)이 one_img에 반환되었고, i는 index이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 range, append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range(-5,5):  -5  -4  -3  -2  -1  0  1  2  3  4  \n",
      "Range(5):      0  1  2  3  4  \n",
      "Range(-5,5,2):-5  -3  -1  1  3  "
     ]
    }
   ],
   "source": [
    "print(\"Range(-5,5):  \",end='')\n",
    "for i in range(-5,5):\n",
    "    print(i,' ',end='')\n",
    "print(\"\")\n",
    "print(\"Range(5):      \",end='')\n",
    "for i in range(5):\n",
    "    print(i,' ',end='')\n",
    "print(\"\")\n",
    "print(\"Range(-5,5,2):\",end='')\n",
    "for i in range(-5,5,2):\n",
    "    print(i,' ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[-30, -27, -24, -21, -18, -15, -12, -9, -6, -3, 0, 3, 6, 9, 12, 15, 18, 21, 24, 27]\n",
      "[100, 81, 64, 49, 36, 25, 16, 9, 4, 1, 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "Z=[]\n",
    "for i in range(-10,10):\n",
    "    X.append(i)\n",
    "    Z.append(3*i)\n",
    "    Y.append(pow(i,2))\n",
    "print(X)\n",
    "print(Z)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w8ML3ZicCuIOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7f4236c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNvf/AT6FVPbu+YIEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7f4062c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7f41e1940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X,Y)\n",
    "plt.show()    #x축은 같은 index값의 X가 된다.\n",
    "plt.plot(Z,Y)\n",
    "plt.show()   #x축은 같은 index값의 Z가 된다.\n",
    "plt.plot(Y)\n",
    "plt.show()  #x축은 index값이 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 통해 append는 어떻게 쓰는지, 그리고 plt.plot과 plt.show는 어떻게 쓰는지 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 dir:메서드를 보여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'append',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'pop',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'sort']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir([1,2,3]))\n",
    "dir([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir은 객체가 자체적으로 가지고 있는 변수나 관련 함수(메서드)를 보여 준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 np.eye, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위의 저걸 왜 썼나 했더니 저걸로 one hot encoding이 된다\n",
    "print(np.eye(10)[[3,6,9]])\n",
    "\n",
    "print(np.float32(np.eye(10)[[3,6,9]]))\n",
    "np.eye(10)[[3,6,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3, 4]), array(['a', 'b', 'c', 'd'],\n",
      "      dtype='<U1'))\n",
      "['1' '2' '3' '4' 'a' 'b' 'c' 'd']\n",
      "(array([[1],\n",
      "       [2],\n",
      "       [3]]), array([['a'],\n",
      "       ['b'],\n",
      "       ['c']],\n",
      "      dtype='<U1'))\n",
      "[['1' 'a']\n",
      " ['2' 'b']\n",
      " ['3' 'c']]\n",
      "##############################\n",
      "(array([[1, 2, 3, 4],\n",
      "       [5, 6, 7, 8]]), array([['a', 'b', 'c', 'd'],\n",
      "       ['e', 'f', 'g', 'h']],\n",
      "      dtype='<U1'))\n",
      "[['1' '2' '3' '4' 'a' 'b' 'c' 'd']\n",
      " ['5' '6' '7' '8' 'e' 'f' 'g' 'h']]\n",
      "(2, 8)\n",
      "([1, 2, 3, 4], ['a', 'b', 'c', 'd'])\n",
      "['1' '2' '3' '4' 'a' 'b' 'c' 'd']\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "b=np.array(['a','b','c','d'])\n",
    "c=np.array([[1],[2],[3]])\n",
    "d=np.array([['a'],['b'],['c']])\n",
    "print((a,b))\n",
    "print(np.hstack((a,b)))\n",
    "print((c,d))\n",
    "print(np.hstack((c,d)))\n",
    "print(\"#\"*30)\n",
    "#columnwise, 행렬로 만들었을 때 첫 행부터 읽어나가며 array로 만든다.\n",
    "#뭐야 이거 결국 여러개 array붙여서 행렬 만드는거 아님?\n",
    "a=np.array([[1,2,3,4],[5,6,7,8]])\n",
    "b=np.array([['a','b','c','d'],['e','f','g','h']])\n",
    "print((a,b))\n",
    "print(np.hstack((a,b)))\n",
    "print(np.hstack((a,b)).shape)\n",
    "#리스트도 된다~\n",
    "a=[1,2,3,4]\n",
    "b=['a','b','c','d']\n",
    "print((a,b))\n",
    "print(np.hstack((a,b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack arrays in sequence horizontally (column wise).\n",
    "\n",
    "Take a sequence of arrays and stack them horizontally to make a single array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[1]\n",
      " [2]\n",
      " [3]]\n",
      "b: [1 2 3]\n",
      "shape of a: (3, 1)  shape of b: (3,)\n",
      "shape of c: (3,)\n",
      "shape of d: (3,)\n"
     ]
    }
   ],
   "source": [
    "#We can use this function to flatten some array\n",
    "a=np.array([[1],[2],[3]])\n",
    "b=np.hstack(a)\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"shape of a:\",a.shape,\"shape of b:\",b.shape)\n",
    "\n",
    "c=np.array([1,2,3])\n",
    "print(\"shape of c:\",c.shape)\n",
    "\n",
    "d=a.reshape(3,)\n",
    "print(\"shape of d:\",d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 binomial 베르누이 분포, 확률분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 0, 1, 2, 2, 1, 2, 2],\n",
       "       [1, 3, 1, 0, 1, 0, 1, 2, 1, 1],\n",
       "       [2, 2, 2, 1, 1, 1, 0, 2, 1, 0],\n",
       "       [2, 3, 1, 0, 0, 1, 0, 1, 2, 0],\n",
       "       [1, 1, 2, 2, 0, 2, 0, 0, 3, 3],\n",
       "       [1, 2, 2, 1, 1, 1, 1, 2, 1, 0],\n",
       "       [1, 2, 1, 0, 2, 0, 1, 1, 2, 1],\n",
       "       [2, 2, 2, 1, 0, 1, 2, 2, 2, 1],\n",
       "       [0, 1, 0, 2, 1, 1, 1, 0, 1, 2],\n",
       "       [0, 1, 0, 2, 1, 1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(4,0.3,(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개변수 n,p,size\n",
    "n은 한번 실험 할 때 던지는 동전의 횟수, p는 그 동전이 앞면이 나올 확률, size는 이러한 실험을 반복하는 횟수\n",
    "출력은 n번의 try 중에서 몇번이나 앞면이 나왔는지를 나타낸다.\n",
    "만약 size를 키우면, n일 때의 베르누이 확률 분포를 근사할 수 있을 것이다.\n",
    "예를 들어 n=10,p=0.5,size=10000정도 된다면, 확률이 0.5로 10번을 던졌을 때 앞면이 나오는 횟수의 확률분포가 어떻게 될지 대충 근사가 가능할 것이다.\n",
    "\n",
    "어떤 코드에서는 n=1,p=p0 size=size0 으로 줬는데, 매 원소의 확률에 근거하여 해당 원소를 1로 보낼지 0으로 보낼지를 결정 할 수 있게된다.\n",
    "\n",
    "Draw samples from a binomial distribution.\n",
    "Samples are drawn from a binomial distribution with specified parameters, n trials and p probability of success where n an integer >= 0 and p is in the interval [0,1]. (n may be input as a float, but it is truncated to an integer in use)\n",
    "Parameters:\t\n",
    "n : int or array_like of ints\n",
    "    Parameter of the distribution, >= 0. Floats are also accepted, but they will be truncated to integers.\n",
    "p : float or array_like of floats\n",
    "    Parameter of the distribution, >= 0 and <=1.\n",
    "size : int or tuple of ints, optional\n",
    "    Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.\n",
    "\n",
    "Returns:\t\n",
    "\n",
    "out : ndarray or scalar\n",
    "\n",
    "    Drawn samples from the parameterized binomial distribution, where each sample is equal to the number of successes over the n trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 argparse는 무엇인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-01ca5a518211>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-01ca5a518211>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    $ python prog.py -h\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "아마도 argparse는 argument parser일거다, 즉 무언가 주장을 조각단위로 받는거다.(참고로 파싱을 한다는건 명령어를 유효한 여러 단위로 나눈다는 말)\n",
    "나는 지금 파이썬파일을 실행하는데 파이참이라는걸 쓰고있지만 어떤 경우에는 터미널(커맨드라인)에서 직접 바로 실행하기를 원할 수 있다\n",
    "그때 쓰이는 것이 바로 parser이다. 아래와 같은 방식으로 무엇을 입력으로 어떻게 받을지를 정할 수 있다.\n",
    "여기서는 특별히 정해져있는거 같지는 않지만 default값을 통해서 파이참 같은 인터프리터에서도 바로 실행가능할 것이다.\n",
    "\"\"\"\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "                    help='an integer for the accumulator')\n",
    "parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "                    const=sum, default=max,\n",
    "                    help='sum the integers (default: find the max)')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args.accumulate(args.integers))\n",
    "#커맨드라인에서의 실행\n",
    "$ python prog.py -h\n",
    "usage: prog.py [-h] [--sum] N [N ...]\n",
    "\n",
    "Process some integers.\n",
    "\n",
    "positional arguments:\n",
    " N           an integer for the accumulator\n",
    "\n",
    "optional arguments:\n",
    " -h, --help  show this help message and exit\n",
    " --sum       sum the integers (default: find the max)\n",
    "#결과    \n",
    "$ python prog.py 1 2 3 4\n",
    "4\n",
    "\n",
    "$ python prog.py 1 2 3 4 --sum\n",
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 flag는 무엇인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argparse와 비슷한 용어로 flag가 있다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 딥러닝 다루기\n",
    "### 3.1 train tensor에서 기울기 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(-5.)\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2    #미분결과를 직접 써준 것이다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지는 일반적인 NN과정이다, 코스트함수를 정의해주고,GradientDescent를 이용해서 코스트함수를 최소화하는 웨이트 값을 찾아준다.\n",
    "하지만 웨이트 값을 구하는 일련의 과정은 train텐서 안에서 일어나서 우리가 weight값을 수정하기가 어렵다.(관찰은 할 수 있나???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[(<tf.Tensor 'gradients_2/mul_grad/tuple/control_dependency_1:0' shape=() dtype=float32>, <tf.Variable 'Variable:0' shape=() dtype=float32_ref>)]\n",
      "[(<tf.Tensor 'gradients_2/mul_grad/tuple/control_dependency_1:0' shape=() dtype=float32>, <tf.Variable 'Variable:0' shape=() dtype=float32_ref>)]\n",
      "[(<tf.Tensor 'clip_by_value_1:0' shape=() dtype=float32>, <tf.Variable 'Variable:0' shape=() dtype=float32_ref>)]\n",
      "[(<tf.Tensor 'add_1:0' shape=() dtype=float32>, <tf.Variable 'Variable:0' shape=() dtype=float32_ref>)]\n",
      "[(<tf.Tensor 'mul_8:0' shape=() dtype=float32>, <tf.Tensor 'mul_9:0' shape=() dtype=float32>)]\n"
     ]
    }
   ],
   "source": [
    "# Get gradients \n",
    "# 0831:인자로 코스트함수와 W값을 넣어주면, 코스트함수에서 해당 W일때의 기울기를 반환한다.\n",
    "# 그러면 다시 그걸 옵티마이저에 넣어주는데, 어떤 상황에서 gvs를 수정할 필요가 있는지는 모르겠다.\n",
    "gvs = optimizer.compute_gradients(cost, [W])\n",
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "# Apply gradients\n",
    "print(type(gvs)) #gvs는 그냥 리스트이다, cost함수에서 W위치의 기울기와 입력 된 W값을 리스트로 반환한다.\n",
    "gvs2=[(tf.clip_by_value(grad,-11,1),var) for grad, var in gvs]  #그걸 이런 식으로 수정가능하다\n",
    "gvs3=[(grad*3+var,var) for grad, var in gvs]                      #리스트는 [1,2,3]*3하면, 똑같은 리스트가 3번 반복될 뿐이다, 저렇게 해줘야한다. \n",
    "gvs4=[(grad*3,var*3) for grad, var in gvs]  \n",
    "print(gvs)\n",
    "print(gvs2)\n",
    "print(gvs3)\n",
    "print(gvs4)\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "apply_gradients2= optimizer.apply_gradients(gvs3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 기울기를 뽑아낸게 gvs다, 우리는 gvs를 수정함으로써 다음 스텝에 사용 될 기울기와 웨이트를 수정 할 수 있다.\n",
    "그러며 옵티마이저는 그 웨이트에서 기울기*Learning rate 값을 업데이트 해줄 것이다.\n",
    "다만 위에서 보이다시피 웨이트의 자료형은 variable이기 때문에 그걸 지켜줘야한다, 그냥 곱하기를 하면 자료형이 텐서로 바뀌어서 \n",
    "applyy gradient할 때 오류가 뜬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>\n",
      "<class 'tensorflow.python.framework.ops.Operation'>\n",
      "<class 'tensorflow.python.framework.ops.Operation'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cost))\n",
    "print(type(optimizer))\n",
    "print(type(train))\n",
    "print(type(apply_gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보면 train 과 apply_gradients의 타입이 같다,apply_gradients는 train에서의 업데이트를 마저 끝내는거다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [168.0, -56.0, -5.0, [(-56.0, -5.0)]]\n",
      "0 [168.0, -56.0, -5.0, [(-56.0, -5.0)]]\n",
      "0 [138.10345, -50.773335, -4.4400001, [(-50.773335, -4.4400001)]]\n",
      "1 [138.10345, -50.773335, -4.4400001, [(-50.773335, -4.4400001)]]\n",
      "1 [138.10345, -50.773335, -4.4400001, [(-50.773335, -4.4400001)]]\n",
      "1 [113.5272, -46.034492, -3.9322667, [(-46.034492, -3.9322667)]]\n",
      "2 [113.5272, -46.034492, -3.9322667, [(-46.034492, -3.9322667)]]\n",
      "2 [113.5272, -46.034492, -3.9322667, [(-46.034492, -3.9322667)]]\n",
      "2 [93.324394, -41.737938, -3.4719217, [(-41.737938, -3.4719217)]]\n",
      "3 [93.324394, -41.737938, -3.4719217, [(-41.737938, -3.4719217)]]\n",
      "3 [93.324394, -41.737938, -3.4719217, [(-41.737938, -3.4719217)]]\n",
      "3 [76.716797, -37.842396, -3.0545423, [(-37.842396, -3.0545423)]]\n",
      "4 [76.716797, -37.842396, -3.0545423, [(-37.842396, -3.0545423)]]\n",
      "4 [76.716797, -37.842396, -3.0545423, [(-37.842396, -3.0545423)]]\n",
      "4 [63.064613, -34.310436, -2.6761184, [(-34.310436, -2.6761184)]]\n",
      "5 [63.064613, -34.310436, -2.6761184, [(-34.310436, -2.6761184)]]\n",
      "5 [63.064613, -34.310436, -2.6761184, [(-34.310436, -2.6761184)]]\n",
      "5 [51.841923, -31.108131, -2.333014, [(-31.108131, -2.333014)]]\n",
      "6 [51.841923, -31.108131, -2.333014, [(-31.108131, -2.333014)]]\n",
      "6 [51.841923, -31.108131, -2.333014, [(-31.108131, -2.333014)]]\n",
      "6 [42.616356, -28.204704, -2.0219326, [(-28.204704, -2.0219326)]]\n",
      "7 [42.616356, -28.204704, -2.0219326, [(-28.204704, -2.0219326)]]\n",
      "7 [42.616356, -28.204704, -2.0219326, [(-28.204704, -2.0219326)]]\n",
      "7 [35.032543, -25.572266, -1.7398856, [(-25.572268, -1.7398856)]]\n",
      "8 [35.032543, -25.572266, -1.7398856, [(-25.572268, -1.7398856)]]\n",
      "8 [35.032543, -25.572266, -1.7398856, [(-25.572268, -1.7398856)]]\n",
      "8 [28.798306, -23.18552, -1.4841629, [(-23.18552, -1.4841629)]]\n",
      "9 [28.798306, -23.18552, -1.4841629, [(-23.18552, -1.4841629)]]\n",
      "9 [28.798306, -23.18552, -1.4841629, [(-23.18552, -1.4841629)]]\n",
      "9 [23.673489, -21.02154, -1.2523078, [(-21.02154, -1.2523078)]]\n",
      "10 [23.673489, -21.02154, -1.2523078, [(-21.02154, -1.2523078)]]\n",
      "10 [23.673489, -21.02154, -1.2523078, [(-21.02154, -1.2523078)]]\n",
      "10 [19.460659, -19.059528, -1.0420923, [(-19.059528, -1.0420923)]]\n",
      "11 [19.460659, -19.059528, -1.0420923, [(-19.059528, -1.0420923)]]\n",
      "11 [19.460659, -19.059528, -1.0420923, [(-19.059528, -1.0420923)]]\n",
      "11 [15.997526, -17.28064, -0.85149705, [(-17.28064, -0.85149705)]]\n",
      "12 [15.997526, -17.28064, -0.85149705, [(-17.28064, -0.85149705)]]\n",
      "12 [15.997526, -17.28064, -0.85149705, [(-17.28064, -0.85149705)]]\n",
      "12 [13.150677, -15.66778, -0.67869067, [(-15.66778, -0.67869067)]]\n",
      "13 [13.150677, -15.66778, -0.67869067, [(-15.66778, -0.67869067)]]\n",
      "13 [13.150677, -15.66778, -0.67869067, [(-15.66778, -0.67869067)]]\n",
      "13 [10.810442, -14.205455, -0.52201289, [(-14.205455, -0.52201289)]]\n",
      "14 [10.810442, -14.205455, -0.52201289, [(-14.205455, -0.52201289)]]\n",
      "14 [10.810442, -14.205455, -0.52201289, [(-14.205455, -0.52201289)]]\n",
      "14 [8.8866644, -12.879612, -0.37995833, [(-12.879612, -0.37995833)]]\n",
      "15 [8.8866644, -12.879612, -0.37995833, [(-12.879612, -0.37995833)]]\n",
      "15 [8.8866644, -12.879612, -0.37995833, [(-12.879612, -0.37995833)]]\n",
      "15 [7.3052325, -11.677514, -0.2511622, [(-11.677514, -0.2511622)]]\n",
      "16 [7.3052325, -11.677514, -0.2511622, [(-11.677514, -0.2511622)]]\n",
      "16 [7.3052325, -11.677514, -0.2511622, [(-11.677514, -0.2511622)]]\n",
      "16 [6.0052252, -10.587612, -0.13438706, [(-10.587612, -0.13438706)]]\n",
      "17 [6.0052252, -10.587612, -0.13438706, [(-10.587612, -0.13438706)]]\n",
      "17 [6.0052252, -10.587612, -0.13438706, [(-10.587612, -0.13438706)]]\n",
      "17 [4.9365625, -9.5994358, -0.028510941, [(-9.5994358, -0.028510941)]]\n",
      "18 [4.9365625, -9.5994358, -0.028510941, [(-9.5994358, -0.028510941)]]\n",
      "18 [4.9365625, -9.5994358, -0.028510941, [(-9.5994358, -0.028510941)]]\n",
      "18 [4.0580735, -8.7034883, 0.067483418, [(-8.7034883, 0.067483418)]]\n",
      "19 [4.0580735, -8.7034883, 0.067483418, [(-8.7034883, 0.067483418)]]\n",
      "19 [4.0580735, -8.7034883, 0.067483418, [(-8.7034883, 0.067483418)]]\n",
      "19 [3.335917, -7.8911629, 0.15451831, [(-7.8911629, 0.15451831)]]\n",
      "20 [3.335917, -7.8911629, 0.15451831, [(-7.8911629, 0.15451831)]]\n",
      "20 [3.335917, -7.8911629, 0.15451831, [(-7.8911629, 0.15451831)]]\n",
      "20 [2.7422719, -7.1546545, 0.23342994, [(-7.1546545, 0.23342994)]]\n",
      "21 [2.7422719, -7.1546545, 0.23342994, [(-7.1546545, 0.23342994)]]\n",
      "21 [2.7422719, -7.1546545, 0.23342994, [(-7.1546545, 0.23342994)]]\n",
      "21 [2.2542694, -6.486886, 0.30497649, [(-6.486886, 0.30497649)]]\n",
      "22 [2.2542694, -6.486886, 0.30497649, [(-6.486886, 0.30497649)]]\n",
      "22 [2.2542694, -6.486886, 0.30497649, [(-6.486886, 0.30497649)]]\n",
      "22 [1.8531095, -5.8814435, 0.36984536, [(-5.8814435, 0.36984536)]]\n",
      "23 [1.8531095, -5.8814435, 0.36984536, [(-5.8814435, 0.36984536)]]\n",
      "23 [1.8531095, -5.8814435, 0.36984536, [(-5.8814435, 0.36984536)]]\n",
      "23 [1.5233383, -5.3325086, 0.4286598, [(-5.3325086, 0.4286598)]]\n",
      "24 [1.5233383, -5.3325086, 0.4286598, [(-5.3325086, 0.4286598)]]\n",
      "24 [1.5233383, -5.3325086, 0.4286598, [(-5.3325086, 0.4286598)]]\n",
      "24 [1.2522519, -4.8348079, 0.48198488, [(-4.8348079, 0.48198488)]]\n",
      "25 [1.2522519, -4.8348079, 0.48198488, [(-4.8348079, 0.48198488)]]\n",
      "25 [1.2522519, -4.8348079, 0.48198488, [(-4.8348079, 0.48198488)]]\n",
      "25 [1.0294065, -4.3835592, 0.53033298, [(-4.3835592, 0.53033298)]]\n",
      "26 [1.0294065, -4.3835592, 0.53033298, [(-4.3835592, 0.53033298)]]\n",
      "26 [1.0294065, -4.3835592, 0.53033298, [(-4.3835592, 0.53033298)]]\n",
      "26 [0.84621799, -3.9744267, 0.57416856, [(-3.9744267, 0.57416856)]]\n",
      "27 [0.84621799, -3.9744267, 0.57416856, [(-3.9744267, 0.57416856)]]\n",
      "27 [0.84621799, -3.9744267, 0.57416856, [(-3.9744267, 0.57416856)]]\n",
      "27 [0.69562876, -3.6034803, 0.61391282, [(-3.6034803, 0.61391282)]]\n",
      "28 [0.69562876, -3.6034803, 0.61391282, [(-3.6034803, 0.61391282)]]\n",
      "28 [0.69562876, -3.6034803, 0.61391282, [(-3.6034803, 0.61391282)]]\n",
      "28 [0.57183766, -3.2671552, 0.64994764, [(-3.2671552, 0.64994764)]]\n",
      "29 [0.57183766, -3.2671552, 0.64994764, [(-3.2671552, 0.64994764)]]\n",
      "29 [0.57183766, -3.2671552, 0.64994764, [(-3.2671552, 0.64994764)]]\n",
      "29 [0.47007588, -2.9622204, 0.68261921, [(-2.9622204, 0.68261921)]]\n",
      "30 [0.47007588, -2.9622204, 0.68261921, [(-2.9622204, 0.68261921)]]\n",
      "30 [0.47007588, -2.9622204, 0.68261921, [(-2.9622204, 0.68261921)]]\n",
      "30 [0.38642335, -2.6857469, 0.71224141, [(-2.6857469, 0.71224141)]]\n",
      "31 [0.38642335, -2.6857469, 0.71224141, [(-2.6857469, 0.71224141)]]\n",
      "31 [0.38642335, -2.6857469, 0.71224141, [(-2.6857469, 0.71224141)]]\n",
      "31 [0.31765717, -2.4350772, 0.73909891, [(-2.4350772, 0.73909891)]]\n",
      "32 [0.31765717, -2.4350772, 0.73909891, [(-2.4350772, 0.73909891)]]\n",
      "32 [0.31765717, -2.4350772, 0.73909891, [(-2.4350772, 0.73909891)]]\n",
      "32 [0.26112828, -2.207803, 0.76344967, [(-2.207803, 0.76344967)]]\n",
      "33 [0.26112828, -2.207803, 0.76344967, [(-2.207803, 0.76344967)]]\n",
      "33 [0.26112828, -2.207803, 0.76344967, [(-2.207803, 0.76344967)]]\n",
      "33 [0.21465904, -2.0017414, 0.78552771, [(-2.0017414, 0.78552771)]]\n",
      "34 [0.21465904, -2.0017414, 0.78552771, [(-2.0017414, 0.78552771)]]\n",
      "34 [0.21465904, -2.0017414, 0.78552771, [(-2.0017414, 0.78552771)]]\n",
      "34 [0.17645931, -1.8149126, 0.80554509, [(-1.8149126, 0.80554509)]]\n",
      "35 [0.17645931, -1.8149126, 0.80554509, [(-1.8149126, 0.80554509)]]\n",
      "35 [0.17645931, -1.8149126, 0.80554509, [(-1.8149126, 0.80554509)]]\n",
      "35 [0.1450574, -1.6455204, 0.82369423, [(-1.6455204, 0.82369423)]]\n",
      "36 [0.1450574, -1.6455204, 0.82369423, [(-1.6455204, 0.82369423)]]\n",
      "36 [0.1450574, -1.6455204, 0.82369423, [(-1.6455204, 0.82369423)]]\n",
      "36 [0.11924356, -1.4919382, 0.84014946, [(-1.4919382, 0.84014946)]]\n",
      "37 [0.11924356, -1.4919382, 0.84014946, [(-1.4919382, 0.84014946)]]\n",
      "37 [0.11924356, -1.4919382, 0.84014946, [(-1.4919382, 0.84014946)]]\n",
      "37 [0.098023519, -1.3526907, 0.85506886, [(-1.3526908, 0.85506886)]]\n",
      "38 [0.098023519, -1.3526907, 0.85506886, [(-1.3526908, 0.85506886)]]\n",
      "38 [0.098023519, -1.3526907, 0.85506886, [(-1.3526908, 0.85506886)]]\n",
      "38 [0.080579676, -1.2264395, 0.86859578, [(-1.2264396, 0.86859578)]]\n",
      "39 [0.080579676, -1.2264395, 0.86859578, [(-1.2264396, 0.86859578)]]\n",
      "39 [0.080579676, -1.2264395, 0.86859578, [(-1.2264396, 0.86859578)]]\n",
      "39 [0.066240072, -1.1119719, 0.88086015, [(-1.1119719, 0.88086015)]]\n",
      "40 [0.066240072, -1.1119719, 0.88086015, [(-1.1119719, 0.88086015)]]\n",
      "40 [0.066240072, -1.1119719, 0.88086015, [(-1.1119719, 0.88086015)]]\n",
      "40 [0.0544523, -1.008188, 0.89197987, [(-1.008188, 0.89197987)]]\n",
      "41 [0.0544523, -1.008188, 0.89197987, [(-1.008188, 0.89197987)]]\n",
      "41 [0.0544523, -1.008188, 0.89197987, [(-1.008188, 0.89197987)]]\n",
      "41 [0.044762187, -0.91409016, 0.90206176, [(-0.91409016, 0.90206176)]]\n",
      "42 [0.044762187, -0.91409016, 0.90206176, [(-0.91409016, 0.90206176)]]\n",
      "42 [0.044762187, -0.91409016, 0.90206176, [(-0.91409016, 0.90206176)]]\n",
      "42 [0.03679651, -0.82877505, 0.91120267, [(-0.82877505, 0.91120267)]]\n",
      "43 [0.03679651, -0.82877505, 0.91120267, [(-0.82877505, 0.91120267)]]\n",
      "43 [0.03679651, -0.82877505, 0.91120267, [(-0.82877505, 0.91120267)]]\n",
      "43 [0.030248374, -0.75142288, 0.9194904, [(-0.75142288, 0.9194904)]]\n",
      "44 [0.030248374, -0.75142288, 0.9194904, [(-0.75142288, 0.9194904)]]\n",
      "44 [0.030248374, -0.75142288, 0.9194904, [(-0.75142288, 0.9194904)]]\n",
      "44 [0.024865501, -0.68128997, 0.92700464, [(-0.68129003, 0.92700464)]]\n",
      "45 [0.024865501, -0.68128997, 0.92700464, [(-0.68129003, 0.92700464)]]\n",
      "45 [0.024865501, -0.68128997, 0.92700464, [(-0.68129003, 0.92700464)]]\n",
      "45 [0.020440556, -0.61770302, 0.93381751, [(-0.61770302, 0.93381751)]]\n",
      "46 [0.020440556, -0.61770302, 0.93381751, [(-0.61770302, 0.93381751)]]\n",
      "46 [0.020440556, -0.61770302, 0.93381751, [(-0.61770302, 0.93381751)]]\n",
      "46 [0.01680308, -0.56005132, 0.93999451, [(-0.56005132, 0.93999451)]]\n",
      "47 [0.01680308, -0.56005132, 0.93999451, [(-0.56005132, 0.93999451)]]\n",
      "47 [0.01680308, -0.56005132, 0.93999451, [(-0.56005132, 0.93999451)]]\n",
      "47 [0.013812874, -0.50777978, 0.94559503, [(-0.50777978, 0.94559503)]]\n",
      "48 [0.013812874, -0.50777978, 0.94559503, [(-0.50777978, 0.94559503)]]\n",
      "48 [0.013812874, -0.50777978, 0.94559503, [(-0.50777978, 0.94559503)]]\n",
      "48 [0.01135481, -0.46038729, 0.95067281, [(-0.46038729, 0.95067281)]]\n",
      "49 [0.01135481, -0.46038729, 0.95067281, [(-0.46038729, 0.95067281)]]\n",
      "49 [0.01135481, -0.46038729, 0.95067281, [(-0.46038729, 0.95067281)]]\n",
      "49 [0.0093341619, -0.41741788, 0.95527667, [(-0.41741788, 0.95527667)]]\n",
      "50 [0.0093341619, -0.41741788, 0.95527667, [(-0.41741788, 0.95527667)]]\n",
      "50 [0.0093341619, -0.41741788, 0.95527667, [(-0.41741788, 0.95527667)]]\n",
      "50 [0.0076730838, -0.37845856, 0.95945084, [(-0.37845862, 0.95945084)]]\n",
      "51 [0.0076730838, -0.37845856, 0.95945084, [(-0.37845862, 0.95945084)]]\n",
      "51 [0.0076730838, -0.37845856, 0.95945084, [(-0.37845862, 0.95945084)]]\n",
      "51 [0.0063076168, -0.3431358, 0.96323544, [(-0.3431358, 0.96323544)]]\n",
      "52 [0.0063076168, -0.3431358, 0.96323544, [(-0.3431358, 0.96323544)]]\n",
      "52 [0.0063076168, -0.3431358, 0.96323544, [(-0.3431358, 0.96323544)]]\n",
      "52 [0.0051851301, -0.31110945, 0.96666682, [(-0.31110948, 0.96666682)]]\n",
      "53 [0.0051851301, -0.31110945, 0.96666682, [(-0.31110948, 0.96666682)]]\n",
      "53 [0.0051851301, -0.31110945, 0.96666682, [(-0.31110948, 0.96666682)]]\n",
      "53 [0.0042624138, -0.28207278, 0.96977794, [(-0.28207278, 0.96977794)]]\n",
      "54 [0.0042624138, -0.28207278, 0.96977794, [(-0.28207278, 0.96977794)]]\n",
      "54 [0.0042624138, -0.28207278, 0.96977794, [(-0.28207278, 0.96977794)]]\n",
      "54 [0.00350388, -0.2557455, 0.97259867, [(-0.2557455, 0.97259867)]]\n",
      "55 [0.00350388, -0.2557455, 0.97259867, [(-0.2557455, 0.97259867)]]\n",
      "55 [0.00350388, -0.2557455, 0.97259867, [(-0.2557455, 0.97259867)]]\n",
      "55 [0.0028803477, -0.23187602, 0.97515613, [(-0.23187602, 0.97515613)]]\n",
      "56 [0.0028803477, -0.23187602, 0.97515613, [(-0.23187602, 0.97515613)]]\n",
      "56 [0.0028803477, -0.23187602, 0.97515613, [(-0.23187602, 0.97515613)]]\n",
      "56 [0.0023677833, -0.21023467, 0.97747487, [(-0.21023467, 0.97747487)]]\n",
      "57 [0.0023677833, -0.21023467, 0.97747487, [(-0.21023467, 0.97747487)]]\n",
      "57 [0.0023677833, -0.21023467, 0.97747487, [(-0.21023467, 0.97747487)]]\n",
      "57 [0.0019464178, -0.19061252, 0.97957724, [(-0.19061252, 0.97957724)]]\n",
      "58 [0.0019464178, -0.19061252, 0.97957724, [(-0.19061252, 0.97957724)]]\n",
      "58 [0.0019464178, -0.19061252, 0.97957724, [(-0.19061252, 0.97957724)]]\n",
      "58 [0.0016000491, -0.1728224, 0.98148334, [(-0.17282242, 0.98148334)]]\n",
      "59 [0.0016000491, -0.1728224, 0.98148334, [(-0.17282242, 0.98148334)]]\n",
      "59 [0.0016000491, -0.1728224, 0.98148334, [(-0.17282242, 0.98148334)]]\n",
      "59 [0.0013153034, -0.15669183, 0.98321158, [(-0.15669185, 0.98321158)]]\n",
      "60 [0.0013153034, -0.15669183, 0.98321158, [(-0.15669185, 0.98321158)]]\n",
      "60 [0.0013153034, -0.15669183, 0.98321158, [(-0.15669185, 0.98321158)]]\n",
      "60 [0.001081232, -0.14206688, 0.98477852, [(-0.14206688, 0.98477852)]]\n",
      "61 [0.001081232, -0.14206688, 0.98477852, [(-0.14206688, 0.98477852)]]\n",
      "61 [0.001081232, -0.14206688, 0.98477852, [(-0.14206688, 0.98477852)]]\n",
      "61 [0.00088882144, -0.12880735, 0.9861992, [(-0.12880735, 0.9861992)]]\n",
      "62 [0.00088882144, -0.12880735, 0.9861992, [(-0.12880735, 0.9861992)]]\n",
      "62 [0.00088882144, -0.12880735, 0.9861992, [(-0.12880735, 0.9861992)]]\n",
      "62 [0.00073065568, -0.11678572, 0.98748726, [(-0.11678572, 0.98748726)]]\n",
      "63 [0.00073065568, -0.11678572, 0.98748726, [(-0.11678572, 0.98748726)]]\n",
      "63 [0.00073065568, -0.11678572, 0.98748726, [(-0.11678572, 0.98748726)]]\n",
      "63 [0.00060063257, -0.10588582, 0.98865509, [(-0.10588582, 0.98865509)]]\n",
      "64 [0.00060063257, -0.10588582, 0.98865509, [(-0.10588582, 0.98865509)]]\n",
      "64 [0.00060063257, -0.10588582, 0.98865509, [(-0.10588582, 0.98865509)]]\n",
      "64 [0.00049374369, -0.096002862, 0.98971397, [(-0.096002862, 0.98971397)]]\n",
      "65 [0.00049374369, -0.096002862, 0.98971397, [(-0.096002862, 0.98971397)]]\n",
      "65 [0.00049374369, -0.096002862, 0.98971397, [(-0.096002862, 0.98971397)]]\n",
      "65 [0.00040587832, -0.087042488, 0.99067402, [(-0.087042488, 0.99067402)]]\n",
      "66 [0.00040587832, -0.087042488, 0.99067402, [(-0.087042488, 0.99067402)]]\n",
      "66 [0.00040587832, -0.087042488, 0.99067402, [(-0.087042488, 0.99067402)]]\n",
      "66 [0.00033365248, -0.078918815, 0.99154443, [(-0.078918815, 0.99154443)]]\n",
      "67 [0.00033365248, -0.078918815, 0.99154443, [(-0.078918815, 0.99154443)]]\n",
      "67 [0.00033365248, -0.078918815, 0.99154443, [(-0.078918815, 0.99154443)]]\n",
      "67 [0.00027427878, -0.071553268, 0.99233359, [(-0.071553268, 0.99233359)]]\n",
      "68 [0.00027427878, -0.071553268, 0.99233359, [(-0.071553268, 0.99233359)]]\n",
      "68 [0.00027427878, -0.071553268, 0.99233359, [(-0.071553268, 0.99233359)]]\n",
      "68 [0.00022546716, -0.064874649, 0.99304914, [(-0.064874649, 0.99304914)]]\n",
      "69 [0.00022546716, -0.064874649, 0.99304914, [(-0.064874649, 0.99304914)]]\n",
      "69 [0.00022546716, -0.064874649, 0.99304914, [(-0.064874649, 0.99304914)]]\n",
      "69 [0.00018534459, -0.058819771, 0.99369788, [(-0.058819771, 0.99369788)]]\n",
      "70 [0.00018534459, -0.058819771, 0.99369788, [(-0.058819771, 0.99369788)]]\n",
      "70 [0.00018534459, -0.058819771, 0.99369788, [(-0.058819771, 0.99369788)]]\n",
      "70 [0.00015236251, -0.053330101, 0.99428606, [(-0.053330101, 0.99428606)]]\n",
      "71 [0.00015236251, -0.053330101, 0.99428606, [(-0.053330101, 0.99428606)]]\n",
      "71 [0.00015236251, -0.053330101, 0.99428606, [(-0.053330101, 0.99428606)]]\n",
      "71 [0.00012525026, -0.04835292, 0.99481934, [(-0.04835292, 0.99481934)]]\n",
      "72 [0.00012525026, -0.04835292, 0.99481934, [(-0.04835292, 0.99481934)]]\n",
      "72 [0.00012525026, -0.04835292, 0.99481934, [(-0.04835292, 0.99481934)]]\n",
      "72 [0.00010296199, -0.043840133, 0.99530286, [(-0.043840133, 0.99530286)]]\n",
      "73 [0.00010296199, -0.043840133, 0.99530286, [(-0.043840133, 0.99530286)]]\n",
      "73 [0.00010296199, -0.043840133, 0.99530286, [(-0.043840133, 0.99530286)]]\n",
      "73 [8.4640196e-05, -0.03974859, 0.99574125, [(-0.039748594, 0.99574125)]]\n",
      "74 [8.4640196e-05, -0.03974859, 0.99574125, [(-0.039748594, 0.99574125)]]\n",
      "74 [8.4640196e-05, -0.03974859, 0.99574125, [(-0.039748594, 0.99574125)]]\n",
      "74 [6.9576912e-05, -0.03603844, 0.99613875, [(-0.03603844, 0.99613875)]]\n",
      "75 [6.9576912e-05, -0.03603844, 0.99613875, [(-0.03603844, 0.99613875)]]\n",
      "75 [6.9576912e-05, -0.03603844, 0.99613875, [(-0.03603844, 0.99613875)]]\n",
      "75 [5.7194964e-05, -0.032674748, 0.99649912, [(-0.032674748, 0.99649912)]]\n",
      "76 [5.7194964e-05, -0.032674748, 0.99649912, [(-0.032674748, 0.99649912)]]\n",
      "76 [5.7194964e-05, -0.032674748, 0.99649912, [(-0.032674748, 0.99649912)]]\n",
      "76 [4.7017405e-05, -0.029625297, 0.99682587, [(-0.029625297, 0.99682587)]]\n",
      "77 [4.7017405e-05, -0.029625297, 0.99682587, [(-0.029625297, 0.99682587)]]\n",
      "77 [4.7017405e-05, -0.029625297, 0.99682587, [(-0.029625297, 0.99682587)]]\n",
      "77 [3.8650192e-05, -0.026860196, 0.99712211, [(-0.026860196, 0.99712211)]]\n",
      "78 [3.8650192e-05, -0.026860196, 0.99712211, [(-0.026860196, 0.99712211)]]\n",
      "78 [3.8650192e-05, -0.026860196, 0.99712211, [(-0.026860196, 0.99712211)]]\n",
      "78 [3.1773365e-05, -0.024353702, 0.99739069, [(-0.024353702, 0.99739069)]]\n",
      "79 [3.1773365e-05, -0.024353702, 0.99739069, [(-0.024353702, 0.99739069)]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 [3.1773365e-05, -0.024353702, 0.99739069, [(-0.024353702, 0.99739069)]]\n",
      "79 [2.6118389e-05, -0.02208038, 0.99763423, [(-0.02208038, 0.99763423)]]\n",
      "80 [2.6118389e-05, -0.02208038, 0.99763423, [(-0.02208038, 0.99763423)]]\n",
      "80 [2.6118389e-05, -0.02208038, 0.99763423, [(-0.02208038, 0.99763423)]]\n",
      "80 [2.1471042e-05, -0.020019809, 0.99785501, [(-0.020019809, 0.99785501)]]\n",
      "81 [2.1471042e-05, -0.020019809, 0.99785501, [(-0.020019809, 0.99785501)]]\n",
      "81 [2.1471042e-05, -0.020019809, 0.99785501, [(-0.020019809, 0.99785501)]]\n",
      "81 [1.7650131e-05, -0.018151283, 0.99805522, [(-0.018151283, 0.99805522)]]\n",
      "82 [1.7650131e-05, -0.018151283, 0.99805522, [(-0.018151283, 0.99805522)]]\n",
      "82 [1.7650131e-05, -0.018151283, 0.99805522, [(-0.018151283, 0.99805522)]]\n",
      "82 [1.4509256e-05, -0.0164572, 0.99823672, [(-0.0164572, 0.99823672)]]\n",
      "83 [1.4509256e-05, -0.0164572, 0.99823672, [(-0.0164572, 0.99823672)]]\n",
      "83 [1.4509256e-05, -0.0164572, 0.99823672, [(-0.0164572, 0.99823672)]]\n",
      "83 [1.1927115e-05, -0.014921108, 0.99840128, [(-0.014921108, 0.99840128)]]\n",
      "84 [1.1927115e-05, -0.014921108, 0.99840128, [(-0.014921108, 0.99840128)]]\n",
      "84 [1.1927115e-05, -0.014921108, 0.99840128, [(-0.014921108, 0.99840128)]]\n",
      "84 [9.8050714e-06, -0.013528785, 0.99855047, [(-0.013528785, 0.99855047)]]\n",
      "85 [9.8050714e-06, -0.013528785, 0.99855047, [(-0.013528785, 0.99855047)]]\n",
      "85 [9.8050714e-06, -0.013528785, 0.99855047, [(-0.013528785, 0.99855047)]]\n",
      "85 [8.0603377e-06, -0.012266198, 0.99868578, [(-0.012266198, 0.99868578)]]\n",
      "86 [8.0603377e-06, -0.012266198, 0.99868578, [(-0.012266198, 0.99868578)]]\n",
      "86 [8.0603377e-06, -0.012266198, 0.99868578, [(-0.012266198, 0.99868578)]]\n",
      "86 [6.625623e-06, -0.011121074, 0.99880844, [(-0.011121076, 0.99880844)]]\n",
      "87 [6.625623e-06, -0.011121074, 0.99880844, [(-0.011121076, 0.99880844)]]\n",
      "87 [6.625623e-06, -0.011121074, 0.99880844, [(-0.011121076, 0.99880844)]]\n",
      "87 [5.4466977e-06, -0.010083239, 0.99891967, [(-0.010083239, 0.99891967)]]\n",
      "88 [5.4466977e-06, -0.010083239, 0.99891967, [(-0.010083239, 0.99891967)]]\n",
      "88 [5.4466977e-06, -0.010083239, 0.99891967, [(-0.010083239, 0.99891967)]]\n",
      "88 [4.4772573e-06, -0.009141962, 0.99902052, [(-0.009141962, 0.99902052)]]\n",
      "89 [4.4772573e-06, -0.009141962, 0.99902052, [(-0.009141962, 0.99902052)]]\n",
      "89 [4.4772573e-06, -0.009141962, 0.99902052, [(-0.009141962, 0.99902052)]]\n",
      "89 [3.6801773e-06, -0.0082883434, 0.99911195, [(-0.0082883434, 0.99911195)]]\n",
      "90 [3.6801773e-06, -0.0082883434, 0.99911195, [(-0.0082883434, 0.99911195)]]\n",
      "90 [3.6801773e-06, -0.0082883434, 0.99911195, [(-0.0082883434, 0.99911195)]]\n",
      "90 [3.0251654e-06, -0.007514636, 0.99919486, [(-0.007514636, 0.99919486)]]\n",
      "91 [3.0251654e-06, -0.007514636, 0.99919486, [(-0.007514636, 0.99919486)]]\n",
      "91 [3.0251654e-06, -0.007514636, 0.99919486, [(-0.007514636, 0.99919486)]]\n",
      "91 [2.4866304e-06, -0.0068130093, 0.99927002, [(-0.0068130093, 0.99927002)]]\n",
      "92 [2.4866304e-06, -0.0068130093, 0.99927002, [(-0.0068130093, 0.99927002)]]\n",
      "92 [2.4866304e-06, -0.0068130093, 0.99927002, [(-0.0068130093, 0.99927002)]]\n",
      "92 [2.0442119e-06, -0.0061772661, 0.99933815, [(-0.0061772661, 0.99933815)]]\n",
      "93 [2.0442119e-06, -0.0061772661, 0.99933815, [(-0.0061772661, 0.99933815)]]\n",
      "93 [2.0442119e-06, -0.0061772661, 0.99933815, [(-0.0061772661, 0.99933815)]]\n",
      "93 [1.6805577e-06, -0.0056009293, 0.9993999, [(-0.0056009293, 0.9993999)]]\n",
      "94 [1.6805577e-06, -0.0056009293, 0.9993999, [(-0.0056009293, 0.9993999)]]\n",
      "94 [1.6805577e-06, -0.0056009293, 0.9993999, [(-0.0056009293, 0.9993999)]]\n",
      "94 [1.3813963e-06, -0.0050779977, 0.99945593, [(-0.0050779977, 0.99945593)]]\n",
      "95 [1.3813963e-06, -0.0050779977, 0.99945593, [(-0.0050779977, 0.99945593)]]\n",
      "95 [1.3813963e-06, -0.0050779977, 0.99945593, [(-0.0050779977, 0.99945593)]]\n",
      "95 [1.1355544e-06, -0.0046040216, 0.99950671, [(-0.0046040216, 0.99950671)]]\n",
      "96 [1.1355544e-06, -0.0046040216, 0.99950671, [(-0.0046040216, 0.99950671)]]\n",
      "96 [1.1355544e-06, -0.0046040216, 0.99950671, [(-0.0046040216, 0.99950671)]]\n",
      "96 [9.3358233e-07, -0.0041745505, 0.99955273, [(-0.0041745505, 0.99955273)]]\n",
      "97 [9.3358233e-07, -0.0041745505, 0.99955273, [(-0.0041745505, 0.99955273)]]\n",
      "97 [9.3358233e-07, -0.0041745505, 0.99955273, [(-0.0041745505, 0.99955273)]]\n",
      "97 [7.6753042e-07, -0.0037851334, 0.99959445, [(-0.0037851334, 0.99959445)]]\n",
      "98 [7.6753042e-07, -0.0037851334, 0.99959445, [(-0.0037851334, 0.99959445)]]\n",
      "98 [7.6753042e-07, -0.0037851334, 0.99959445, [(-0.0037851334, 0.99959445)]]\n",
      "98 [6.3099611e-07, -0.0034319959, 0.9996323, [(-0.0034319959, 0.9996323)]]\n",
      "99 [6.3099611e-07, -0.0034319959, 0.9996323, [(-0.0034319959, 0.9996323)]]\n",
      "99 [6.3099611e-07, -0.0034319959, 0.9996323, [(-0.0034319959, 0.9996323)]]\n",
      "99 [5.1866851e-07, -0.0031115613, 0.99966663, [(-0.0031115613, 0.99966663)]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    #여기서는 실제로 계산한 gradient와 옵티마이저에서 compute_gradients를 이용해 뽑아낸 gradient가 똑같다는 것을 보여주려는 듯하다\n",
    "    print(step, sess.run([cost,gradient, W, gvs]))\n",
    "    print(step, sess.run([cost,gradient, W, gvs]))\n",
    "    #이미 gvs에서 cost함수가 들어갔기 때문에, apply_gradients는 암묵적으로 그 cost함수를 최소화시키는 쪽으로 작동하는 듯 하다.\n",
    "    sess.run(apply_gradients)\n",
    "    print(step, sess.run([cost,gradient, W, gvs]))\n",
    "    # Same as sess.run(train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "위 코드를 보면 세번째 print(step, sess.run([cost,gradient, W, gvs]))에서 값의 업데이트가 일어나는데, 값의 업데이트는 W가 업데이트 될 때 일어난다,즉  sess.run(apply_gradients)에서 W값이 업데이트 되었다는 이야기이다. 결국 일반적인 train텐서를 두부분으로 나눠서 진행하는 것~어려울 것 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [168.0, -56.0, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-56.0, -5.0)]]\n",
      "0 [168.0, -56.0, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-56.0, -5.0)]]\n",
      "0 [87.091194, -40.32, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-40.32, -3.3199999)]]\n",
      "1 [87.091194, -40.32, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-40.32, -3.3199999)]]\n",
      "1 [87.091194, -40.32, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-40.32, -3.3199999)]]\n",
      "1 [45.148079, -29.030399, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-29.030399, -2.1104)]]\n",
      "2 [45.148079, -29.030399, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-29.030399, -2.1104)]]\n",
      "2 [45.148079, -29.030399, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-29.030399, -2.1104)]]\n",
      "2 [23.404764, -20.90189, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-20.90189, -1.239488)]]\n",
      "3 [23.404764, -20.90189, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-20.90189, -1.239488)]]\n",
      "3 [23.404764, -20.90189, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-20.90189, -1.239488)]]\n",
      "3 [12.133029, -15.04936, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-15.049359, -0.61243135)]]\n",
      "4 [12.133029, -15.04936, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-15.049359, -0.61243135)]]\n",
      "4 [12.133029, -15.04936, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-15.049359, -0.61243135)]]\n",
      "4 [6.2897625, -10.835539, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-10.835539, -0.16095057)]]\n",
      "5 [6.2897625, -10.835539, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-10.835539, -0.16095057)]]\n",
      "5 [6.2897625, -10.835539, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-10.835539, -0.16095057)]]\n",
      "5 [3.260613, -7.8015881, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.8015881, 0.16411556)]]\n",
      "6 [3.260613, -7.8015881, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.8015881, 0.16411556)]]\n",
      "6 [3.260613, -7.8015881, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.8015881, 0.16411556)]]\n",
      "6 [1.690302, -5.6171436, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.6171436, 0.3981632)]]\n",
      "7 [1.690302, -5.6171436, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.6171436, 0.3981632)]]\n",
      "7 [1.690302, -5.6171436, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.6171436, 0.3981632)]]\n",
      "7 [0.87625253, -4.0443435, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-4.0443435, 0.56667751)]]\n",
      "8 [0.87625253, -4.0443435, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-4.0443435, 0.56667751)]]\n",
      "8 [0.87625253, -4.0443435, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-4.0443435, 0.56667751)]]\n",
      "8 [0.4542492, -2.9119267, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9119267, 0.68800783)]]\n",
      "9 [0.4542492, -2.9119267, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9119267, 0.68800783)]]\n",
      "9 [0.4542492, -2.9119267, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9119267, 0.68800783)]]\n",
      "9 [0.23548275, -2.0965872, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.0965872, 0.77536565)]]\n",
      "10 [0.23548275, -2.0965872, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.0965872, 0.77536565)]]\n",
      "10 [0.23548275, -2.0965872, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.0965872, 0.77536565)]]\n",
      "10 [0.12207426, -1.5095427, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.5095427, 0.83826327)]]\n",
      "11 [0.12207426, -1.5095427, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.5095427, 0.83826327)]]\n",
      "11 [0.12207426, -1.5095427, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.5095427, 0.83826327)]]\n",
      "11 [0.063283309, -1.0868709, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.0868709, 0.88354957)]]\n",
      "12 [0.063283309, -1.0868709, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.0868709, 0.88354957)]]\n",
      "12 [0.063283309, -1.0868709, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.0868709, 0.88354957)]]\n",
      "12 [0.032806069, -0.78254712, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.78254712, 0.9161557)]]\n",
      "13 [0.032806069, -0.78254712, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.78254712, 0.9161557)]]\n",
      "13 [0.032806069, -0.78254712, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.78254712, 0.9161557)]]\n",
      "13 [0.017006652, -0.56343371, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.56343371, 0.93963212)]]\n",
      "14 [0.017006652, -0.56343371, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.56343371, 0.93963212)]]\n",
      "14 [0.017006652, -0.56343371, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.56343371, 0.93963212)]]\n",
      "14 [0.0088162553, -0.40567237, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.40567237, 0.9565351)]]\n",
      "15 [0.0088162553, -0.40567237, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.40567237, 0.9565351)]]\n",
      "15 [0.0088162553, -0.40567237, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.40567237, 0.9565351)]]\n",
      "15 [0.0045703324, -0.29208365, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.29208368, 0.9687053)]]\n",
      "16 [0.0045703324, -0.29208365, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.29208368, 0.9687053)]]\n",
      "16 [0.0045703324, -0.29208365, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.29208368, 0.9687053)]]\n",
      "16 [0.0023692569, -0.21030009, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.21030009, 0.97746783)]]\n",
      "17 [0.0023692569, -0.21030009, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.21030009, 0.97746783)]]\n",
      "17 [0.0023692569, -0.21030009, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.21030009, 0.97746783)]]\n",
      "17 [0.0012282291, -0.15141647, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.15141647, 0.98377681)]]\n",
      "18 [0.0012282291, -0.15141647, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.15141647, 0.98377681)]]\n",
      "18 [0.0012282291, -0.15141647, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.15141647, 0.98377681)]]\n",
      "18 [0.0006367194, -0.10902031, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.10902031, 0.98831928)]]\n",
      "19 [0.0006367194, -0.10902031, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.10902031, 0.98831928)]]\n",
      "19 [0.0006367194, -0.10902031, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.10902031, 0.98831928)]]\n",
      "19 [0.00033007405, -0.078494467, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.078494467, 0.9915899)]]\n",
      "20 [0.00033007405, -0.078494467, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.078494467, 0.9915899)]]\n",
      "20 [0.00033007405, -0.078494467, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.078494467, 0.9915899)]]\n",
      "20 [0.000171106, -0.056515299, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.056515299, 0.99394476)]]\n",
      "21 [0.000171106, -0.056515299, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.056515299, 0.99394476)]]\n",
      "21 [0.000171106, -0.056515299, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.056515299, 0.99394476)]]\n",
      "21 [8.8703106e-05, -0.040691417, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.040691417, 0.99564022)]]\n",
      "22 [8.8703106e-05, -0.040691417, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.040691417, 0.99564022)]]\n",
      "22 [8.8703106e-05, -0.040691417, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.040691417, 0.99564022)]]\n",
      "22 [4.5982721e-05, -0.029297512, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.029297512, 0.99686098)]]\n",
      "23 [4.5982721e-05, -0.029297512, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.029297512, 0.99686098)]]\n",
      "23 [4.5982721e-05, -0.029297512, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.029297512, 0.99686098)]]\n",
      "23 [2.3836805e-05, -0.021093925, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.021093927, 0.99773991)]]\n",
      "24 [2.3836805e-05, -0.021093925, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.021093927, 0.99773991)]]\n",
      "24 [2.3836805e-05, -0.021093925, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.021093927, 0.99773991)]]\n",
      "24 [1.2357508e-05, -0.01518794, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.01518794, 0.99837273)]]\n",
      "25 [1.2357508e-05, -0.01518794, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.01518794, 0.99837273)]]\n",
      "25 [1.2357508e-05, -0.01518794, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.01518794, 0.99837273)]]\n",
      "25 [6.4063543e-06, -0.010935505, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.010935505, 0.99882835)]]\n",
      "26 [6.4063543e-06, -0.010935505, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.010935505, 0.99882835)]]\n",
      "26 [6.4063543e-06, -0.010935505, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.010935505, 0.99882835)]]\n",
      "26 [3.3210633e-06, -0.0078735752, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0078735752, 0.99915642)]]\n",
      "27 [3.3210633e-06, -0.0078735752, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0078735752, 0.99915642)]]\n",
      "27 [3.3210633e-06, -0.0078735752, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0078735752, 0.99915642)]]\n",
      "27 [1.7213882e-06, -0.005668561, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.005668561, 0.99939263)]]\n",
      "28 [1.7213882e-06, -0.005668561, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.005668561, 0.99939263)]]\n",
      "28 [1.7213882e-06, -0.005668561, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.005668561, 0.99939263)]]\n",
      "28 [8.9254348e-07, -0.0040817657, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0040817657, 0.99956268)]]\n",
      "29 [8.9254348e-07, -0.0040817657, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0040817657, 0.99956268)]]\n",
      "29 [8.9254348e-07, -0.0040817657, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0040817657, 0.99956268)]]\n",
      "29 [4.6269304e-07, -0.0029388666, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0029388666, 0.99968511)]]\n",
      "30 [4.6269304e-07, -0.0029388666, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0029388666, 0.99968511)]]\n",
      "30 [4.6269304e-07, -0.0029388666, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0029388666, 0.99968511)]]\n",
      "30 [2.3990981e-07, -0.0021162033, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0021162033, 0.99977326)]]\n",
      "31 [2.3990981e-07, -0.0021162033, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0021162033, 0.99977326)]]\n",
      "31 [2.3990981e-07, -0.0021162033, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0021162033, 0.99977326)]]\n",
      "31 [1.2436068e-07, -0.0015236139, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0015236139, 0.99983674)]]\n",
      "32 [1.2436068e-07, -0.0015236139, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0015236139, 0.99983674)]]\n",
      "32 [1.2436068e-07, -0.0015236139, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0015236139, 0.99983674)]]\n",
      "32 [6.4473433e-08, -0.0010970434, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0010970434, 0.99988246)]]\n",
      "33 [6.4473433e-08, -0.0010970434, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0010970434, 0.99988246)]]\n",
      "33 [6.4473433e-08, -0.0010970434, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.0010970434, 0.99988246)]]\n",
      "33 [3.3430563e-08, -0.00078996026, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00078996026, 0.99991536)]]\n",
      "34 [3.3430563e-08, -0.00078996026, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00078996026, 0.99991536)]]\n",
      "34 [3.3430563e-08, -0.00078996026, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00078996026, 0.99991536)]]\n",
      "34 [1.7302327e-08, -0.00056831044, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00056831044, 0.99993908)]]\n",
      "35 [1.7302327e-08, -0.00056831044, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00056831044, 0.99993908)]]\n",
      "35 [1.7302327e-08, -0.00056831044, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00056831044, 0.99993908)]]\n",
      "35 [8.9809573e-09, -0.00040944415, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00040944415, 0.99995613)]]\n",
      "36 [8.9809573e-09, -0.00040944415, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00040944415, 0.99995613)]]\n",
      "36 [8.9809573e-09, -0.00040944415, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00040944415, 0.99995613)]]\n",
      "36 [4.6646704e-09, -0.00029508272, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00029508272, 0.99996841)]]\n",
      "37 [4.6646704e-09, -0.00029508272, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00029508272, 0.99996841)]]\n",
      "37 [4.6646704e-09, -0.00029508272, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00029508272, 0.99996841)]]\n",
      "37 [2.4093807e-09, -0.00021207333, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00021207333, 0.99997729)]]\n",
      "38 [2.4093807e-09, -0.00021207333, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00021207333, 0.99997729)]]\n",
      "38 [2.4093807e-09, -0.00021207333, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00021207333, 0.99997729)]]\n",
      "38 [1.2486083e-09, -0.00015266736, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00015266737, 0.99998367)]]\n",
      "39 [1.2486083e-09, -0.00015266736, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00015266737, 0.99998367)]]\n",
      "39 [1.2486083e-09, -0.00015266736, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00015266737, 0.99998367)]]\n",
      "39 [6.4482819e-10, -0.00010971229, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00010971229, 0.99998826)]]\n",
      "40 [6.4482819e-10, -0.00010971229, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00010971229, 0.99998826)]]\n",
      "40 [6.4482819e-10, -0.00010971229, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-0.00010971229, 0.99998826)]]\n",
      "40 [3.3229244e-10, -7.8757606e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.8757614e-05, 0.99999154)]]\n",
      "41 [3.3229244e-10, -7.8757606e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.8757614e-05, 0.99999154)]]\n",
      "41 [3.3229244e-10, -7.8757606e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.8757614e-05, 0.99999154)]]\n",
      "41 [1.7104659e-10, -5.6505203e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.6505203e-05, 0.99999392)]]\n",
      "42 [1.7104659e-10, -5.6505203e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.6505203e-05, 0.99999392)]]\n",
      "42 [1.7104659e-10, -5.6505203e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.6505203e-05, 0.99999392)]]\n",
      "42 [9.1844761e-11, -4.1405361e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-4.1405365e-05, 0.99999559)]]\n",
      "43 [9.1844761e-11, -4.1405361e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-4.1405365e-05, 0.99999559)]]\n",
      "43 [9.1844761e-11, -4.1405361e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-4.1405365e-05, 0.99999559)]]\n",
      "43 [4.6949111e-11, -2.9603641e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9603641e-05, 0.99999684)]]\n",
      "44 [4.6949111e-11, -2.9603641e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9603641e-05, 0.99999684)]]\n",
      "44 [4.6949111e-11, -2.9603641e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9603641e-05, 0.99999684)]]\n",
      "44 [2.3405279e-11, -2.0901361e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.0901363e-05, 0.99999774)]]\n",
      "45 [2.3405279e-11, -2.0901361e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.0901363e-05, 0.99999774)]]\n",
      "45 [2.3405279e-11, -2.0901361e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.0901363e-05, 0.99999774)]]\n",
      "45 [1.189567e-11, -1.4901161e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.4901161e-05, 0.99999839)]]\n",
      "46 [1.189567e-11, -1.4901161e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.4901161e-05, 0.99999839)]]\n",
      "46 [1.189567e-11, -1.4901161e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.4901161e-05, 0.99999839)]]\n",
      "46 [6.6317325e-12, -1.11262e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.11262e-05, 0.99999881)]]\n",
      "47 [6.6317325e-12, -1.11262e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.11262e-05, 0.99999881)]]\n",
      "47 [6.6317325e-12, -1.11262e-05, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.11262e-05, 0.99999881)]]\n",
      "47 [3.0553338e-12, -7.549922e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.549922e-06, 0.99999917)]]\n",
      "48 [3.0553338e-12, -7.549922e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.549922e-06, 0.99999917)]]\n",
      "48 [3.0553338e-12, -7.549922e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-7.549922e-06, 0.99999917)]]\n",
      "48 [1.8047787e-12, -5.8015185e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.8015185e-06, 0.9999994)]]\n",
      "49 [1.8047787e-12, -5.8015185e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.8015185e-06, 0.9999994)]]\n",
      "49 [1.8047787e-12, -5.8015185e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-5.8015185e-06, 0.9999994)]]\n",
      "49 [7.6383344e-13, -3.774961e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-3.774961e-06, 0.99999958)]]\n",
      "50 [7.6383344e-13, -3.774961e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-3.774961e-06, 0.99999958)]]\n",
      "50 [7.6383344e-13, -3.774961e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-3.774961e-06, 0.99999958)]]\n",
      "50 [4.5119466e-13, -2.9007592e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9007592e-06, 0.9999997)]]\n",
      "51 [4.5119466e-13, -2.9007592e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9007592e-06, 0.9999997)]]\n",
      "51 [4.5119466e-13, -2.9007592e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.9007592e-06, 0.9999997)]]\n",
      "51 [2.6526928e-13, -2.22524e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.22524e-06, 0.99999976)]]\n",
      "52 [2.6526928e-13, -2.22524e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.22524e-06, 0.99999976)]]\n",
      "52 [2.6526928e-13, -2.22524e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-2.22524e-06, 0.99999976)]]\n",
      "52 [1.2908194e-13, -1.5497208e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.5497208e-06, 0.99999982)]]\n",
      "53 [1.2908194e-13, -1.5497208e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.5497208e-06, 0.99999982)]]\n",
      "53 [1.2908194e-13, -1.5497208e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.5497208e-06, 0.99999982)]]\n",
      "53 [9.947599e-14, -1.3510387e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.3510387e-06, 0.99999988)]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 [9.947599e-14, -1.3510387e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.3510387e-06, 0.99999988)]]\n",
      "54 [9.947599e-14, -1.3510387e-06, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-1.3510387e-06, 0.99999988)]]\n",
      "54 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "55 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "55 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "55 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "56 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "56 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "56 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "57 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "57 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "57 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "58 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "58 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "58 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "59 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "59 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "59 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "60 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "60 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "60 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "61 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "61 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "61 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "62 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "62 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "62 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "63 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "63 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "63 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "64 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "64 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "64 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "65 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "65 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "65 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "66 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "66 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "66 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "67 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "67 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "67 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "68 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "68 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "68 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "69 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "69 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "69 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "70 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "70 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "70 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "71 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "71 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "71 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "72 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "72 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "72 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "73 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "73 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "73 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "74 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "74 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "74 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "75 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "75 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "75 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "76 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "76 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "76 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "77 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "77 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "77 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "78 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "78 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "78 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "79 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "79 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "79 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "80 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "80 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "80 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "81 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "81 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "81 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "82 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "82 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "82 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "83 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "83 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "83 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "84 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "84 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "84 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "85 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "85 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "85 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "86 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "86 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "86 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "87 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "87 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "87 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "88 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "88 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "88 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "89 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "89 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "89 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "90 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "90 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "90 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "91 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "91 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "91 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "92 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "92 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "92 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "93 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "93 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "93 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "94 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "94 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "94 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "95 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "95 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "95 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "96 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "96 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "96 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "97 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "97 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "97 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "98 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "98 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "98 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "99 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "99 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n",
      "99 [2.4868997e-14, -6.7551935e-07, array([[ 0.84586352],\n",
      "       [ 0.10391822],\n",
      "       [-0.61499453]], dtype=float32), [(-6.7551935e-07, 0.99999994)]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    #여기서는 실제로 계산한 gradient와 옵티마이저에서 compute_gradients를 이용해 뽑아낸 gradient가 똑같다는 것을 보여주려는 듯하다\n",
    "    print(step, sess.run([cost,gradient, W, gvs]))\n",
    "    print(step, sess.run([cost,gradient, W, gvs]))\n",
    "    #이미 gvs에서 cost함수가 들어갔기 때문에, apply_gradients는 암묵적으로 그 cost함수를 최소화시키는 쪽으로 작동하는 듯 하다.\n",
    "    sess.run(apply_gradients2)\n",
    "    print(step, sess.run([cost,gradient, W, gvs]))\n",
    "    # Same as sess.run(train)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기를 크게 하니까 수렴속도가 빨라졌다, 이건 간단하니까, 무조건 convex한 부분이 하나니까 가능한거고 원래는 함부로 이렇게 못한다,\n",
    "(learning rate)높이는거랑 같은 효과라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20171007: MyBinary를 만들 때(BinaryConnect라는 모델을 텐서플로우로 내가 옮긴거라고 볼 수 있다) 위의 개념을 썼다.\n",
    "일반적인 optimizer는 결국 계산하는 파트와 그걸 적용하여 업데이트하는 파트로 나누어져있는데\n",
    "계산하는 대상은 보통 Variable만 gradient를 계산해준다, 그래서 그 부분을 수정하여 tf.grdient함수를 이용하여 내가 직접 기울기를 계산해야하는 변수를 지정해줘서 새로 compute_gradients파트를 만들었고, apply_gradients부분은 기존의 것을 사용하였다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensor에 대하여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서라는 뼈대를 미리 만들고 세션에서 그걸 실행했을 때 뼈대를 따라 주욱 진행된다..머리로는 알겠는데 아직 개념이 붙지는 않는다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(4,), dtype=int32)\n",
      "[4 4 2 2]\n",
      "Tensor(\"add_3:0\", shape=(4,), dtype=int32)\n",
      "[2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "Z = tf.constant([1, 1, -1, -1])\n",
    "Y=Z+3\n",
    "Z=Z+1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(Y)\n",
    "    print(Y.eval())\n",
    "    print(Z)\n",
    "    print(Z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y에는 Z라는 텐서가 들어간거고, 그래서 [1,1,-1,-1]에 3을 더한 값이 나왔다.\n",
    "그리고 Z=Z+1부분에서는 기존의 constant텐서를 바꿔준게 아니고 add tensor를 하나 새로 만들어준 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_8:0\", shape=(4,), dtype=int32)\n",
      "[4 4 2 2]\n",
      "Tensor(\"add_9:0\", shape=(4,), dtype=int32)\n",
      "[2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#첫번째 방법 assign을 이용해본다\n",
    "Z = tf.constant([1, 1, -1, -1])\n",
    "Y=Z+3\n",
    "ZZ=Z+1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.assign(Z,[0,0,0,0]))\n",
    "    print(Y)\n",
    "    print(Y.eval())\n",
    "    print(ZZ)\n",
    "    print(ZZ.eval())\n",
    "\"\"\"\n",
    "#두번째 방법, 아예 새로 정의해본다, 하지만 콘스탄트로!\n",
    "Z = tf.constant([1, 1, -1, -1])\n",
    "Y=Z+3\n",
    "ZZ=Z+1\n",
    "Z=tf.constant([0,0,0,0])\n",
    "with tf.Session() as sess:\n",
    "    print(Y)\n",
    "    print(Y.eval())\n",
    "    print(ZZ)\n",
    "    print(ZZ.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 새로운 텐서를 만드는게 아니라 기존의 constant텐서의 값을 변화시키려는 시도를 해본다 하지만 실패~\n",
    "첫번째 방법은 assign이라는 함수가 없다며 실패, 두번째방법은 같은 이름,속성의 텐서이지만 사실상 새로운 텐서를 정의한 것임으로 실패.\n",
    "우리는 위의 실험을 통해 tf.constant는 정말 콘스탄트하다는 것, 여기다가 우리가 매트랩 같은데서 하던 것처럼 값을 저장하는게 가능하겠구나..라는 생각을 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1 -1]\n",
      "[4 4 2 2]\n",
      "[0 0 0 0]\n",
      "[3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "z = tf.Variable([1,1,-1,-1])\n",
    "y=z+3\n",
    "zz=z+1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(z.eval())\n",
    "    print(y.eval())\n",
    "    sess.run(z.assign([0,0,0,0]))\n",
    "    print(z.eval())\n",
    "    print(y.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보이는가! 바로 tf.Variable을 통해 살아있는 텐서플로우!를 만들 수 있다. tf.Variable로 전사이클의 어떤 값을 저장하려고 했다..무조건 실패다!! 왜냐면 저절로 변할거거든."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 위의 실험들을 왜했냐면, 내가 하고싶은건 이거다, 한번 optimizer로 돌리고 W가 나오잖아? 그거를 다음 optimize사이클로 전달하고싶은거다, 그래서 전꺼랑 비교해서 W가 변한 녀석들만 어떤 변화를 주고싶은거야. 근데 그걸 어떻게 전달할지가 고민이다. 가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 scope에 대해 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1:0\n",
      "my_scope/var2:0\n",
      "my_scope/Add:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf.name_scope 와 tf.vairable_scope 에 대해 알아보자\n",
    "Let's begin by a short introduction to variable sharing. It is a mechanism in TensorFlow that allows for sharing variables accessed in different \n",
    "parts of the code without passing references to the variable around. \n",
    "The method tf.get_variable can be used with the name of the variable as the argument to either create a new variable \n",
    "with such name or retrieve the one that was created before. This is different from using the tf.Variable constructor \n",
    "which will create a new variable every time it is called (and potentially add a suffix to the variable name \n",
    "if a variable with such name already exists). It is for the purpose of the variable sharing mechanism that a separate type of scope \n",
    "(variable scope) was introduced.\n",
    "\n",
    "As a result, we end up having two different types of scopes:\n",
    "\n",
    "    name scope, created using tf.name_scope\n",
    "    variable scope, created using tf.variable_scope\n",
    "\n",
    "Both scopes have the same effect on all operations as well as variables created using tf.Variable, i.e., \n",
    "the scope will be added as a prefix to the operation or variable name.\n",
    "\n",
    "However, name scope is ignored by tf.get_variable. We can see that in the following example:\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "with tf.name_scope(\"my_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32)\n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "\n",
    "print(v1.name)  # var1:0\n",
    "print(v2.name)  # my_scope/var2:0\n",
    "print(a.name)   # my_scope/Add:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_scope/var1:0\n",
      "my_scope_1/var2:0\n",
      "my_scope_1/Add:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The only way to place a variable accessed using tf.get_variable in a scope is to use a variable scope, as in the following example:\n",
    "\"\"\"\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32)\n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "\n",
    "print(v1.name)  # my_scope/var1:0\n",
    "print(v2.name)  # my_scope/var2:0\n",
    "print(a.name)   # my_scope/Add:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_scope/var:0\n",
      "var_scope/var:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This allows us to easily share variables across different parts of the program, even within different name scopes:\n",
    "name_scope는 텐서플로우 그래프를 보기 간편하게 만들어준다, 그런데 이렇게 scope가 여러겹있다보면 get_variable이 variable을 reuse 할 때 \n",
    "name을 명시하기 어려워 질 수 있다.\n",
    "그래서 name_scope는 무시하고 variable_scope는 인식하도록 해서 get_variable로 하여금 reuse를 간편하게 할 수 있게 하였다.\n",
    "\"\"\"\n",
    "with tf.name_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"var_scope\"):\n",
    "        v = tf.get_variable(\"var\", [1])\n",
    "with tf.name_scope(\"bar\"):\n",
    "    with tf.variable_scope(\"var_scope\", reuse=True):\n",
    "        v1 = tf.get_variable(\"var\", [1])\n",
    "assert v1 == v\n",
    "print(v.name)   # var_scope/var:0\n",
    "print(v1.name)  # var_scope/var:0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 자잘한 지식들\n",
    "### 4.1 placeholder, feed_dict, matmul, boradcasting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [[1., 6., 15.],\n",
    "          [2., 7., 14.],\n",
    "          [3., 8., 13.],\n",
    "          [4., 9., 12.],\n",
    "          [5., 10., 11.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X1 = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "X2 = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "Y1= tf.matmul(X1, W)\n",
    "Y2=Y1+b\n",
    "Y3=X1+X2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholder를 썼으면 뒤에서 feed_dict를 사용해 그에 맞는 데이터를 넣어주어야한다.\n",
    "matmul은 행렬연산에 쓰인다.\n",
    "차원수가 안맞는 행렬을 더 할 때는 저절로 브로드캐스팅이된다, 3*3 행렬에 1*1행렬을 더하게 되면, 1*1은 3*3으로 복사확장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-6.62524891],\n",
       "        [-6.60650206],\n",
       "        [-6.58775473],\n",
       "        [-6.56900787],\n",
       "        [-6.55026054]], dtype=float32), array([[-7.48072004],\n",
       "        [-7.46197319],\n",
       "        [-7.44322586],\n",
       "        [-7.42447901],\n",
       "        [-7.40573168]], dtype=float32), array([-0.85547125], dtype=float32), array([[ 153.,  158.,  167.],\n",
       "        [ 187.,  192.,  199.],\n",
       "        [ 183.,  188.,  193.],\n",
       "        [ 200.,  205.,  208.],\n",
       "        [ 147.,  152.,  153.]], dtype=float32)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run([Y1,Y2,b,Y3],feed_dict={X1:x_data,X2:y_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 데이터 읽고 슬라이싱,shape,len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[  73.   80.   75.  152.]\n",
      " [  93.   88.   93.  185.]\n",
      " [  89.   91.   90.  180.]\n",
      " [  96.   98.  100.  196.]\n",
      " [  73.   66.   70.  142.]]\n",
      "[[  73.   80.   75.]\n",
      " [  93.   88.   93.]\n",
      " [  89.   91.   90.]\n",
      " [  96.   98.  100.]\n",
      " [  73.   66.   70.]]\n",
      "[ 152.  185.  180.  196.  142.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xy = np.loadtxt('./data/Mytest/data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "print(type(xy))\n",
    "print(xy[0:5,:])\n",
    "x_data = xy[0:5, 0:-1]\n",
    "print(x_data)\n",
    "y_data = xy[0:5, -1]\n",
    "print(y_data)\n",
    "#슬라이싱, -1은 마지막 한줄을 가르킨다, 그리고 0:4이면 0,1,2,3 마지막 수가 가르키는 줄은 포함하지않는다\n",
    "#즉 0:-1은 마지막 한줄만 뺴놓고 전부다,\n",
    "#즉 -1은 그냥 마지막 한줄만이다.   여기서는 train용과 test용을 나눠준거라고 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (5, 3) len 5   y_data shape: (5,)\n",
      "test1: 1    test2: 4\n",
      "test3: type <class 'numpy.ndarray'> Shape: (1, 4) Len: 1\n",
      "test4: type <class 'numpy.ndarray'> Shape: (4, 1) Len: 4\n",
      "test5: type <class 'numpy.ndarray'> Shape: (4,) Len: 4\n"
     ]
    }
   ],
   "source": [
    "test1=[[1,2,3,4]]\n",
    "test2=[[1],[2],[3],[4]]\n",
    "test3=np.array([[1,2,3,4]])\n",
    "test4=np.array([[1],[2],[3],[4]])\n",
    "test5=np.array([1,2,3,4])\n",
    "# Make sure the shape and data are OK\n",
    "#저렇게 일반적으로 행렬을 넣으면 데이터타입이 list가 되고, 이 타입은 shape가 없다\n",
    "#보통은 array를 많이 쓰는거 같다,선언 방법은 간단하다, 그냥 저렇게 앞에 저것만 붙여주면 됨..\n",
    "# Len은 보다시피 행의 크기이다\n",
    "#그런데 1차원배열일때는 그냥 배열의 길이라고 보는게 낫겠다.(1차원배열을 행렬로 보면 행이 한개니까..)\n",
    "#굳이 통용되는 규칙을 찾차면 쉐잎에서 첫번째 수를 가져온다.\n",
    "print(\"x_data shape:\",x_data.shape, \"len\",len(x_data),\"  y_data shape:\" , y_data.shape)\n",
    "#print(\"test1: type\",type(test1),\"Shape:\", test1.shape,\"Len:\", len(test1))  #이 코드는 에러가 뜬다.\n",
    "#print(\"test2: type\",type(test2),\"Shape:\", test2.shape,\"Len:\", len(test2))\n",
    "print(\"test1:\",len(test1),\"   test2:\",len(test2))\n",
    "print(\"test3: type\",type(test3),\"Shape:\", test3.shape,\"Len:\", len(test3))\n",
    "print(\"test4: type\",type(test4),\"Shape:\", test4.shape,\"Len:\", len(test4))\n",
    "print(\"test5: type\",type(test5),\"Shape:\", test5.shape,\"Len:\", len(test5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 tf.map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"map_21/TensorArrayStack/TensorArrayGatherV3:0\", shape=(6,), dtype=int64)\n",
      "<map object at 0x7f55f1b44978>\n",
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "squares = tf.map_fn(lambda x: x * x, elems)\n",
    "# squares == [1, 4, 9, 16, 25, 36]\n",
    "\n",
    "elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\n",
    "alternate = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n",
    "# alternate == [-1, 2, -3]\n",
    "\n",
    "elems = np.array([1, 2, 3])\n",
    "alternates = tf.map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\n",
    "# alternates[0] == [1, 2, 3]\n",
    "# alternates[1] == [-1, -2, -3]\n",
    "print(squares)\n",
    "squares_python=map(lambda x: x*x,elems)\n",
    "print(squares_python)\n",
    "print(list(squares_python))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squares를 프린트 해보면 알 수 있듯이 map_fn함수의 실행결과는 값이 아니라 텐서이다, 세션에서 실행 될 것이다.\n",
    "파이썬의 map함수는 map 객체를 반환하기는 하지만 그 자리에서 바로 실행된다, 이것이 파이썬의 map함수와의 차이이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 np.random.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 9 7] [0 6 9 6] [0 3 7 7]\n"
     ]
    }
   ],
   "source": [
    "a1=np.random.choice(10,size=4,replace=True)\n",
    "a2=np.random.choice(10,size=4,replace=True)\n",
    "a3=np.random.choice(10,size=4,replace=True)#True는 결과가 겹치는게 있다.\n",
    "print(a1,a2,a3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 5 6] [1 9 5 8] [7 8 4 6]\n"
     ]
    }
   ],
   "source": [
    "b1=np.random.choice(10,size=4,replace=False)\n",
    "b2=np.random.choice(10,size=4,replace=False)\n",
    "b3=np.random.choice(10,size=4,replace=False)#False는 결과에 겹치는게 없다.\n",
    "print(b1,b2,b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.5 super()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아빠를 닮아 잘생겼다\n",
      "얼굴 말이야\n",
      "아빠를 닮아 잘생겼다\n"
     ]
    }
   ],
   "source": [
    "class father():  # 부모 클래스\n",
    "    def __init__(self, who):\n",
    "        self.who = who\n",
    " \n",
    "    def handsome(self):\n",
    "        print(\"{}를 닮아 잘생겼다\".format(self.who))\n",
    " \n",
    "class sister(father):  # 자식클래스(부모클래스) 아빠매소드를 상속받겠다\n",
    "    def __init__(self, who, where):\n",
    "        super().__init__(who)\n",
    "        self.where = where\n",
    " \n",
    "    def choice(self):\n",
    "        print(\"{} 말이야\".format(self.where))\n",
    " \n",
    "    def handsome(self):\n",
    "       super().handsome()\n",
    "       self.choice()\n",
    " \n",
    "girl = sister(\"아빠\", \"얼굴\")\n",
    "girl.handsome()\n",
    "\n",
    "#- 자식 클래스에서 부모클래스의 내용을 사용하고 싶을경우 사용\n",
    "#아래처럼 그냥 상속 받으면 되지 왜 super가 필요할까?\n",
    "\n",
    "class brother(father):\n",
    "    pass\n",
    "brother=brother(\"아빠\")\n",
    "brother.handsome()\n",
    "\n",
    "#위처럼 하면 여전히 자식클래스에서 부모클래스이 내용을 사용가능하다.\n",
    "#다만 super는 상속 받은 매서드의 내용을 유지하면서 또 무언가를 더 하고싶을 때 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 hstack(flatten), eye->one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([[1],[2],[0]])\n",
    "flat=np.hstack(a)\n",
    "print(flat)\n",
    "b=np.eye(3)\n",
    "print(b[flat])\n",
    "#numpy array에서 []에 열을 넣으면, 그 열의 각 element에 해당하는 row들이 모여서 반환된다.\n",
    "#np.eye와 함께 활용하면 one hot의 효과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.7 Difference between sparse_softmax_cross_entropy_with_logits and softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Having two different functions is a convenience, as they produce the same result.\n",
    "\n",
    "The difference is simple:\n",
    "\n",
    "    For sparse_softmax_cross_entropy_with_logits, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1].\n",
    "    For softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.\n",
    "     즉 전자에는 일반 label을 후자에는 one hot label을 넣어줘야한다.\n",
    "Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits.\n",
    "\n",
    "Another tiny difference is that with sparse_softmax_cross_entropy_with_logits, you can give -1 as a label to have loss 0 on this label.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.8 What is the parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To parse means to break down into smaller components (pieces) of the whole.\n",
    "\n",
    "Suppose you have a servlet that is passed a get request consisting of a\n",
    "string that reads \"name=joe&game=tennis&fame=none\". You would parse the\n",
    "string to get name / value pairs(\"name = joe\", \"game = tennis\", \"fame =none\").\n",
    "흔히 커맨드라인 같은 곳에 인자를 따로따로 명시해주는게 아니라 \n",
    "python3 main.py --model BNN_cifar10 --save BNN_cifar10 --dataset cifar10 --gpu True\n",
    "이런 식으로 한줄로 코드파일을 실행하곤 하는데, 이 때 이 긴 문장을 각각의 단위로 쪼개어 인자로 쓸 수 있게 만드는 것..을\n",
    "파싱이라고 한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 What is the FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 sess.run([a,b,c]) 같은 run에 들어간다는 것의 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= 3.0 \n",
      "b= [-13.20299244] \n",
      "c= [-17.71009064]\n",
      "We expect: [-39.60897827] \n",
      "but we get: [-17.71009064]\n",
      "=====================Case2======================\n",
      "a= 3.0 \n",
      "b= [-0.63937044] \n",
      "c= [-1.91811132]\n",
      "We expect: [-1.91811132] \n",
      "but we get: [-1.91811132]\n",
      "=====================Case3======================\n",
      "a= 3.0 \n",
      "b= [ 5.73916674] \n",
      "c= [ 17.21750069]\n",
      "We expect: [ 17.21750069] \n",
      "but we get: [ 17.21750069]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a=tf.constant(3.0)\n",
    "b=tf.distributions.Normal(loc=0.,scale=5.).sample(1)\n",
    "c=a*b\n",
    "with tf.Session() as sess:\n",
    "    q=sess.run(a)\n",
    "    w=sess.run(b)\n",
    "    e=sess.run(c)\n",
    "    print(\"a=\",q,\"\\nb=\",w,\"\\nc=\",e)\n",
    "    print(\"We expect:\",q*w,\"\\nbut we get:\",e)\n",
    "    print(\"=====================Case2======================\")\n",
    "    q,w,e=sess.run([a,b,c])\n",
    "    print(\"a=\",q,\"\\nb=\",w,\"\\nc=\",e)\n",
    "    print(\"We expect:\",q*w,\"\\nbut we get:\",e)\n",
    "    print(\"=====================Case3======================\")\n",
    "    e,q,w=sess.run([c,a,b])\n",
    "    print(\"a=\",q,\"\\nb=\",w,\"\\nc=\",e)\n",
    "    print(\"We expect:\",q*w,\"\\nbut we get:\",e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "4.0\n",
      "6.0\n",
      "[8.0, 6.0, 9.0]\n",
      "[10.0, 10.0, 11.0]\n",
      "[12.0, 10.0, 13.0]\n",
      "[14.0, 14.0, 15.0]\n",
      "[16.0, 14.0, 17.0]\n",
      "16.0\n",
      "[18.0, 19.0]\n",
      "18.0\n",
      "[20.0, 21.0]\n",
      "20.0\n",
      "[22.0, 23.0]\n",
      "22.0\n",
      "[24.0, 25.0]\n",
      "24.0\n",
      "[26.0, 27.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 위의 예에서 sess.run 안에서 한번에 실행되는 녀석들은 중복되는 실행을 하지않고,\n",
    "# 텐서그래프에서의 순차에 따라 실행이 된다.\n",
    "# 예를 들어서 sess.run([a,b,c])에서 c를 실행 할 때 c=a*b에서 필요한 a,b를 새로 실행하는 것이 아니라 앞에서 a,b를 실행하여 나온 값을 그대로 쓰는 것이다.\n",
    "# 그런데 텐서그래프의 순차라는게 조금 애매한 부분이 있다.\n",
    "# 예를 들어서 아래 예제를 보자\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.Variable(0.0, name='x')\n",
    "\n",
    "with tf.control_dependencies([x]):\n",
    "    xx = tf.assign(x, x+2)\n",
    "# x=tf.add(x,2)\n",
    "y = xx + 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "    for idx in range(3):\n",
    "        print(sess.run(xx))\n",
    "    for idx in range(5):\n",
    "        print( sess.run([xx,x,y]))\n",
    "    for idx in range(5):   #이렇게 해줘야 결과가 제대로 출력된다.\n",
    "        print(sess.run(x))\n",
    "        print( sess.run([xx,y]))\n",
    "\n",
    "# 분명 control_dependencies까지 넣어서 xx전에 x가 실행되도록 했음에도 불구하고 실행 결과는 아래와 같으며\n",
    "# 심지어 실행 할 때 마다 맨 앞자리(x부분)의 출력이 다르다\n",
    "# 아무래도 xx(tf.assign)을 먼저할지 x(variable의 eval)을 먼저할지 갈피를 못잡는 듯 하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 get_collection은 리스트를 반환한다, 그리고 collection에 중복되지않게 넣는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 3, 4, 5]\n",
      "[6, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a=tf.constant(3)\n",
    "b=tf.constant(4)\n",
    "c=tf.constant(5)\n",
    "d=tf.constant(6)\n",
    "if tf.get_collection('mytest')==[]:\n",
    "    tf.add_to_collection('mytest',a)\n",
    "    tf.add_to_collection('mytest',b)\n",
    "    tf.add_to_collection('mytest',c)\n",
    "#조금 더 구체적으로 가자면 내가 추가하고자 하는 변수가 콜렉션에 있는지 확인하고 넣을 수 있다(이름 통해서~)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([d]+tf.get_collection('mytest')))\n",
    "    val=sess.run([d]+tf.get_collection('mytest'))\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3cce4944b6b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "print(str([1,2,3]))\n",
    "import numpy as np\n",
    "a=np.array([3,4,5])\n",
    "print([1,2]+'3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12 gradient_override_map을 이용하여 fluctuate함수의 grad함수 identity화 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"NotEqual_1:0\", shape=(3,), dtype=bool)\n",
      "[ 0.  0.  0.]\n",
      "[ 1.  1.  1.]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[ 1.  2.  3.]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[ 1.  1.  1.]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[ 1.  2.  3.]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[ 2.  3.  4.]\n",
      "[array([ 1.,  1.,  1.], dtype=float32)]\n",
      "[array([ 4.,  6.,  8.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()  #이거 해도 Register한 grad함수는 사라지지않는다. 어떻게 없애지?\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "\n",
    "g=tf.get_default_graph()\n",
    "\"\"\"\n",
    "my_grad함수에 대하여\n",
    "이 부분이 아주 중요하다 @tf.RegisterGradient(\"my_grad\")을 이용하여 grad함수를 등록 할 수 있다.\n",
    "등록하는 함수의 형식은 정해져 있다 def 함수명(op,grad)이다.\n",
    "위에서 각각 뭘 하는지 설명해놓았지만 여기서 한번 더 설명한다. op는 이 grad함수를 쓰게 될 텐서플로우 operation이다.\n",
    "tf.equal,tf.cast.tf.add,tf.greater 등이 모두 포함된다. 여기서 등록 하는 함수가 \"my_grad\"라는 이름을 가진 것처럼\n",
    "원래 텐서플로우의 오퍼레션들은 자신들의 grad함수의 이름이 있다(혹은 None) 그 이름은 지금까지의 확인 결과 그냥 operation name이다.\n",
    "\n",
    "grad는 op의 아웃풋과 이 op의 gradient다, 여기서 아웃풋이란 이 op에서 뻗어져나가 최종적으로 닫는 결과까지, 혹은 우리가 관찰하고자 하는 끝까지를 말한다.\n",
    "예를 들어서 W->Wbin->Wfluc->cost가 있는데 내가 tf.gradients(Wfluc,Wbin)을 하면 Wfluc에게 있어 grad는 그냥 자기 자신이다. 즉 1이라는 것이다. 왜냐면 우리가\n",
    "관찰하고자 하는 끝점이 자기자신이기 때문이다. 말이 어려워지는데 사실 간단히 말하면 이 op까지 계산 된 gradient 자체를 말한다.\n",
    "여기서 주의 할 점은 두가지다\n",
    "1) 아웃풋과 op의 grad라는 점을 다시 한번 명심하자, backpropagation인거다, gradient는 거꾸로 거슬러 인풋까지 올라온다.\n",
    "그래서 Wbin,Wfluc에서 gradient를 identity로 만들면, cost를 Wfluc으로 미분한 값이 그대로 W까지 전달이 되는 것이다.\n",
    "2) 아웃풋에 관련되어 있기에, 만약 op의 인풋을 n개,아웃풋을 m개라고 한다면 grad는 m개의 원소가 있다. \n",
    "그리고 return은 인풋에 관한 것이기에 n개의 원소를 리턴해줘야한다.\n",
    "각각의 return값은 각각의 인풋과 같은 shape를 가지고 있어야한다. 아래와 같이 하면 인풋 두개가 다른 쉐잎을 가지고 있을 때 오류가 뜰 것이다.\n",
    "#아래는 초기의 my_grad함수\n",
    "# @tf.RegisterGradient(\"my_grad\")\n",
    "# def my_grad(op,grad):\n",
    "#     return grad,grad\n",
    "\"\"\"\n",
    "#아래는 초기의 my_grad함수\n",
    "# @tf.RegisterGradient(\"my_grad\")\n",
    "# def my_grad(op,grad):\n",
    "#     return grad,op.inputs[1]\n",
    "#위의 함수는 op.inputs[1]을 통해 함수의 두번쨰 인풋에 관한 grad는 버린셈이다.\n",
    "#그래서 a*b*c 같은 형식에서 a,b에 모두 x가 들어가 있을 때 심각한 오차를 만들었다.\n",
    "\n",
    "#아래는 실제로 사용 할 my_grad함수\n",
    "# @tf.RegisterGradient(\"my_grad\")\n",
    "# def my_grad(op,grad):\n",
    "#     shape=op.inputs[1]._shape_as_list()     #텐서 쉐잎 리스트로 얻는 법\n",
    "#     return grad,tf.zeros(shape=shape)\n",
    "#사실 이 함수도 두번째 인풋에 관한 grad는 버린 셈인데, 아예 제대로 버린거다. 이게 왜 가능하냐면\n",
    "#우리는 결국 gradient=1만 얻으면 된다. 그렇기에 a*b*c여도 a에서만 grad=1(identity)취해주고 뒤에서 나오는 grad는 다 버리면 된다.\n",
    "x=tf.constant([1.,1.,1.])\n",
    "pre_Wbin=[0.,0.,0.]\n",
    "pre_Wfluc=[1.1,2.2,3.3]\n",
    "fluc_Reset=[1.,2.,3.]\n",
    "fluc_Set=[555,23.1,11.5]\n",
    "\n",
    "with g.gradient_override_map({\"Mul\":\"my_grad\",\"Cast\":\"Identity\",\"Greater\":\n",
    "                              \"my_grad\",\"Equal\":\"my_grad\",\"LessEqual\":\"my_grad\",\n",
    "                             \"NotEqual\":\"my_grad\",\"Add\":\"my_grad\"}):\n",
    "        keep_element = tf.cast(tf.equal(x,pre_Wbin), tf.float32)\n",
    "        update_element = tf.cast(tf.not_equal(x,pre_Wbin), tf.float32)\n",
    "        Wfluc_Reset = update_element * fluc_Reset \n",
    "        Wfluc_Reset2=tf.cast(x > 0, tf.float32)*2  #grad함수에서 첫번쨰 인자는 살리고, 두번째 인자는 죽이기에 만약 2를 곱하려면 뒤에 곱해야 한다.  \n",
    "        Wfluc_Reset3=Wfluc_Reset*Wfluc_Reset2     #혹시 같은 연산을 쪼개면 gradient가 바뀔까싶어 쪼개보았지만 똑같다.\n",
    "        Wfluc_Set = update_element * fluc_Set * tf.cast(x <= 0, tf.float32)\n",
    "        Wfluc = tf.multiply(x, update_element)+tf.cast(tf.greater(pre_Wbin,0), tf.float32) * keep_element * pre_Wfluc * 1. + \\\n",
    "                    tf.cast(tf.less_equal(pre_Wbin,0), tf.float32) * keep_element * pre_Wfluc * 1. \\\n",
    "                     + Wfluc_Reset + Wfluc_Set\n",
    "        #여기서도 마찬가지다 원래 tf.multiply(x, update_element)만 살릴 것이므로 맨 앞에 둔다, 뒤에는 이제 어떻게 놔도 딱히 상관이 없을 것이다.\n",
    "        #grad를 저렇게 특수하게 쓴 이상 식도 마음대로 못쓰는 제약이 생겨버린다. 그리고 저 수많은 ops에 grad를 덮어써야한다.\n",
    "y=Wfluc*Wfluc\n",
    "\n",
    "print(ops.get_gradient_function(keep_element.op)) #gradient함수가 있는지 확인 해볼 수 있는 함수\n",
    "print(update_element.op.name)                   #op이름 얻는 법\n",
    "\n",
    "line1=tf.gradients(keep_element,x)\n",
    "line2=tf.gradients(update_element,x)\n",
    "line3=tf.gradients(Wfluc_Reset,x)\n",
    "line4=tf.gradients(Wfluc_Reset2,x)\n",
    "line5=tf.gradients(Wfluc_Reset3,x)\n",
    "line6=tf.gradients(Wfluc_Set,x)\n",
    "line7=tf.gradients(Wfluc,x)\n",
    "line8=tf.gradients(y,x)         #최종적으로 계산하는 gradient가 거꾸로 온 것인지(Wfluc을 기준으로 계산하여 W까지 전달)\n",
    "                                #forward해서 온 것인지(W를 기준으로 계산하여 Wfluc까지 전달,어차피 이건 부자연스럽네!)를 알기 위해 씀\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(keep_element))\n",
    "    print(sess.run(line1))\n",
    "    print(sess.run(update_element))\n",
    "    print(sess.run(line2))\n",
    "    print(sess.run(Wfluc_Reset))\n",
    "    print(sess.run(line3))\n",
    "    print(sess.run(Wfluc_Reset2))\n",
    "    print(sess.run(line4))\n",
    "    print(sess.run(Wfluc_Reset3))\n",
    "    print(sess.run(line5))\n",
    "    print(sess.run(line6))\n",
    "    print(sess.run(Wfluc))\n",
    "    print(sess.run(line7))\n",
    "    print(sess.run(line8))     #결과를 통해 Wfluc=[2,3,4]를 통해 계산한 gradient라는걸 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "#Get tensor shape as list:\n",
    "a=tf.constant([[3,32,2],[2,2,2]])\n",
    "dir(a)\n",
    "print(a._shape_as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
