{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. num_cell과 num_level에 따른 적절한 max_level은 아래와 같다.\n",
    "아래와 같은 표를 만들어서 한번 볼 필요가 있는 이유는, 내가 max_level(하나의 weight를 표현하는데 사용 할 총레벨 수)을 100이라고 잡았다고 치자, 그리고 셀하나에 8레벨을 써, 근데 2 cell을 쓰면 8^2=64레벨밖에 표현 못하기 때문에 3 cells을 써야하는데 그렇게 되면 8^3=512레벨을 표현하게 된다. 이런 potential level(잠재적으로 실현가능한 max_level)을 가진 녀석을 고작 100레벨 표현하는데 쓰는건 비효율적이고 손해다. 그래서 각 세팅에 따른 적절한 max_level을 알아두는 것이 좋다.\n",
    "MNIST가 6비트=64레벨에서 94프로 학습을 이룰 수 있으니까 64레벨을 이루려면..이게 좋다! 이런 식으로 감을 잡을 수 있지않을까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[  1   4   9  16  25  36  49  64  81 100 121 144 169 196 225 256 289 324\n",
      " 361 400]\n",
      "[   1    8   27   64  125  216  343  512  729 1000 1331 1728 2197 2744\n",
      " 3375 4096 4913 5832 6859 8000]\n",
      "[     1     16     81    256    625   1296   2401   4096   6561  10000\n",
      "  14641  20736  28561  38416  50625  65536  83521 104976 130321 160000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "max_numoflevel_per_cell=20\n",
    "max_numofcell_per_weight=10\n",
    "table=np.zeros([max_numofcell_per_weight,max_numoflevel_per_cell],dtype=np.int32)\n",
    "for num_level in range(1,max_numoflevel_per_cell+1):\n",
    "    for num_cell in range(1,max_numofcell_per_weight+1):\n",
    "        table[num_cell-1,num_level-1]=num_level**num_cell\n",
    "print(table[0,:])\n",
    "print(table[1,:])\n",
    "print(table[2,:])\n",
    "print(table[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 사소한 팁 1:브로드캐스팅\n",
    "참묘하게 직관적이란 말이야, 근데 원리를 딱 한마디로 설명은 못하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]\n",
      "  [0. 1. 2. 3. 4.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[30., 30., 30., 30.],\n",
       "       [30., 30., 30., 30.],\n",
       "       [30., 30., 30., 30.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.zeros(shape=[3,4,5])\n",
    "a[:,:,0]=0\n",
    "a[:,:,1]=1\n",
    "a[:,:,2]=2\n",
    "a[:,:,3]=3\n",
    "a[:,:,4]=4\n",
    "print(a)\n",
    "np.sum(a*np.array([0,1,2,3,4]),axis=-1)  #axis=-1도 통하는구나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사소한 팁 2: random function \n",
    "randomness의 generation결과는 텐서가 추가 됨에 따라 달라진다. 그래서 만약 flow에 다른 텐서를 추가했다면 그 틀이 실질적인 영향을 주지는 않더라도 randomness의 generation결과가 다르기 때문에 random seed를 고정해도 기존과 같은 결과를 얻을 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[-0.6436417  -0.94902575  0.3808462 ]\n",
      " [-0.40808907 -0.90408695  0.7453628 ]\n",
      " [-0.44379586  1.0692946   0.02469475]]\n",
      "b= [[ 1.71718429  0.32469333 -0.4745434 ]\n",
      " [-0.1379183   0.52493957 -0.35783583]\n",
      " [-0.72820492 -0.64208645 -0.31704766]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(333)\n",
    "np.random.seed(333)\n",
    "a=tf.random_normal(shape=[3,3])\n",
    "b=np.random.normal(size=[3,3])\n",
    "with tf.Session() as sess:\n",
    "    print(\"a=\",sess.run(a))\n",
    "    print(\"b=\",b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[[-1.1583331  -1.9289364   0.1853901 ]]\n",
      "\n",
      " [[-0.32068866  1.3227402   0.5897821 ]]\n",
      "\n",
      " [[-1.3542254  -2.1340234  -0.8468843 ]]]\n",
      "b= [[[ 1.71718429  0.32469333 -0.4745434 ]]\n",
      "\n",
      " [[-0.1379183   0.52493957 -0.35783583]]\n",
      "\n",
      " [[-0.72820492 -0.64208645 -0.31704766]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(333)\n",
    "np.random.seed(333)\n",
    "c=tf.Variable(tf.zeros(shape=[3,3]))\n",
    "# d=tf.constant(value=tf.zeros(shape=[3,3]))\n",
    "a=tf.random_normal(shape=[3,1,3])\n",
    "b=np.random.normal(size=[3,1,3])\n",
    "with tf.Session() as sess:\n",
    "    print(\"a=\",sess.run(a))\n",
    "    print(\"b=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. split and convert 함수 버전1\n",
    "버전1인데..ㅎ 실행은 안된다, 안되는 이유는 적절하지 못한 방법으로 numpy와 tensorflow를 혼용했기 때문이다. 이 부분이 텐서플로우를 어렵게 만든다.\n",
    "텐서플로우의 텐서는 기본적으로 삭제되고 새로 생성되고를 반복하는게 아니다. 이 말이 무슨 말이냐면,<br> \n",
    "1) 일반 코드로 iteration을 한다고 하면 매번 특정 matrix를 새로 만들어 쓰고 버리고를 반복하지만 텐서플로우는 틀을 만들어놓고, 그 틀에 맞춰 flow를 주고받는다.<br>\n",
    "2) numpy를 코드의 맨 앞부분에 써서 flow의 인풋으로 넣는건 가능하다. 그런데 아마도 flow중간에 numpy를 넣는건 안되는거 같다.<br>\n",
    "이게 무슨 말인지는 내가 했지만 나도 잘모른다, 엄밀하지 못한데 어쨌든 이런 느낌이다. 그래서 그 flow를 정해줘야한다.\n",
    "\n",
    "덧1:고치다보니 여기다 그냥 고쳤네..버전1, numpy랑 섞어쓴건 없어진 셈이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def split_level(level,array_shape, num_cell=None,num_level=None,mode=0):\n",
    "    assert num_cell!=None and num_level!=None,\"Please input num_cell and num_level\"\n",
    "    splited_level = tf.Variable(tf.transpose(tf.zeros(shape=array_shape)),dtype=tf.float32)\n",
    "    level = tf.transpose(tf.cast(level,dtype=tf.float32))\n",
    "    if mode==0:\n",
    "        quotient=level  #level-1 if level=1,2,3,4,5,6...\n",
    "        for i in range(num_cell):\n",
    "            print(level)\n",
    "            print(i,(quotient % num_level))\n",
    "            print(splited_level[num_cell - i - 1,:] )\n",
    "            assign_splited=tf.assign(splited_level[num_cell - i - 1,:], tf.cast(quotient%num_level,dtype=tf.float32))\n",
    "            with tf.control_dependencies([assign_splited]): \n",
    "                quotient = (quotient - splited_level[num_cell - i - 1, :]) / num_level\n",
    "        print(\"Original mode\")\n",
    "    elif mode==1:\n",
    "        print(\"Simplified mode\")\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise ValueError(\"Error! Please select correct mode.\")\n",
    "    return tf.transpose(splited_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose_31:0\", shape=(8, 3), dtype=float32)\n",
      "0 Tensor(\"mod_46:0\", shape=(8, 3), dtype=float32)\n",
      "Tensor(\"strided_slice_67:0\", shape=(8, 3), dtype=float32)\n",
      "Tensor(\"transpose_31:0\", shape=(8, 3), dtype=float32)\n",
      "1 Tensor(\"mod_48:0\", shape=(8, 3), dtype=float32)\n",
      "Tensor(\"strided_slice_70:0\", shape=(8, 3), dtype=float32)\n",
      "Tensor(\"transpose_31:0\", shape=(8, 3), dtype=float32)\n",
      "2 Tensor(\"mod_50:0\", shape=(8, 3), dtype=float32)\n",
      "Tensor(\"strided_slice_73:0\", shape=(8, 3), dtype=float32)\n",
      "Original mode\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "splited_level=split_level(5*np.ones([3,8]),[3,8,3],3,3)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(splited_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-왜 결과가 전부 0으로 나오지?\n",
    "-내 코드에는 tf.cast가 왜 이리 많이 나오냐.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotient= Tensor(\"truediv_111:0\", shape=(8, 3), dtype=float32)\n",
      "quotient= Tensor(\"truediv_112:0\", shape=(8, 3), dtype=float32)\n",
      "quotient= Tensor(\"truediv_113:0\", shape=(8, 3), dtype=float32)\n",
      "Original mode\n",
      "[<tf.Variable 'Variable_60:0' shape=(3, 8, 3) dtype=float32_ref>, <tf.Variable 'Variable_60:0' shape=(3, 8, 3) dtype=float32_ref>, <tf.Variable 'Variable_60:0' shape=(3, 8, 3) dtype=float32_ref>]\n",
      "Tensor(\"transpose_125:0\", shape=(3, 8, 3), dtype=float32)\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "[array([[5., 5., 5.],\n",
      "       [5., 5., 5.],\n",
      "       [5., 5., 5.],\n",
      "       [5., 5., 5.],\n",
      "       [5., 5., 5.],\n",
      "       [5., 5., 5.],\n",
      "       [5., 5., 5.],\n",
      "       [5., 5., 5.]], dtype=float32), array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32), array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)]\n",
      "[[[0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]]\n",
      "\n",
      " [[0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]]\n",
      "\n",
      " [[0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]\n",
      "  [0. 1. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "splited_level,probe_list,probe_list2=split_level(5*np.ones([3,8]),[3,8,3],3,3)\n",
    "print(probe_list2)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run(remainder))\n",
    "    print(splited_level)\n",
    "    print(sess.run(splited_level)) # 원하지 않는 결과!\n",
    "    print(sess.run(probe_list))\n",
    "    print(sess.run(splited_level)) # 원하는 결과!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_level(level,array_shape, num_cell=None,num_level=None,mode=0):\n",
    "    assert num_cell!=None and num_level!=None,\"Please input num_cell and num_level\"\n",
    "    splited_level = tf.Variable(tf.transpose(tf.zeros(shape=array_shape)),dtype=tf.float32,trainable=False)\n",
    "    level = tf.transpose(tf.cast(level,dtype=tf.float32))\n",
    "    if mode==0:\n",
    "        quotient=level  #level-1 if level=1,2,3,4,5,6...\n",
    "        for i in range(num_cell):\n",
    "            assign_splited=tf.assign(splited_level[num_cell - i - 1,:], tf.cast(quotient%num_level,dtype=tf.float32))\n",
    "            with tf.control_dependencies([assign_splited]): \n",
    "                quotient = (quotient - splited_level[num_cell - i - 1, :]) / num_level\n",
    "        with tf.control_dependencies([quotient]):\n",
    "            splited_level=tf.identity(splited_level)\n",
    "        print(\"Original mode\")\n",
    "    elif mode==1:\n",
    "        print(\"Simplified mode\")\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise ValueError(\"Error! Please select correct mode.\")\n",
    "    return tf.transpose(splited_level)\n",
    "\n",
    "def convert_level(splited_level, filter_shape, num_cell=None,num_level=None,mode=0):\n",
    "    assert num_cell != None and num_level != None, \"Please input num_cell and num_level\"\n",
    "    bit_weight=num_level**np.arange(num_cell-1,-1,-1)\n",
    "    if mode==0:\n",
    "        converted_level=tf.reduce_sum(splited_level*bit_weight,axis=-1)\n",
    "    elif mode==1:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise ValueError(\"Error! Please select correct mode.\")\n",
    "    return converted_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original mode\n",
      "randomint= (5, 4) \n",
      " [[17 24  0 24]\n",
      " [ 5 22  9  9]\n",
      " [20 21 24 11]\n",
      " [ 6 18 21  6]\n",
      " [ 8  7 13 16]]\n",
      "splited_level= (5, 4, 3) \n",
      " [[[1. 2. 2.]\n",
      "  [2. 2. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [2. 2. 0.]]\n",
      "\n",
      " [[0. 1. 2.]\n",
      "  [2. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[2. 0. 2.]\n",
      "  [2. 1. 0.]\n",
      "  [2. 2. 0.]\n",
      "  [1. 0. 2.]]\n",
      "\n",
      " [[0. 2. 0.]\n",
      "  [2. 0. 0.]\n",
      "  [2. 1. 0.]\n",
      "  [0. 2. 0.]]\n",
      "\n",
      " [[0. 2. 2.]\n",
      "  [0. 2. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 2. 1.]]]\n",
      "converted_level= (5, 4) \n",
      " [[17. 24.  0. 24.]\n",
      " [ 5. 22.  9.  9.]\n",
      " [20. 21. 24. 11.]\n",
      " [ 6. 18. 21.  6.]\n",
      " [ 8.  7. 13. 16.]]\n"
     ]
    }
   ],
   "source": [
    "randomint=np.random.randint(low=0,high=26,size=[5,4])\n",
    "splited_level=split_level(randomint,[5,4,3],3,3)\n",
    "converted_level=convert_level(splited_level,[5,4],num_cell=3,num_level=3)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"randomint=\",randomint.shape,\"\\n\",randomint)\n",
    "    print(\"splited_level=\",splited_level.shape,\"\\n\",sess.run(splited_level))\n",
    "    print(\"converted_level=\",converted_level.shape,\"\\n\",sess.run(converted_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "와 된다, 쩔었다..비교적 빨리 해결함, 역시 그냥 붙잡고 있는다고 되는게 아니야~<br>\n",
    "꼭 기억하자, 텐서플로우는 절차적코드가 아니라, 코드를 통해 flow를 짜는거야, flow 못만들면 절차 다 소용없어!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "55"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
