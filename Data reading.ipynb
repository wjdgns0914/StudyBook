{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader+batching+coordinator = Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "먼저 가장 간단하게 MNIST를 불러와보자\n",
    "\"\"\"\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "datapath=\"./MNIST\"\n",
    "data=input_data.read_data_sets(datapath,one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet'>\n",
      "<class 'numpy.ndarray'>\n",
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(55000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAADuCAYAAACnM7W+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npkZQRLymRlBEvKZGUES8pkZQRLz2PwdgL4Cj0a+eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61dc035748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(data))   \n",
    "print(type(data.train))   #이 두개는 뭔지 모르는 타입이다, 그냥 두자 \n",
    "print(type(data.train.images))  #numpy array이다.\n",
    "print((data.train.images).shape)\n",
    "print((data.test.images).shape)\n",
    "print(data.train.labels.shape)  #numpy에서 shape은 괄호 안친다.\n",
    "\"\"\"\n",
    "위의 쉐잎에서 이 방법은 모든 데이터를 메모리에 올린다는 것을 알 수 있다.\n",
    "앞의 데이터 9개를 한번 보도록 하자\n",
    "\"\"\"\n",
    "images=data.train.images\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i].reshape([28,28]), cmap='binary')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "이제 queue를 이용하여 데이터를 불러올 것이다.\n",
    "1.어떤 파일에서 데이터를 불러올 것인지를 명시한다, MNIST는 이미지파일,라벨파일이 나누어져있다. 그래서 두개를 써준다.\n",
    "2.어떤 리더기로 데이터를 읽어올 것인지를 명시한다, MNIST,CIFAR 같은 데이터셋은 보통 바이너리화 된 데이터파일이다, 그래서 FixedLengthRecordReader를 써준다.\n",
    "tip:http://yann.lecun.com/exdb/mnist/ 여기에 가면 이 데이터파일에 대한 구성정보가 나와있다, 예를 들어 이미지파일은 헤더가 16바이트 있다고한다.\n",
    "->header_bytes=16\n",
    "3.리더기와 파일목록을 연결시켜준다. (key,value)를 리턴하는데 key는 이 설정대로 읽었을 때 몇번째 데이터인지를 나타낸다, value는 데이터의 구체적인 정보\n",
    "4.binary데이터를 읽은 것이기에 이걸 디코드해주어야한다, 8비트씩 묶어주면 된다.\n",
    "\"\"\"\n",
    "def __read_cifar(filenames, shuffle=False, cifar100=False):\n",
    "  \"\"\"Reads and parses examples from MNIST data files.\"\"\"\n",
    "  filename_queue_image = tf.train.string_input_producer(filenames[0], shuffle=shuffle,num_epochs=None)\n",
    "  filename_queue_label = tf.train.string_input_producer(filenames[1], shuffle=shuffle,num_epochs=None)\n",
    "  label_bytes = 1\n",
    "  height = 28\n",
    "  width = 28\n",
    "  depth = 1\n",
    "  image_bytes = height * width * depth\n",
    "  reader_image = tf.FixedLengthRecordReader(record_bytes=image_bytes,header_bytes=16)\n",
    "  reader_label = tf.FixedLengthRecordReader(record_bytes=label_bytes,header_bytes=8)\n",
    "  key_image, value_image = reader_image.read(filename_queue_image)\n",
    "  key_label, value_label = reader_label.read(filename_queue_label)\n",
    "\n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_bytes_image= tf.decode_raw(value_image, tf.uint8)\n",
    "  record_bytes_label = tf.decode_raw(value_label, tf.uint8)\n",
    "\n",
    "  # The first bytes represent the label, which we convert from uint8->int32.\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "    \n",
    "    \n",
    "  depth_major = tf.reshape(tf.slice(record_bytes_image, [0], [image_bytes]),  #def slice(input_, begin, size, name=None):\n",
    "                           [depth, height, width])\n",
    "  label = tf.cast(tf.slice(record_bytes_label, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "  image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "  return tf.cast(image, tf.float32),label,key_image,key_label\n",
    "# image=__read_cifar(['./train-images.idx3-ubyte'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.여기서는 batching을 해준다, 이걸 안해주더라도 데이터를 읽을 수는 있다.\n",
    "\"\"\"\n",
    "class DataProvider:\n",
    "    def __init__(self, data, size=None, training=True):\n",
    "        self.size = size or [None]*4\n",
    "        self.data = data\n",
    "        self.training = training\n",
    "\n",
    "    def next_batch(self, batch_size, min_queue_examples=1000, num_threads=8):\n",
    "        \"\"\"Construct a queued batch of images and labels.\n",
    "\n",
    "        Args:\n",
    "        image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "        label: 1-D Tensor of type.int32\n",
    "        min_queue_examples: int32, minimum number of samples to retain\n",
    "        in the queue that provides of batches of examples.\n",
    "        batch_size: Number of images per batch.\n",
    "\n",
    "        Returns:\n",
    "        images: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "        labels: Labels. 1D tensor of [batch_size] size.\n",
    "        \"\"\"\n",
    "        # Create a queue that shuffles the examples, and then\n",
    "        # read 'batch_size' images + labels from the example queue.\n",
    "\n",
    "        image, label,key_image,key_label = self.data\n",
    "        if self.training:\n",
    "            images, label_batch,key_images,key_labels = tf.train.shuffle_batch(\n",
    "            [image, label,key_image,key_label],\n",
    "            batch_size=batch_size,\n",
    "            num_threads=24,\n",
    "            capacity=min_queue_examples + 3 * batch_size,\n",
    "            min_after_dequeue=min_queue_examples)\n",
    "        else:\n",
    "            images, label_batch = tf.train.batch(\n",
    "            [preprocess_evaluation(image, height=self.size[1], width=self.size[2]), label],\n",
    "            batch_size=batch_size,\n",
    "            num_threads=num_threads,\n",
    "            capacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "        return images, tf.reshape(label_batch, [batch_size]),key_images,key_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MNIST/train-images.idx3-ubyte\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a=DataProvider(__read_cifar([[datapath+'/train-images.idx3-ubyte'],[datapath+'/train-labels.idx1-ubyte']]), [55000, 28,28,1], True)\n",
    "print(datapath+'/train-images.idx3-ubyte')\n",
    "import os\n",
    "print(os.path.isdir(datapath))\n",
    "print(os.path.isfile(datapath+'/train-images.idx3-ubyte'))\n",
    "print(os.path.isfile(datapath+'/train-images.idx3-ubyt2e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"shuffle_batch_2:0\", shape=(64, 28, 28, 1), dtype=float32) Tensor(\"Reshape_59:0\", shape=(64,), dtype=int32)\n",
      "Tensor(\"shuffle_batch_2:2\", shape=(64,), dtype=string) Tensor(\"shuffle_batch_2:3\", shape=(64,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "b,c,d,e=a.next_batch(64)\n",
    "print(b,c)\n",
    "print(d,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Coordinator가 꼭 필요하다, 위에서 나온건 겉으로 보기에는 멀쩡해보이지만 조정담당이 없으면 데이터를 한개도!! 뱉어내지 못한다.\n",
    "\"\"\"\n",
    "sess=tf.InteractiveSession()\n",
    "coord=tf.train.Coordinator()\n",
    "threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot reshape a tensor with 50176 elements to shape [28,28,1] (784 elements) for 'Reshape_60' (op: 'Reshape') with input shapes: [64,28,28,1], [3] and with input tensors computed as partial shapes: input[1] = [28,28,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot reshape a tensor with 50176 elements to shape [28,28,1] (784 elements) for 'Reshape_60' (op: 'Reshape') with input shapes: [64,28,28,1], [3] and with input tensors computed as partial shapes: input[1] = [28,28,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d6a88627bf4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexamples_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexamples_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtemp_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mexamples_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexamples_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   2617\u001b[0m   \"\"\"\n\u001b[1;32m   2618\u001b[0m   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\n\u001b[0;32m-> 2619\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m   2620\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot reshape a tensor with 50176 elements to shape [28,28,1] (784 elements) for 'Reshape_60' (op: 'Reshape') with input shapes: [64,28,28,1], [3] and with input tensors computed as partial shapes: input[1] = [28,28,1]."
     ]
    }
   ],
   "source": [
    "examples_img,examples_label=[],[]\n",
    "for i in range(9):\n",
    "    temp_img,temp_label=sess.run([tf.reshape(b,shape=[28,28,1]),c])\n",
    "    examples_img.append(temp_img)\n",
    "    examples_label.append(temp_label)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(examples_img[i].reshape([28,28]), cmap='binary')\n",
    "    ax.set_xlabel(str(examples_label[i]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6303 7192 6368 6542 6225 5776 6316 6669 6246 6363]\n"
     ]
    }
   ],
   "source": [
    "count_num=np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "for i in range(1000):\n",
    "    images,labels=sess.run([b,c])\n",
    "    unique_elements,elements_counts=np.unique(labels,return_counts=True)\n",
    "    num_set=dict(zip(unique_elements,elements_counts))\n",
    "    for ii in range(10):\n",
    "        if num_set.__contains__(ii):\n",
    "            count_num[ii]=count_num[ii]+num_set[ii]\n",
    "\n",
    "print(count_num)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2)coordinator없이 데이터 읽으려고 함\\n3)헤더 지정 안해줌\\n4)label하고 이미지하고 다른현상,sess.run\\n5)subplot\\n6)gz file자동압축해제 안되는 현상\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "coord.clear_stop()\n",
    "sess.close()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2)coordinator없이 데이터 읽으려고 함\n",
    "3)헤더 지정 안해줌\n",
    "4)label하고 이미지하고 다른현상,sess.run\n",
    "5)subplot\n",
    "6)gz file자동압축해제 안되는 현상\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "count: {1: 16, 2: 16, 3: 18}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "위와 같이 MNIST를 불러오면 reader가 두개여서 그런지 threads가 여러개가 되면 학습이 잘안된다 왜그럴까?\n",
    "\"\"\"\n",
    "QUEUE_LENGTH = 20\n",
    "q = tf.FIFOQueue(QUEUE_LENGTH,\"float\")\n",
    "enq_ops1 = q.enqueue_many(([1.],) )\n",
    "enq_ops2 = q.enqueue_many(([2.],) )\n",
    "enq_ops3 = q.enqueue_many(([3.],) )\n",
    "qr = tf.train.QueueRunner(q,[enq_ops1,enq_ops2,enq_ops3])\n",
    "sess = tf.Session()\n",
    "# Create a coordinator, launch the queue runner threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = qr.create_threads(sess, coord=coord, start=True)\n",
    "count={1:0,2:0,3:0}\n",
    "for step in range(50):\n",
    "    temp=sess.run(q.dequeue())\n",
    "    count[temp]=count[temp]+1\n",
    "    print(temp)\n",
    "print('count:',count)\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
